{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9472093,"sourceType":"datasetVersion","datasetId":5760379}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-24T17:33:20.333182Z","iopub.execute_input":"2024-09-24T17:33:20.333926Z","iopub.status.idle":"2024-09-24T17:33:20.732373Z","shell.execute_reply.started":"2024-09-24T17:33:20.333879Z","shell.execute_reply":"2024-09-24T17:33:20.731432Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/7440.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/6213.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/13526.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/11002.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/4295.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/11004.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/14049.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/15278.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/8757.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/4333.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/13957.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/3293.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/1287.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/14251.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/1901.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/15581.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/11613.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/13164.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/17156.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/17677.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/16030.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/13553.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/15703.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/1234.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/5339.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/6977.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/15286.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/11273.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/16268.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/10986.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/4285.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/3504.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/7262.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/12989.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/11630.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/8468.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/6879.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/11563.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/13533.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/val2017/12018.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/annotations/instances_val2017.json\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/annotations/instances_train2017.json\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/6229.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/1024.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/6458.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/11265.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/8765.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/8794.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/17065.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/12918.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/8556.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/7065.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/11318.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/6334.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/10990.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/15306.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/13199.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/5744.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/6576.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/3333.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/15297.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/17137.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/8791.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/1997.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/13038.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/13579.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/17066.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/15679.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/5665.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/12046.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/6222.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/9178.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/1904.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/11248.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/5918.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/9260.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/13518.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/1335.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/8471.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/6276.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/3276.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/13984.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/11588.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/16065.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/15342.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/9304.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/5235.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/3449.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/8416.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/1313.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/1038.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/1960.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/6962.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/11088.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/9167.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/13842.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/15348.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/9317.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/1254.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/16160.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/5952.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/1088.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/7063.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/11937.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/8810.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/15285.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/15303.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/11080.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/15330.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/13031.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/3557.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/4312.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/12970.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/8836.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/17526.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/7310.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/7452.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/4238.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/5704.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/5298.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/15584.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/3438.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/3485.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/17143.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/7474.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/3299.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/17645.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/6932.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/1292.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/5263.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/13836.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/1898.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/11281.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/15319.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/16253.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/7002.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/3328.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/10999.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/3571.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/7224.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/11297.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/17679.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/1298.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/13547.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/13511.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/11267.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/15656.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/5866.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/8531.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/11324.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/3512.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/7320.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/6215.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/15962.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/12964.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/16213.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/3568.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/14264.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/1083.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/15287.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/4242.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/8763.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/17033.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/5336.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/15283.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/7477.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/4347.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/3478.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/7367.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/15292.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/5931.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/9339.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/7450.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/11946.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/12056.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/16128.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/5879.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/13176.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/13995.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/13211.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/15329.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/12040.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/15353.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/10957.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/11303.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/8466.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/3516.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/11987.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/1229.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/11909.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/11283.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/16154.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/13187.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/7466.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/8563.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/11953.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/5963.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/3237.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/15349.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/11100.jpg\n/kaggle/input/dreamdelhi/COCODIR/COCODIR/train2017/5303.jpg\n/kaggle/input/dreamdelhi/checkpoint/checkpoint/checkpoint0033_4scale.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/IDEA-Research/DINO.git\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:33:20.738604Z","iopub.execute_input":"2024-09-24T17:33:20.739015Z","iopub.status.idle":"2024-09-24T17:33:21.730999Z","shell.execute_reply.started":"2024-09-24T17:33:20.738966Z","shell.execute_reply":"2024-09-24T17:33:21.729732Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"fatal: destination path 'DINO' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls /kaggle/working/DINO/\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:33:21.732836Z","iopub.execute_input":"2024-09-24T17:33:21.733792Z","iopub.status.idle":"2024-09-24T17:33:22.745376Z","shell.execute_reply.started":"2024-09-24T17:33:21.733727Z","shell.execute_reply":"2024-09-24T17:33:22.744116Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"COCODIR\t\t\t\t\tengine.py\nDeformable-DETR\t\t\t\tfigs\nLICENSE\t\t\t\t\tinference_and_visualization.ipynb\nMultiScaleDeformableAttention.egg-info\tmain.py\nREADME.md\t\t\t\tmodels\nbuild\t\t\t\t\trequirements.txt\ncheckpoint\t\t\t\trun_with_submitit.py\nconfig\t\t\t\t\tscripts\ndatasets\t\t\t\ttools\ndist\t\t\t\t\tutil\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls /kaggle/input/dreamdelhi/COCODIR\n!ls /kaggle/input/dreamdelhi/checkpoint\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:33:22.749391Z","iopub.execute_input":"2024-09-24T17:33:22.749845Z","iopub.status.idle":"2024-09-24T17:33:24.719667Z","shell.execute_reply.started":"2024-09-24T17:33:22.749807Z","shell.execute_reply":"2024-09-24T17:33:24.718533Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"COCODIR\ncheckpoint\n","output_type":"stream"}]},{"cell_type":"code","source":"# Copy the contents of COCODIR directly into the DINO directory\n!cp -r /kaggle/input/dreamdelhi/COCODIR/* /kaggle/working/DINO/COCODIR/\n\n# Copy the contents of the checkpoint directly into the DINO checkpoint directory\n!cp -r /kaggle/input/dreamdelhi/checkpoint/* /kaggle/working/DINO/checkpoint/\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:33:24.721366Z","iopub.execute_input":"2024-09-24T17:33:24.722260Z","iopub.status.idle":"2024-09-24T17:33:27.628943Z","shell.execute_reply.started":"2024-09-24T17:33:24.722212Z","shell.execute_reply":"2024-09-24T17:33:27.627676Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:33:27.630675Z","iopub.execute_input":"2024-09-24T17:33:27.631653Z","iopub.status.idle":"2024-09-24T17:33:27.639797Z","shell.execute_reply.started":"2024-09-24T17:33:27.631600Z","shell.execute_reply":"2024-09-24T17:33:27.638803Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"code","source":"!pip install yapf==0.40.1","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:33:27.641126Z","iopub.execute_input":"2024-09-24T17:33:27.641418Z","iopub.status.idle":"2024-09-24T17:33:39.165830Z","shell.execute_reply.started":"2024-09-24T17:33:27.641387Z","shell.execute_reply":"2024-09-24T17:33:39.164668Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Requirement already satisfied: yapf==0.40.1 in /opt/conda/lib/python3.10/site-packages (0.40.1)\nRequirement already satisfied: importlib-metadata>=6.6.0 in /opt/conda/lib/python3.10/site-packages (from yapf==0.40.1) (7.0.0)\nRequirement already satisfied: platformdirs>=3.5.1 in /opt/conda/lib/python3.10/site-packages (from yapf==0.40.1) (3.11.0)\nRequirement already satisfied: tomli>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from yapf==0.40.1) (2.0.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=6.6.0->yapf==0.40.1) (3.19.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/DINO","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:33:39.167191Z","iopub.execute_input":"2024-09-24T17:33:39.167521Z","iopub.status.idle":"2024-09-24T17:33:39.174336Z","shell.execute_reply.started":"2024-09-24T17:33:39.167486Z","shell.execute_reply":"2024-09-24T17:33:39.173396Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"/kaggle/working/DINO\n","output_type":"stream"}]},{"cell_type":"code","source":"pwd","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:33:39.175580Z","iopub.execute_input":"2024-09-24T17:33:39.175907Z","iopub.status.idle":"2024-09-24T17:33:39.187969Z","shell.execute_reply.started":"2024-09-24T17:33:39.175871Z","shell.execute_reply":"2024-09-24T17:33:39.187022Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/DINO'"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torchvision","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:33:39.189214Z","iopub.execute_input":"2024-09-24T17:33:39.189506Z","iopub.status.idle":"2024-09-24T17:33:41.787175Z","shell.execute_reply.started":"2024-09-24T17:33:39.189475Z","shell.execute_reply":"2024-09-24T17:33:41.786140Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!pip install -r requirements.txt ","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:33:41.788340Z","iopub.execute_input":"2024-09-24T17:33:41.788795Z","iopub.status.idle":"2024-09-24T17:33:56.858028Z","shell.execute_reply.started":"2024-09-24T17:33:41.788742Z","shell.execute_reply":"2024-09-24T17:33:56.857018Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Collecting pycocotools (from -r requirements.txt (line 2))\n  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-install-v9duuu3f/pycocotools_431521ed4d73472d98b092775a766ab4\n  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/cocoapi.git /tmp/pip-install-v9duuu3f/pycocotools_431521ed4d73472d98b092775a766ab4\n  Resolved https://github.com/cocodataset/cocoapi.git to commit 8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting panopticapi (from -r requirements.txt (line 6))\n  Cloning https://github.com/cocodataset/panopticapi.git to /tmp/pip-install-v9duuu3f/panopticapi_2104695631554afb9e8e8f9dbebb7cab\n  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/panopticapi.git /tmp/pip-install-v9duuu3f/panopticapi_2104695631554afb9e8e8f9dbebb7cab\n  Resolved https://github.com/cocodataset/panopticapi.git to commit 7bb4655548f98f3fedc07bf37e9040a992b054b0\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: cython in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (3.0.10)\nRequirement already satisfied: submitit in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (1.5.2)\nRequirement already satisfied: torch>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (2.4.0)\nRequirement already satisfied: torchvision>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.19.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.14.1)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.4.0)\nRequirement already satisfied: addict in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (2.4.0)\nRequirement already satisfied: yapf in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.40.1)\nRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (1.0.9)\nRequirement already satisfied: setuptools>=18.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools->-r requirements.txt (line 2)) (70.0.0)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools->-r requirements.txt (line 2)) (3.7.5)\nRequirement already satisfied: cloudpickle>=1.2.1 in /opt/conda/lib/python3.10/site-packages (from submitit->-r requirements.txt (line 3)) (3.0.0)\nRequirement already satisfied: typing_extensions>=3.7.4.2 in /opt/conda/lib/python3.10/site-packages (from submitit->-r requirements.txt (line 3)) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (2024.6.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.6.0->-r requirements.txt (line 5)) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.6.0->-r requirements.txt (line 5)) (10.3.0)\nRequirement already satisfied: importlib-metadata>=6.6.0 in /opt/conda/lib/python3.10/site-packages (from yapf->-r requirements.txt (line 10)) (7.0.0)\nRequirement already satisfied: platformdirs>=3.5.1 in /opt/conda/lib/python3.10/site-packages (from yapf->-r requirements.txt (line 10)) (3.11.0)\nRequirement already satisfied: tomli>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from yapf->-r requirements.txt (line 10)) (2.0.1)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm->-r requirements.txt (line 11)) (6.0.2)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm->-r requirements.txt (line 11)) (0.25.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm->-r requirements.txt (line 11)) (0.4.5)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=6.6.0->yapf->-r requirements.txt (line 10)) (3.19.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (2.9.0.post0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm->-r requirements.txt (line 11)) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm->-r requirements.txt (line 11)) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.5.0->-r requirements.txt (line 4)) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.5.0->-r requirements.txt (line 4)) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 11)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 11)) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 11)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 11)) (2024.8.30)\n","output_type":"stream"}]},{"cell_type":"code","source":"!python models/dino/ops/setup.py build install","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:35:56.753798Z","iopub.execute_input":"2024-09-24T17:35:56.754217Z","iopub.status.idle":"2024-09-24T17:35:57.849962Z","shell.execute_reply.started":"2024-09-24T17:35:56.754178Z","shell.execute_reply":"2024-09-24T17:35:57.848779Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"python: can't open file '/kaggle/working/DINO/Deformable-DETR/models/ops/models/dino/ops/setup.py': [Errno 2] No such file or directory\n","output_type":"stream"}]},{"cell_type":"code","source":"!python models/dino/ops/test.py","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:34:03.908939Z","iopub.execute_input":"2024-09-24T17:34:03.909304Z","iopub.status.idle":"2024-09-24T17:35:36.058685Z","shell.execute_reply.started":"2024-09-24T17:34:03.909251Z","shell.execute_reply":"2024-09-24T17:35:36.057416Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"* True check_forward_equal_with_pytorch_double: max_abs_err 8.67e-19 max_rel_err 2.35e-16\n* True check_forward_equal_with_pytorch_float: max_abs_err 4.66e-10 max_rel_err 1.13e-07\n* True check_gradient_numerical(D=30)\n* True check_gradient_numerical(D=32)\n* True check_gradient_numerical(D=64)\n* True check_gradient_numerical(D=71)\n* True check_gradient_numerical(D=1025)\nTraceback (most recent call last):\n  File \"/kaggle/working/DINO/models/dino/ops/test.py\", line 86, in <module>\n    check_gradient_numerical(channels, True, True, True)\n  File \"/kaggle/working/DINO/models/dino/ops/test.py\", line 76, in check_gradient_numerical\n    gradok = gradcheck(func, (value.double(), shapes, level_start_index, sampling_locations.double(), attention_weights.double(), im2col_step))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/gradcheck.py\", line 2053, in gradcheck\n    return _gradcheck_helper(**args)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/gradcheck.py\", line 2082, in _gradcheck_helper\n    _gradcheck_real_imag(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/gradcheck.py\", line 1492, in _gradcheck_real_imag\n    gradcheck_fn(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/gradcheck.py\", line 1627, in _slow_gradcheck\n    analytical = _check_analytical_jacobian_attributes(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/gradcheck.py\", line 777, in _check_analytical_jacobian_attributes\n    vjps2 = _compute_analytical_jacobian_rows(vjp_fn, output.clone())\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/gradcheck.py\", line 894, in _compute_analytical_jacobian_rows\n    grad_inputs = vjp_fn(grad_out_base)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/gradcheck.py\", line 767, in vjp_fn\n    return torch.autograd.grad(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 436, in grad\n    result = _engine_run_backward(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py\", line 768, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py\", line 306, in apply\n    return user_fn(self, *args)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py\", line 599, in wrapper\n    outputs = fn(ctx, *args)\n  File \"/kaggle/working/DINO/models/dino/ops/functions/ms_deform_attn_func.py\", line 35, in backward\n    MSDA.ms_deform_attn_backward(\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 29.12 MiB is free. Process 14064 has 15.86 GiB memory in use. Of the allocated memory 15.07 GiB is allocated by PyTorch, and 526.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/fundamentalvision/Deformable-DETR.git","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:35:36.060937Z","iopub.execute_input":"2024-09-24T17:35:36.061712Z","iopub.status.idle":"2024-09-24T17:35:37.063908Z","shell.execute_reply.started":"2024-09-24T17:35:36.061662Z","shell.execute_reply":"2024-09-24T17:35:37.062630Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"fatal: destination path 'Deformable-DETR' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"!python --version\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:35:37.066032Z","iopub.execute_input":"2024-09-24T17:35:37.066361Z","iopub.status.idle":"2024-09-24T17:35:38.058069Z","shell.execute_reply.started":"2024-09-24T17:35:37.066328Z","shell.execute_reply":"2024-09-24T17:35:38.057093Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Python 3.10.14\n","output_type":"stream"}]},{"cell_type":"code","source":"cd /kaggle/working/DINO/Deformable-DETR","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:35:38.059591Z","iopub.execute_input":"2024-09-24T17:35:38.059991Z","iopub.status.idle":"2024-09-24T17:35:38.067348Z","shell.execute_reply.started":"2024-09-24T17:35:38.059954Z","shell.execute_reply":"2024-09-24T17:35:38.066503Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"/kaggle/working/DINO/Deformable-DETR\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:35:38.068364Z","iopub.execute_input":"2024-09-24T17:35:38.068664Z","iopub.status.idle":"2024-09-24T17:35:49.622762Z","shell.execute_reply.started":"2024-09-24T17:35:38.068627Z","shell.execute_reply":"2024-09-24T17:35:49.621521Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pycocotools in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.66.4)\nRequirement already satisfied: cython in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (3.0.10)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.14.1)\nRequirement already satisfied: setuptools>=18.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools->-r requirements.txt (line 1)) (70.0.0)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools->-r requirements.txt (line 1)) (3.7.5)\nRequirement already satisfied: numpy<2.3,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from scipy->-r requirements.txt (line 4)) (1.26.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 1)) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 1)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 1)) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 1)) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 1)) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 1)) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 1)) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 1)) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"cd /kaggle/working/DINO/Deformable-DETR/models/ops","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:35:49.624496Z","iopub.execute_input":"2024-09-24T17:35:49.624958Z","iopub.status.idle":"2024-09-24T17:35:49.632792Z","shell.execute_reply.started":"2024-09-24T17:35:49.624908Z","shell.execute_reply":"2024-09-24T17:35:49.631887Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"/kaggle/working/DINO/Deformable-DETR/models/ops\n","output_type":"stream"}]},{"cell_type":"code","source":"!sh ./make.sh","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:35:49.634077Z","iopub.execute_input":"2024-09-24T17:35:49.634418Z","iopub.status.idle":"2024-09-24T17:35:56.392147Z","shell.execute_reply.started":"2024-09-24T17:35:49.634384Z","shell.execute_reply":"2024-09-24T17:35:56.390848Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no g++ version bounds defined for CUDA version 12.3\n  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n/opt/conda/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nEmitting ninja build file /kaggle/working/DINO/Deformable-DETR/models/ops/build/temp.linux-x86_64-cpython-310/build.ninja...\nCompiling objects...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\nninja: no work to do.\n/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` directly.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\n/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` and ``easy_install``.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://github.com/pypa/setuptools/issues/917 for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\n","output_type":"stream"}]},{"cell_type":"code","source":"!python test.py","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:44:56.972338Z","iopub.execute_input":"2024-09-24T17:44:56.972814Z","iopub.status.idle":"2024-09-24T17:46:28.596549Z","shell.execute_reply.started":"2024-09-24T17:44:56.972772Z","shell.execute_reply":"2024-09-24T17:46:28.595315Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"* True check_forward_equal_with_pytorch_double: max_abs_err 8.67e-19 max_rel_err 2.35e-16\n* True check_forward_equal_with_pytorch_float: max_abs_err 4.66e-10 max_rel_err 1.13e-07\n* True check_gradient_numerical(D=30)\n* True check_gradient_numerical(D=32)\n* True check_gradient_numerical(D=64)\n* True check_gradient_numerical(D=71)\n* True check_gradient_numerical(D=1025)\nTraceback (most recent call last):\n  File \"/kaggle/working/DINO/Deformable-DETR/models/ops/test.py\", line 86, in <module>\n    check_gradient_numerical(channels, True, True, True)\n  File \"/kaggle/working/DINO/Deformable-DETR/models/ops/test.py\", line 76, in check_gradient_numerical\n    gradok = gradcheck(func, (value.double(), shapes, level_start_index, sampling_locations.double(), attention_weights.double(), im2col_step))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/gradcheck.py\", line 2053, in gradcheck\n    return _gradcheck_helper(**args)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/gradcheck.py\", line 2082, in _gradcheck_helper\n    _gradcheck_real_imag(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/gradcheck.py\", line 1492, in _gradcheck_real_imag\n    gradcheck_fn(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/gradcheck.py\", line 1627, in _slow_gradcheck\n    analytical = _check_analytical_jacobian_attributes(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/gradcheck.py\", line 777, in _check_analytical_jacobian_attributes\n    vjps2 = _compute_analytical_jacobian_rows(vjp_fn, output.clone())\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/gradcheck.py\", line 894, in _compute_analytical_jacobian_rows\n    grad_inputs = vjp_fn(grad_out_base)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/gradcheck.py\", line 767, in vjp_fn\n    return torch.autograd.grad(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 436, in grad\n    result = _engine_run_backward(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py\", line 768, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py\", line 306, in apply\n    return user_fn(self, *args)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py\", line 599, in wrapper\n    outputs = fn(ctx, *args)\n  File \"/kaggle/working/DINO/Deformable-DETR/models/ops/functions/ms_deform_attn_func.py\", line 35, in backward\n    MSDA.ms_deform_attn_backward(\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 29.12 MiB is free. Process 18533 has 15.86 GiB memory in use. Of the allocated memory 15.07 GiB is allocated by PyTorch, and 526.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"}]},{"cell_type":"code","source":"cd ..","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:47:22.826056Z","iopub.execute_input":"2024-09-24T17:47:22.826474Z","iopub.status.idle":"2024-09-24T17:47:22.832209Z","shell.execute_reply.started":"2024-09-24T17:47:22.826439Z","shell.execute_reply":"2024-09-24T17:47:22.831291Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"/kaggle/working/DINO\n","output_type":"stream"}]},{"cell_type":"code","source":"cd models/dino/ops","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:47:42.978660Z","iopub.execute_input":"2024-09-24T17:47:42.979081Z","iopub.status.idle":"2024-09-24T17:47:42.985238Z","shell.execute_reply.started":"2024-09-24T17:47:42.979043Z","shell.execute_reply":"2024-09-24T17:47:42.984337Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"/kaggle/working/DINO/models/dino/ops\n","output_type":"stream"}]},{"cell_type":"code","source":"!python setup.py build install","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:48:07.059781Z","iopub.execute_input":"2024-09-24T17:48:07.060525Z","iopub.status.idle":"2024-09-24T17:48:44.061258Z","shell.execute_reply.started":"2024-09-24T17:48:07.060485Z","shell.execute_reply":"2024-09-24T17:48:44.060062Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no g++ version bounds defined for CUDA version 12.3\n  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n/opt/conda/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nEmitting ninja build file /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-310/build.ninja...\nCompiling objects...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-310/kaggle/working/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.o.d -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -DWITH_CUDA -I/kaggle/working/DINO/models/dino/ops/src -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.cpp -o /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-310/kaggle/working/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n[2/3] c++ -MMD -MF /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-310/kaggle/working/DINO/models/dino/ops/src/vision.o.d -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -DWITH_CUDA -I/kaggle/working/DINO/models/dino/ops/src -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/DINO/models/dino/ops/src/vision.cpp -o /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-310/kaggle/working/DINO/models/dino/ops/src/vision.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\nIn file included from /kaggle/working/DINO/models/dino/ops/src/vision.cpp:11:\n/kaggle/working/DINO/models/dino/ops/src/ms_deform_attn.h: In function 'at::Tensor ms_deform_attn_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)':\n/kaggle/working/DINO/models/dino/ops/src/ms_deform_attn.h:29:19: warning: 'at::DeprecatedTypeProperties& at::Tensor::type() const' is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n   29 |     if (value.type().is_cuda())\n      |         ~~~~~~~~~~^~\nIn file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                 from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                 from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                 from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                 from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                 from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                 from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                 from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                 from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:5,\n                 from /kaggle/working/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.h:12,\n                 from /kaggle/working/DINO/models/dino/ops/src/ms_deform_attn.h:13,\n                 from /kaggle/working/DINO/models/dino/ops/src/vision.cpp:11:\n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      |                              ^~~~\nIn file included from /kaggle/working/DINO/models/dino/ops/src/vision.cpp:11:\n/kaggle/working/DINO/models/dino/ops/src/ms_deform_attn.h: In function 'std::vector<at::Tensor> ms_deform_attn_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)':\n/kaggle/working/DINO/models/dino/ops/src/ms_deform_attn.h:51:19: warning: 'at::DeprecatedTypeProperties& at::Tensor::type() const' is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n   51 |     if (value.type().is_cuda())\n      |         ~~~~~~~~~~^~\nIn file included from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/Tensor.h:3,\n                 from /opt/conda/lib/python3.10/site-packages/torch/include/ATen/Tensor.h:3,\n                 from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                 from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                 from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                 from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                 from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                 from /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                 from /opt/conda/lib/python3.10/site-packages/torch/include/torch/extension.h:5,\n                 from /kaggle/working/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.h:12,\n                 from /kaggle/working/DINO/models/dino/ops/src/ms_deform_attn.h:13,\n                 from /kaggle/working/DINO/models/dino/ops/src/vision.cpp:11:\n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      |                              ^~~~\n[3/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-310/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.o.d -DWITH_CUDA -I/kaggle/working/DINO/models/dino/ops/src -I/opt/conda/lib/python3.10/site-packages/torch/include -I/opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.10 -c -c /kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu -o /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-310/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1016\"' -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=1 -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 -std=c++17\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(261): warning #177-D: variable \"q_col\" was declared but never referenced\n      const int q_col = _temp % num_query;\n                ^\n          detected during instantiation of \"void ms_deformable_im2col_cuda(cudaStream_t, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *) [with scalar_t=double]\" at line 64 of /kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(762): warning #177-D: variable \"q_col\" was declared but never referenced\n      const int q_col = _temp % num_query;\n                ^\n          detected during instantiation of \"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\" at line 134 of /kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\n\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(872): warning #177-D: variable \"q_col\" was declared but never referenced\n      const int q_col = _temp % num_query;\n                ^\n          detected during instantiation of \"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\" at line 134 of /kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\n\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(331): warning #177-D: variable \"q_col\" was declared but never referenced\n      const int q_col = _temp % num_query;\n                ^\n          detected during instantiation of \"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\" at line 134 of /kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\n\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(436): warning #177-D: variable \"q_col\" was declared but never referenced\n      const int q_col = _temp % num_query;\n                ^\n          detected during instantiation of \"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\" at line 134 of /kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\n\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(544): warning #177-D: variable \"q_col\" was declared but never referenced\n      const int q_col = _temp % num_query;\n                ^\n          detected during instantiation of \"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\" at line 134 of /kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\n\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(649): warning #177-D: variable \"q_col\" was declared but never referenced\n      const int q_col = _temp % num_query;\n                ^\n          detected during instantiation of \"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\" at line 134 of /kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\n\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu: In function 'at::Tensor ms_deform_attn_cuda_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)':\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:34:61: warning: 'at::DeprecatedTypeProperties& at::Tensor::type() const' is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n   34 |     AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~^~\n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:35:70: warning: 'at::DeprecatedTypeProperties& at::Tensor::type() const' is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n   35 |     AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~~~~~~~~~~^~\n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:36:73: warning: 'at::DeprecatedTypeProperties& at::Tensor::type() const' is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n   36 |     AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~~~~~~~~~~~~~^~\n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:37:68: warning: 'at::DeprecatedTypeProperties& at::Tensor::type() const' is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n   37 |     AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~~~~~~~~^~\n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:38:67: warning: 'at::DeprecatedTypeProperties& at::Tensor::type() const' is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n   38 |     AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~~~~~~~^~\n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu: In lambda function:\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:42: warning: 'at::DeprecatedTypeProperties& at::Tensor::type() const' is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                              ~~~~~~~~~~~~^~\n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:163: warning: 'c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)' is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                   ^         \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:109:1: note: declared here\n  109 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n      | ^~~~~~~~~~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu: In lambda function:\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1047: warning: 'T* at::Tensor::data() const [with T = double]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1133: warning: 'T* at::Tensor::data() const [with T = long int]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1176: warning: 'T* at::Tensor::data() const [with T = long int]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1209: warning: 'T* at::Tensor::data() const [with T = double]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1292: warning: 'T* at::Tensor::data() const [with T = double]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1450: warning: 'T* at::Tensor::data() const [with T = double]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu: In lambda function:\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2314: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2400: warning: 'T* at::Tensor::data() const [with T = long int]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2443: warning: 'T* at::Tensor::data() const [with T = long int]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2475: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2557: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2714: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu: In function 'std::vector<at::Tensor> ms_deform_attn_cuda_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)':\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:100:61: warning: 'at::DeprecatedTypeProperties& at::Tensor::type() const' is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n  100 |     AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~^~\n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:101:70: warning: 'at::DeprecatedTypeProperties& at::Tensor::type() const' is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n  101 |     AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~~~~~~~~~~^~\n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:102:73: warning: 'at::DeprecatedTypeProperties& at::Tensor::type() const' is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n  102 |     AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~~~~~~~~~~~~~^~\n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:103:68: warning: 'at::DeprecatedTypeProperties& at::Tensor::type() const' is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n  103 |     AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~~~~~~~~^~\n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:104:67: warning: 'at::DeprecatedTypeProperties& at::Tensor::type() const' is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n  104 |     AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~~~~~~~^~\n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:105:67: warning: 'at::DeprecatedTypeProperties& at::Tensor::type() const' is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n  105 |     AT_ASSERTM(grad_output.type().is_cuda(), \"grad_output must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~~~~~~~^~\n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu: In lambda function:\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:42: warning: 'at::DeprecatedTypeProperties& at::Tensor::type() const' is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                              ~~~~~~~~~~~~^~\n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:164: warning: 'c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)' is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                    ^         \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/Dispatch.h:109:1: note: declared here\n  109 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n      | ^~~~~~~~~~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu: In lambda function:\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1057: warning: 'T* at::Tensor::data() const [with T = double]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1083: warning: 'T* at::Tensor::data() const [with T = double]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1169: warning: 'T* at::Tensor::data() const [with T = long int]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1212: warning: 'T* at::Tensor::data() const [with T = long int]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1245: warning: 'T* at::Tensor::data() const [with T = double]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1328: warning: 'T* at::Tensor::data() const [with T = double]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1489: warning: 'T* at::Tensor::data() const [with T = double]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1573: warning: 'T* at::Tensor::data() const [with T = double]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1661: warning: 'T* at::Tensor::data() const [with T = double]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu: In lambda function:\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2586: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2611: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2697: warning: 'T* at::Tensor::data() const [with T = long int]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2740: warning: 'T* at::Tensor::data() const [with T = long int]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2772: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2854: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:3014: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:3097: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:3184: warning: 'T* at::Tensor::data() const [with T = float]' is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^ \n/opt/conda/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` directly.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\n/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` and ``easy_install``.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://github.com/pypa/setuptools/issues/917 for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-09-24T18:16:05.594959Z","iopub.execute_input":"2024-09-24T18:16:05.595933Z","iopub.status.idle":"2024-09-24T18:16:05.600147Z","shell.execute_reply.started":"2024-09-24T18:16:05.595891Z","shell.execute_reply":"2024-09-24T18:16:05.599002Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"!bash scripts/DINO_eval.sh /kaggle/input/dreamdelhi/COCODIR/COCODIR  /kaggle/input/dreamdelhi/checkpoint/checkpoint/checkpoint0033_4scale.pth","metadata":{"execution":{"iopub.status.busy":"2024-09-24T18:16:12.303166Z","iopub.execute_input":"2024-09-24T18:16:12.303596Z","iopub.status.idle":"2024-09-24T18:16:29.357154Z","shell.execute_reply.started":"2024-09-24T18:16:12.303556Z","shell.execute_reply":"2024-09-24T18:16:29.355895Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Not using distributed mode\nLoading config file from config/DINO/DINO_4scale.py\n[09/24 18:16:16.479]: git:\n  sha: d84a491d41898b3befd8294d1cf2614661fc0953, status: clean, branch: main\n\n[09/24 18:16:16.479]: Command: main.py --output_dir logs/DINO/R50-MS4-%j -c config/DINO/DINO_4scale.py --coco_path /kaggle/input/dreamdelhi/COCODIR/COCODIR --eval --resume /kaggle/input/dreamdelhi/checkpoint/checkpoint/checkpoint0033_4scale.pth --options dn_scalar=100 embed_init_tgt=TRUE dn_label_coef=1.0 dn_bbox_coef=1.0 use_ema=False dn_box_noise_scale=1.0\n[09/24 18:16:16.480]: Full config saved to logs/DINO/R50-MS4-%j/config_args_all.json\n[09/24 18:16:16.480]: world size: 1\n[09/24 18:16:16.480]: rank: 0\n[09/24 18:16:16.480]: local_rank: 0\n[09/24 18:16:16.480]: args: Namespace(config_file='config/DINO/DINO_4scale.py', options={'dn_scalar': 100, 'embed_init_tgt': True, 'dn_label_coef': 1.0, 'dn_bbox_coef': 1.0, 'use_ema': False, 'dn_box_noise_scale': 1.0}, dataset_file='coco', coco_path='/kaggle/input/dreamdelhi/COCODIR/COCODIR', coco_panoptic_path=None, remove_difficult=False, fix_size=False, output_dir='logs/DINO/R50-MS4-%j', note='', device='cuda', seed=42, resume='/kaggle/input/dreamdelhi/checkpoint/checkpoint/checkpoint0033_4scale.pth', pretrain_model_path=None, finetune_ignore=None, start_epoch=0, eval=True, num_workers=10, test=False, debug=False, find_unused_params=False, save_results=False, save_log=False, world_size=1, dist_url='env://', rank=0, local_rank=0, amp=False, distributed=False, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_max_size=1333, data_aug_scales2_resize=[400, 500, 600], data_aug_scales2_crop=[384, 600], data_aug_scale_overlap=None, num_classes=91, lr=0.0001, param_dict_type='default', lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, ddetr_lr_param=False, batch_size=2, weight_decay=0.0001, epochs=12, lr_drop=11, save_checkpoint_interval=1, clip_max_norm=0.1, onecyclelr=False, multi_step_lr=False, lr_drop_list=[33, 45], modelname='dino', frozen_weights=None, backbone='resnet50', use_checkpoint=False, dilation=False, position_embedding='sine', pe_temperatureH=20, pe_temperatureW=20, return_interm_indices=[1, 2, 3], backbone_freeze_keywords=None, enc_layers=6, dec_layers=6, unic_layers=0, pre_norm=False, dim_feedforward=2048, hidden_dim=256, dropout=0.0, nheads=8, num_queries=900, query_dim=4, num_patterns=0, pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, random_refpoints_xy=False, fix_refpoints_hw=-1, dabdetr_yolo_like_anchor_update=False, dabdetr_deformable_encoder=False, dabdetr_deformable_decoder=False, use_deformable_box_attn=False, box_attn_type='roi_align', dec_layer_number=None, num_feature_levels=4, enc_n_points=4, dec_n_points=4, decoder_layer_noise=False, dln_xy_noise=0.2, dln_hw_noise=0.2, add_channel_attention=False, add_pos_value=False, two_stage_type='standard', two_stage_pat_embed=0, two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_learn_wh=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, num_select=300, transformer_activation='relu', batch_norm_type='FrozenBatchNorm2d', masks=False, aux_loss=True, set_cost_class=2.0, set_cost_bbox=5.0, set_cost_giou=2.0, cls_loss_coef=1.0, mask_loss_coef=1.0, dice_loss_coef=1.0, bbox_loss_coef=5.0, giou_loss_coef=2.0, enc_loss_coef=1.0, interm_loss_coef=1.0, no_interm_box_loss=False, focal_alpha=0.25, decoder_sa_type='sa', matcher_type='HungarianMatcher', decoder_module_seq=['sa', 'ca', 'ffn'], nms_iou_threshold=-1, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, use_dn=True, dn_number=100, dn_box_noise_scale=1.0, dn_label_noise_ratio=0.5, embed_init_tgt=True, dn_labelbook_size=91, match_unstable_error=True, use_ema=False, ema_decay=0.9997, ema_epoch=0, use_detached_boxes_dec_out=False, dn_scalar=100, dn_label_coef=1.0, dn_bbox_coef=1.0)\n\nNamespace(config_file='config/DINO/DINO_4scale.py', options={'dn_scalar': 100, 'embed_init_tgt': True, 'dn_label_coef': 1.0, 'dn_bbox_coef': 1.0, 'use_ema': False, 'dn_box_noise_scale': 1.0}, dataset_file='coco', coco_path='/kaggle/input/dreamdelhi/COCODIR/COCODIR', coco_panoptic_path=None, remove_difficult=False, fix_size=False, output_dir='logs/DINO/R50-MS4-%j', note='', device='cuda', seed=42, resume='/kaggle/input/dreamdelhi/checkpoint/checkpoint/checkpoint0033_4scale.pth', pretrain_model_path=None, finetune_ignore=None, start_epoch=0, eval=True, num_workers=10, test=False, debug=False, find_unused_params=False, save_results=False, save_log=False, world_size=1, dist_url='env://', rank=0, local_rank=0, amp=False, distributed=False, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_max_size=1333, data_aug_scales2_resize=[400, 500, 600], data_aug_scales2_crop=[384, 600], data_aug_scale_overlap=None, num_classes=91, lr=0.0001, param_dict_type='default', lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, ddetr_lr_param=False, batch_size=2, weight_decay=0.0001, epochs=12, lr_drop=11, save_checkpoint_interval=1, clip_max_norm=0.1, onecyclelr=False, multi_step_lr=False, lr_drop_list=[33, 45], modelname='dino', frozen_weights=None, backbone='resnet50', use_checkpoint=False, dilation=False, position_embedding='sine', pe_temperatureH=20, pe_temperatureW=20, return_interm_indices=[1, 2, 3], backbone_freeze_keywords=None, enc_layers=6, dec_layers=6, unic_layers=0, pre_norm=False, dim_feedforward=2048, hidden_dim=256, dropout=0.0, nheads=8, num_queries=900, query_dim=4, num_patterns=0, pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, random_refpoints_xy=False, fix_refpoints_hw=-1, dabdetr_yolo_like_anchor_update=False, dabdetr_deformable_encoder=False, dabdetr_deformable_decoder=False, use_deformable_box_attn=False, box_attn_type='roi_align', dec_layer_number=None, num_feature_levels=4, enc_n_points=4, dec_n_points=4, decoder_layer_noise=False, dln_xy_noise=0.2, dln_hw_noise=0.2, add_channel_attention=False, add_pos_value=False, two_stage_type='standard', two_stage_pat_embed=0, two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_learn_wh=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, num_select=300, transformer_activation='relu', batch_norm_type='FrozenBatchNorm2d', masks=False, aux_loss=True, set_cost_class=2.0, set_cost_bbox=5.0, set_cost_giou=2.0, cls_loss_coef=1.0, mask_loss_coef=1.0, dice_loss_coef=1.0, bbox_loss_coef=5.0, giou_loss_coef=2.0, enc_loss_coef=1.0, interm_loss_coef=1.0, no_interm_box_loss=False, focal_alpha=0.25, decoder_sa_type='sa', matcher_type='HungarianMatcher', decoder_module_seq=['sa', 'ca', 'ffn'], nms_iou_threshold=-1, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, use_dn=True, dn_number=100, dn_box_noise_scale=1.0, dn_label_noise_ratio=0.5, embed_init_tgt=True, dn_labelbook_size=91, match_unstable_error=True, use_ema=False, ema_decay=0.9997, ema_epoch=0, use_detached_boxes_dec_out=False, dn_scalar=100, dn_label_coef=1.0, dn_bbox_coef=1.0)\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n[09/24 18:16:18.646]: number of params:46670782\n[09/24 18:16:18.649]: params:\n{\n  \"transformer.level_embed\": 1024,\n  \"transformer.encoder.layers.0.self_attn.sampling_offsets.weight\": 65536,\n  \"transformer.encoder.layers.0.self_attn.sampling_offsets.bias\": 256,\n  \"transformer.encoder.layers.0.self_attn.attention_weights.weight\": 32768,\n  \"transformer.encoder.layers.0.self_attn.attention_weights.bias\": 128,\n  \"transformer.encoder.layers.0.self_attn.value_proj.weight\": 65536,\n  \"transformer.encoder.layers.0.self_attn.value_proj.bias\": 256,\n  \"transformer.encoder.layers.0.self_attn.output_proj.weight\": 65536,\n  \"transformer.encoder.layers.0.self_attn.output_proj.bias\": 256,\n  \"transformer.encoder.layers.0.norm1.weight\": 256,\n  \"transformer.encoder.layers.0.norm1.bias\": 256,\n  \"transformer.encoder.layers.0.linear1.weight\": 524288,\n  \"transformer.encoder.layers.0.linear1.bias\": 2048,\n  \"transformer.encoder.layers.0.linear2.weight\": 524288,\n  \"transformer.encoder.layers.0.linear2.bias\": 256,\n  \"transformer.encoder.layers.0.norm2.weight\": 256,\n  \"transformer.encoder.layers.0.norm2.bias\": 256,\n  \"transformer.encoder.layers.1.self_attn.sampling_offsets.weight\": 65536,\n  \"transformer.encoder.layers.1.self_attn.sampling_offsets.bias\": 256,\n  \"transformer.encoder.layers.1.self_attn.attention_weights.weight\": 32768,\n  \"transformer.encoder.layers.1.self_attn.attention_weights.bias\": 128,\n  \"transformer.encoder.layers.1.self_attn.value_proj.weight\": 65536,\n  \"transformer.encoder.layers.1.self_attn.value_proj.bias\": 256,\n  \"transformer.encoder.layers.1.self_attn.output_proj.weight\": 65536,\n  \"transformer.encoder.layers.1.self_attn.output_proj.bias\": 256,\n  \"transformer.encoder.layers.1.norm1.weight\": 256,\n  \"transformer.encoder.layers.1.norm1.bias\": 256,\n  \"transformer.encoder.layers.1.linear1.weight\": 524288,\n  \"transformer.encoder.layers.1.linear1.bias\": 2048,\n  \"transformer.encoder.layers.1.linear2.weight\": 524288,\n  \"transformer.encoder.layers.1.linear2.bias\": 256,\n  \"transformer.encoder.layers.1.norm2.weight\": 256,\n  \"transformer.encoder.layers.1.norm2.bias\": 256,\n  \"transformer.encoder.layers.2.self_attn.sampling_offsets.weight\": 65536,\n  \"transformer.encoder.layers.2.self_attn.sampling_offsets.bias\": 256,\n  \"transformer.encoder.layers.2.self_attn.attention_weights.weight\": 32768,\n  \"transformer.encoder.layers.2.self_attn.attention_weights.bias\": 128,\n  \"transformer.encoder.layers.2.self_attn.value_proj.weight\": 65536,\n  \"transformer.encoder.layers.2.self_attn.value_proj.bias\": 256,\n  \"transformer.encoder.layers.2.self_attn.output_proj.weight\": 65536,\n  \"transformer.encoder.layers.2.self_attn.output_proj.bias\": 256,\n  \"transformer.encoder.layers.2.norm1.weight\": 256,\n  \"transformer.encoder.layers.2.norm1.bias\": 256,\n  \"transformer.encoder.layers.2.linear1.weight\": 524288,\n  \"transformer.encoder.layers.2.linear1.bias\": 2048,\n  \"transformer.encoder.layers.2.linear2.weight\": 524288,\n  \"transformer.encoder.layers.2.linear2.bias\": 256,\n  \"transformer.encoder.layers.2.norm2.weight\": 256,\n  \"transformer.encoder.layers.2.norm2.bias\": 256,\n  \"transformer.encoder.layers.3.self_attn.sampling_offsets.weight\": 65536,\n  \"transformer.encoder.layers.3.self_attn.sampling_offsets.bias\": 256,\n  \"transformer.encoder.layers.3.self_attn.attention_weights.weight\": 32768,\n  \"transformer.encoder.layers.3.self_attn.attention_weights.bias\": 128,\n  \"transformer.encoder.layers.3.self_attn.value_proj.weight\": 65536,\n  \"transformer.encoder.layers.3.self_attn.value_proj.bias\": 256,\n  \"transformer.encoder.layers.3.self_attn.output_proj.weight\": 65536,\n  \"transformer.encoder.layers.3.self_attn.output_proj.bias\": 256,\n  \"transformer.encoder.layers.3.norm1.weight\": 256,\n  \"transformer.encoder.layers.3.norm1.bias\": 256,\n  \"transformer.encoder.layers.3.linear1.weight\": 524288,\n  \"transformer.encoder.layers.3.linear1.bias\": 2048,\n  \"transformer.encoder.layers.3.linear2.weight\": 524288,\n  \"transformer.encoder.layers.3.linear2.bias\": 256,\n  \"transformer.encoder.layers.3.norm2.weight\": 256,\n  \"transformer.encoder.layers.3.norm2.bias\": 256,\n  \"transformer.encoder.layers.4.self_attn.sampling_offsets.weight\": 65536,\n  \"transformer.encoder.layers.4.self_attn.sampling_offsets.bias\": 256,\n  \"transformer.encoder.layers.4.self_attn.attention_weights.weight\": 32768,\n  \"transformer.encoder.layers.4.self_attn.attention_weights.bias\": 128,\n  \"transformer.encoder.layers.4.self_attn.value_proj.weight\": 65536,\n  \"transformer.encoder.layers.4.self_attn.value_proj.bias\": 256,\n  \"transformer.encoder.layers.4.self_attn.output_proj.weight\": 65536,\n  \"transformer.encoder.layers.4.self_attn.output_proj.bias\": 256,\n  \"transformer.encoder.layers.4.norm1.weight\": 256,\n  \"transformer.encoder.layers.4.norm1.bias\": 256,\n  \"transformer.encoder.layers.4.linear1.weight\": 524288,\n  \"transformer.encoder.layers.4.linear1.bias\": 2048,\n  \"transformer.encoder.layers.4.linear2.weight\": 524288,\n  \"transformer.encoder.layers.4.linear2.bias\": 256,\n  \"transformer.encoder.layers.4.norm2.weight\": 256,\n  \"transformer.encoder.layers.4.norm2.bias\": 256,\n  \"transformer.encoder.layers.5.self_attn.sampling_offsets.weight\": 65536,\n  \"transformer.encoder.layers.5.self_attn.sampling_offsets.bias\": 256,\n  \"transformer.encoder.layers.5.self_attn.attention_weights.weight\": 32768,\n  \"transformer.encoder.layers.5.self_attn.attention_weights.bias\": 128,\n  \"transformer.encoder.layers.5.self_attn.value_proj.weight\": 65536,\n  \"transformer.encoder.layers.5.self_attn.value_proj.bias\": 256,\n  \"transformer.encoder.layers.5.self_attn.output_proj.weight\": 65536,\n  \"transformer.encoder.layers.5.self_attn.output_proj.bias\": 256,\n  \"transformer.encoder.layers.5.norm1.weight\": 256,\n  \"transformer.encoder.layers.5.norm1.bias\": 256,\n  \"transformer.encoder.layers.5.linear1.weight\": 524288,\n  \"transformer.encoder.layers.5.linear1.bias\": 2048,\n  \"transformer.encoder.layers.5.linear2.weight\": 524288,\n  \"transformer.encoder.layers.5.linear2.bias\": 256,\n  \"transformer.encoder.layers.5.norm2.weight\": 256,\n  \"transformer.encoder.layers.5.norm2.bias\": 256,\n  \"transformer.decoder.layers.0.cross_attn.sampling_offsets.weight\": 65536,\n  \"transformer.decoder.layers.0.cross_attn.sampling_offsets.bias\": 256,\n  \"transformer.decoder.layers.0.cross_attn.attention_weights.weight\": 32768,\n  \"transformer.decoder.layers.0.cross_attn.attention_weights.bias\": 128,\n  \"transformer.decoder.layers.0.cross_attn.value_proj.weight\": 65536,\n  \"transformer.decoder.layers.0.cross_attn.value_proj.bias\": 256,\n  \"transformer.decoder.layers.0.cross_attn.output_proj.weight\": 65536,\n  \"transformer.decoder.layers.0.cross_attn.output_proj.bias\": 256,\n  \"transformer.decoder.layers.0.norm1.weight\": 256,\n  \"transformer.decoder.layers.0.norm1.bias\": 256,\n  \"transformer.decoder.layers.0.self_attn.in_proj_weight\": 196608,\n  \"transformer.decoder.layers.0.self_attn.in_proj_bias\": 768,\n  \"transformer.decoder.layers.0.self_attn.out_proj.weight\": 65536,\n  \"transformer.decoder.layers.0.self_attn.out_proj.bias\": 256,\n  \"transformer.decoder.layers.0.norm2.weight\": 256,\n  \"transformer.decoder.layers.0.norm2.bias\": 256,\n  \"transformer.decoder.layers.0.linear1.weight\": 524288,\n  \"transformer.decoder.layers.0.linear1.bias\": 2048,\n  \"transformer.decoder.layers.0.linear2.weight\": 524288,\n  \"transformer.decoder.layers.0.linear2.bias\": 256,\n  \"transformer.decoder.layers.0.norm3.weight\": 256,\n  \"transformer.decoder.layers.0.norm3.bias\": 256,\n  \"transformer.decoder.layers.1.cross_attn.sampling_offsets.weight\": 65536,\n  \"transformer.decoder.layers.1.cross_attn.sampling_offsets.bias\": 256,\n  \"transformer.decoder.layers.1.cross_attn.attention_weights.weight\": 32768,\n  \"transformer.decoder.layers.1.cross_attn.attention_weights.bias\": 128,\n  \"transformer.decoder.layers.1.cross_attn.value_proj.weight\": 65536,\n  \"transformer.decoder.layers.1.cross_attn.value_proj.bias\": 256,\n  \"transformer.decoder.layers.1.cross_attn.output_proj.weight\": 65536,\n  \"transformer.decoder.layers.1.cross_attn.output_proj.bias\": 256,\n  \"transformer.decoder.layers.1.norm1.weight\": 256,\n  \"transformer.decoder.layers.1.norm1.bias\": 256,\n  \"transformer.decoder.layers.1.self_attn.in_proj_weight\": 196608,\n  \"transformer.decoder.layers.1.self_attn.in_proj_bias\": 768,\n  \"transformer.decoder.layers.1.self_attn.out_proj.weight\": 65536,\n  \"transformer.decoder.layers.1.self_attn.out_proj.bias\": 256,\n  \"transformer.decoder.layers.1.norm2.weight\": 256,\n  \"transformer.decoder.layers.1.norm2.bias\": 256,\n  \"transformer.decoder.layers.1.linear1.weight\": 524288,\n  \"transformer.decoder.layers.1.linear1.bias\": 2048,\n  \"transformer.decoder.layers.1.linear2.weight\": 524288,\n  \"transformer.decoder.layers.1.linear2.bias\": 256,\n  \"transformer.decoder.layers.1.norm3.weight\": 256,\n  \"transformer.decoder.layers.1.norm3.bias\": 256,\n  \"transformer.decoder.layers.2.cross_attn.sampling_offsets.weight\": 65536,\n  \"transformer.decoder.layers.2.cross_attn.sampling_offsets.bias\": 256,\n  \"transformer.decoder.layers.2.cross_attn.attention_weights.weight\": 32768,\n  \"transformer.decoder.layers.2.cross_attn.attention_weights.bias\": 128,\n  \"transformer.decoder.layers.2.cross_attn.value_proj.weight\": 65536,\n  \"transformer.decoder.layers.2.cross_attn.value_proj.bias\": 256,\n  \"transformer.decoder.layers.2.cross_attn.output_proj.weight\": 65536,\n  \"transformer.decoder.layers.2.cross_attn.output_proj.bias\": 256,\n  \"transformer.decoder.layers.2.norm1.weight\": 256,\n  \"transformer.decoder.layers.2.norm1.bias\": 256,\n  \"transformer.decoder.layers.2.self_attn.in_proj_weight\": 196608,\n  \"transformer.decoder.layers.2.self_attn.in_proj_bias\": 768,\n  \"transformer.decoder.layers.2.self_attn.out_proj.weight\": 65536,\n  \"transformer.decoder.layers.2.self_attn.out_proj.bias\": 256,\n  \"transformer.decoder.layers.2.norm2.weight\": 256,\n  \"transformer.decoder.layers.2.norm2.bias\": 256,\n  \"transformer.decoder.layers.2.linear1.weight\": 524288,\n  \"transformer.decoder.layers.2.linear1.bias\": 2048,\n  \"transformer.decoder.layers.2.linear2.weight\": 524288,\n  \"transformer.decoder.layers.2.linear2.bias\": 256,\n  \"transformer.decoder.layers.2.norm3.weight\": 256,\n  \"transformer.decoder.layers.2.norm3.bias\": 256,\n  \"transformer.decoder.layers.3.cross_attn.sampling_offsets.weight\": 65536,\n  \"transformer.decoder.layers.3.cross_attn.sampling_offsets.bias\": 256,\n  \"transformer.decoder.layers.3.cross_attn.attention_weights.weight\": 32768,\n  \"transformer.decoder.layers.3.cross_attn.attention_weights.bias\": 128,\n  \"transformer.decoder.layers.3.cross_attn.value_proj.weight\": 65536,\n  \"transformer.decoder.layers.3.cross_attn.value_proj.bias\": 256,\n  \"transformer.decoder.layers.3.cross_attn.output_proj.weight\": 65536,\n  \"transformer.decoder.layers.3.cross_attn.output_proj.bias\": 256,\n  \"transformer.decoder.layers.3.norm1.weight\": 256,\n  \"transformer.decoder.layers.3.norm1.bias\": 256,\n  \"transformer.decoder.layers.3.self_attn.in_proj_weight\": 196608,\n  \"transformer.decoder.layers.3.self_attn.in_proj_bias\": 768,\n  \"transformer.decoder.layers.3.self_attn.out_proj.weight\": 65536,\n  \"transformer.decoder.layers.3.self_attn.out_proj.bias\": 256,\n  \"transformer.decoder.layers.3.norm2.weight\": 256,\n  \"transformer.decoder.layers.3.norm2.bias\": 256,\n  \"transformer.decoder.layers.3.linear1.weight\": 524288,\n  \"transformer.decoder.layers.3.linear1.bias\": 2048,\n  \"transformer.decoder.layers.3.linear2.weight\": 524288,\n  \"transformer.decoder.layers.3.linear2.bias\": 256,\n  \"transformer.decoder.layers.3.norm3.weight\": 256,\n  \"transformer.decoder.layers.3.norm3.bias\": 256,\n  \"transformer.decoder.layers.4.cross_attn.sampling_offsets.weight\": 65536,\n  \"transformer.decoder.layers.4.cross_attn.sampling_offsets.bias\": 256,\n  \"transformer.decoder.layers.4.cross_attn.attention_weights.weight\": 32768,\n  \"transformer.decoder.layers.4.cross_attn.attention_weights.bias\": 128,\n  \"transformer.decoder.layers.4.cross_attn.value_proj.weight\": 65536,\n  \"transformer.decoder.layers.4.cross_attn.value_proj.bias\": 256,\n  \"transformer.decoder.layers.4.cross_attn.output_proj.weight\": 65536,\n  \"transformer.decoder.layers.4.cross_attn.output_proj.bias\": 256,\n  \"transformer.decoder.layers.4.norm1.weight\": 256,\n  \"transformer.decoder.layers.4.norm1.bias\": 256,\n  \"transformer.decoder.layers.4.self_attn.in_proj_weight\": 196608,\n  \"transformer.decoder.layers.4.self_attn.in_proj_bias\": 768,\n  \"transformer.decoder.layers.4.self_attn.out_proj.weight\": 65536,\n  \"transformer.decoder.layers.4.self_attn.out_proj.bias\": 256,\n  \"transformer.decoder.layers.4.norm2.weight\": 256,\n  \"transformer.decoder.layers.4.norm2.bias\": 256,\n  \"transformer.decoder.layers.4.linear1.weight\": 524288,\n  \"transformer.decoder.layers.4.linear1.bias\": 2048,\n  \"transformer.decoder.layers.4.linear2.weight\": 524288,\n  \"transformer.decoder.layers.4.linear2.bias\": 256,\n  \"transformer.decoder.layers.4.norm3.weight\": 256,\n  \"transformer.decoder.layers.4.norm3.bias\": 256,\n  \"transformer.decoder.layers.5.cross_attn.sampling_offsets.weight\": 65536,\n  \"transformer.decoder.layers.5.cross_attn.sampling_offsets.bias\": 256,\n  \"transformer.decoder.layers.5.cross_attn.attention_weights.weight\": 32768,\n  \"transformer.decoder.layers.5.cross_attn.attention_weights.bias\": 128,\n  \"transformer.decoder.layers.5.cross_attn.value_proj.weight\": 65536,\n  \"transformer.decoder.layers.5.cross_attn.value_proj.bias\": 256,\n  \"transformer.decoder.layers.5.cross_attn.output_proj.weight\": 65536,\n  \"transformer.decoder.layers.5.cross_attn.output_proj.bias\": 256,\n  \"transformer.decoder.layers.5.norm1.weight\": 256,\n  \"transformer.decoder.layers.5.norm1.bias\": 256,\n  \"transformer.decoder.layers.5.self_attn.in_proj_weight\": 196608,\n  \"transformer.decoder.layers.5.self_attn.in_proj_bias\": 768,\n  \"transformer.decoder.layers.5.self_attn.out_proj.weight\": 65536,\n  \"transformer.decoder.layers.5.self_attn.out_proj.bias\": 256,\n  \"transformer.decoder.layers.5.norm2.weight\": 256,\n  \"transformer.decoder.layers.5.norm2.bias\": 256,\n  \"transformer.decoder.layers.5.linear1.weight\": 524288,\n  \"transformer.decoder.layers.5.linear1.bias\": 2048,\n  \"transformer.decoder.layers.5.linear2.weight\": 524288,\n  \"transformer.decoder.layers.5.linear2.bias\": 256,\n  \"transformer.decoder.layers.5.norm3.weight\": 256,\n  \"transformer.decoder.layers.5.norm3.bias\": 256,\n  \"transformer.decoder.norm.weight\": 256,\n  \"transformer.decoder.norm.bias\": 256,\n  \"transformer.decoder.ref_point_head.layers.0.weight\": 131072,\n  \"transformer.decoder.ref_point_head.layers.0.bias\": 256,\n  \"transformer.decoder.ref_point_head.layers.1.weight\": 65536,\n  \"transformer.decoder.ref_point_head.layers.1.bias\": 256,\n  \"transformer.decoder.bbox_embed.0.layers.0.weight\": 65536,\n  \"transformer.decoder.bbox_embed.0.layers.0.bias\": 256,\n  \"transformer.decoder.bbox_embed.0.layers.1.weight\": 65536,\n  \"transformer.decoder.bbox_embed.0.layers.1.bias\": 256,\n  \"transformer.decoder.bbox_embed.0.layers.2.weight\": 1024,\n  \"transformer.decoder.bbox_embed.0.layers.2.bias\": 4,\n  \"transformer.decoder.class_embed.0.weight\": 23296,\n  \"transformer.decoder.class_embed.0.bias\": 91,\n  \"transformer.tgt_embed.weight\": 230400,\n  \"transformer.enc_output.weight\": 65536,\n  \"transformer.enc_output.bias\": 256,\n  \"transformer.enc_output_norm.weight\": 256,\n  \"transformer.enc_output_norm.bias\": 256,\n  \"transformer.enc_out_bbox_embed.layers.0.weight\": 65536,\n  \"transformer.enc_out_bbox_embed.layers.0.bias\": 256,\n  \"transformer.enc_out_bbox_embed.layers.1.weight\": 65536,\n  \"transformer.enc_out_bbox_embed.layers.1.bias\": 256,\n  \"transformer.enc_out_bbox_embed.layers.2.weight\": 1024,\n  \"transformer.enc_out_bbox_embed.layers.2.bias\": 4,\n  \"transformer.enc_out_class_embed.weight\": 23296,\n  \"transformer.enc_out_class_embed.bias\": 91,\n  \"label_enc.weight\": 23552,\n  \"input_proj.0.0.weight\": 131072,\n  \"input_proj.0.0.bias\": 256,\n  \"input_proj.0.1.weight\": 256,\n  \"input_proj.0.1.bias\": 256,\n  \"input_proj.1.0.weight\": 262144,\n  \"input_proj.1.0.bias\": 256,\n  \"input_proj.1.1.weight\": 256,\n  \"input_proj.1.1.bias\": 256,\n  \"input_proj.2.0.weight\": 524288,\n  \"input_proj.2.0.bias\": 256,\n  \"input_proj.2.1.weight\": 256,\n  \"input_proj.2.1.bias\": 256,\n  \"input_proj.3.0.weight\": 4718592,\n  \"input_proj.3.0.bias\": 256,\n  \"input_proj.3.1.weight\": 256,\n  \"input_proj.3.1.bias\": 256,\n  \"backbone.0.body.layer2.0.conv1.weight\": 32768,\n  \"backbone.0.body.layer2.0.conv2.weight\": 147456,\n  \"backbone.0.body.layer2.0.conv3.weight\": 65536,\n  \"backbone.0.body.layer2.0.downsample.0.weight\": 131072,\n  \"backbone.0.body.layer2.1.conv1.weight\": 65536,\n  \"backbone.0.body.layer2.1.conv2.weight\": 147456,\n  \"backbone.0.body.layer2.1.conv3.weight\": 65536,\n  \"backbone.0.body.layer2.2.conv1.weight\": 65536,\n  \"backbone.0.body.layer2.2.conv2.weight\": 147456,\n  \"backbone.0.body.layer2.2.conv3.weight\": 65536,\n  \"backbone.0.body.layer2.3.conv1.weight\": 65536,\n  \"backbone.0.body.layer2.3.conv2.weight\": 147456,\n  \"backbone.0.body.layer2.3.conv3.weight\": 65536,\n  \"backbone.0.body.layer3.0.conv1.weight\": 131072,\n  \"backbone.0.body.layer3.0.conv2.weight\": 589824,\n  \"backbone.0.body.layer3.0.conv3.weight\": 262144,\n  \"backbone.0.body.layer3.0.downsample.0.weight\": 524288,\n  \"backbone.0.body.layer3.1.conv1.weight\": 262144,\n  \"backbone.0.body.layer3.1.conv2.weight\": 589824,\n  \"backbone.0.body.layer3.1.conv3.weight\": 262144,\n  \"backbone.0.body.layer3.2.conv1.weight\": 262144,\n  \"backbone.0.body.layer3.2.conv2.weight\": 589824,\n  \"backbone.0.body.layer3.2.conv3.weight\": 262144,\n  \"backbone.0.body.layer3.3.conv1.weight\": 262144,\n  \"backbone.0.body.layer3.3.conv2.weight\": 589824,\n  \"backbone.0.body.layer3.3.conv3.weight\": 262144,\n  \"backbone.0.body.layer3.4.conv1.weight\": 262144,\n  \"backbone.0.body.layer3.4.conv2.weight\": 589824,\n  \"backbone.0.body.layer3.4.conv3.weight\": 262144,\n  \"backbone.0.body.layer3.5.conv1.weight\": 262144,\n  \"backbone.0.body.layer3.5.conv2.weight\": 589824,\n  \"backbone.0.body.layer3.5.conv3.weight\": 262144,\n  \"backbone.0.body.layer4.0.conv1.weight\": 524288,\n  \"backbone.0.body.layer4.0.conv2.weight\": 2359296,\n  \"backbone.0.body.layer4.0.conv3.weight\": 1048576,\n  \"backbone.0.body.layer4.0.downsample.0.weight\": 2097152,\n  \"backbone.0.body.layer4.1.conv1.weight\": 1048576,\n  \"backbone.0.body.layer4.1.conv2.weight\": 2359296,\n  \"backbone.0.body.layer4.1.conv3.weight\": 1048576,\n  \"backbone.0.body.layer4.2.conv1.weight\": 1048576,\n  \"backbone.0.body.layer4.2.conv2.weight\": 2359296,\n  \"backbone.0.body.layer4.2.conv3.weight\": 1048576\n}\ndata_aug_params: {\n  \"scales\": [\n    480,\n    512,\n    544,\n    576,\n    608,\n    640,\n    672,\n    704,\n    736,\n    768,\n    800\n  ],\n  \"max_size\": 1333,\n  \"scales2_resize\": [\n    400,\n    500,\n    600\n  ],\n  \"scales2_crop\": [\n    384,\n    600\n  ]\n}\nloading annotations into memory...\nDone (t=0.02s)\ncreating index...\nindex created!\ndata_aug_params: {\n  \"scales\": [\n    480,\n    512,\n    544,\n    576,\n    608,\n    640,\n    672,\n    704,\n    736,\n    768,\n    800\n  ],\n  \"max_size\": 1333,\n  \"scales2_resize\": [\n    400,\n    500,\n    600\n  ],\n  \"scales2_crop\": [\n    384,\n    600\n  ]\n}\nloading annotations into memory...\nDone (t=0.00s)\ncreating index...\nindex created!\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/kaggle/working/DINO/main.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(args.resume, map_location='cpu')\n/kaggle/working/DINO/engine.py:164: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=args.amp):\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3609.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\nTest:  [ 0/40]  eta: 0:00:57  class_error: 0.00  loss: 6.7265 (6.7265)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.3384 (0.3384)  loss_bbox: 0.0663 (0.0663)  loss_giou: 0.6159 (0.6159)  loss_ce_0: 0.3945 (0.3945)  loss_bbox_0: 0.0489 (0.0489)  loss_giou_0: 0.5625 (0.5625)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.3912 (0.3912)  loss_bbox_1: 0.0471 (0.0471)  loss_giou_1: 0.5540 (0.5540)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.2852 (0.2852)  loss_bbox_2: 0.0491 (0.0491)  loss_giou_2: 0.5614 (0.5614)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.2884 (0.2884)  loss_bbox_3: 0.0564 (0.0564)  loss_giou_3: 0.6038 (0.6038)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.3058 (0.3058)  loss_bbox_4: 0.0671 (0.0671)  loss_giou_4: 0.6155 (0.6155)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.3732 (0.3732)  loss_bbox_interm: 0.0365 (0.0365)  loss_giou_interm: 0.4653 (0.4653)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.3384 (0.3384)  class_error_unscaled: 0.0000 (0.0000)  loss_bbox_unscaled: 0.0133 (0.0133)  loss_giou_unscaled: 0.3080 (0.3080)  loss_xy_unscaled: 0.0037 (0.0037)  loss_hw_unscaled: 0.0096 (0.0096)  cardinality_error_unscaled: 898.0000 (898.0000)  loss_ce_0_unscaled: 0.3945 (0.3945)  loss_bbox_0_unscaled: 0.0098 (0.0098)  loss_giou_0_unscaled: 0.2812 (0.2812)  loss_xy_0_unscaled: 0.0020 (0.0020)  loss_hw_0_unscaled: 0.0078 (0.0078)  cardinality_error_0_unscaled: 898.0000 (898.0000)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.3912 (0.3912)  loss_bbox_1_unscaled: 0.0094 (0.0094)  loss_giou_1_unscaled: 0.2770 (0.2770)  loss_xy_1_unscaled: 0.0023 (0.0023)  loss_hw_1_unscaled: 0.0072 (0.0072)  cardinality_error_1_unscaled: 898.0000 (898.0000)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.2852 (0.2852)  loss_bbox_2_unscaled: 0.0098 (0.0098)  loss_giou_2_unscaled: 0.2807 (0.2807)  loss_xy_2_unscaled: 0.0026 (0.0026)  loss_hw_2_unscaled: 0.0072 (0.0072)  cardinality_error_2_unscaled: 898.0000 (898.0000)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.2884 (0.2884)  loss_bbox_3_unscaled: 0.0113 (0.0113)  loss_giou_3_unscaled: 0.3019 (0.3019)  loss_xy_3_unscaled: 0.0025 (0.0025)  loss_hw_3_unscaled: 0.0088 (0.0088)  cardinality_error_3_unscaled: 898.0000 (898.0000)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.3058 (0.3058)  loss_bbox_4_unscaled: 0.0134 (0.0134)  loss_giou_4_unscaled: 0.3078 (0.3078)  loss_xy_4_unscaled: 0.0036 (0.0036)  loss_hw_4_unscaled: 0.0098 (0.0098)  cardinality_error_4_unscaled: 898.0000 (898.0000)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.3732 (0.3732)  loss_bbox_interm_unscaled: 0.0073 (0.0073)  loss_giou_interm_unscaled: 0.2327 (0.2327)  loss_xy_interm_unscaled: 0.0017 (0.0017)  loss_hw_interm_unscaled: 0.0056 (0.0056)  cardinality_error_interm_unscaled: 898.0000 (898.0000)  time: 1.4286  data: 0.3542  max mem: 647\nTest:  [10/40]  eta: 0:00:09  class_error: 0.00  loss: 5.1628 (5.4984)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.1484 (0.1741)  loss_bbox: 0.0697 (0.0965)  loss_giou: 0.4837 (0.5343)  loss_ce_0: 0.1492 (0.1588)  loss_bbox_0: 0.0729 (0.0969)  loss_giou_0: 0.4805 (0.5273)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.1501 (0.1798)  loss_bbox_1: 0.0726 (0.0884)  loss_giou_1: 0.4726 (0.5172)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.1492 (0.1705)  loss_bbox_2: 0.0709 (0.0876)  loss_giou_2: 0.4764 (0.5134)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.1418 (0.1609)  loss_bbox_3: 0.0698 (0.0951)  loss_giou_3: 0.4837 (0.5300)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.1642 (0.1780)  loss_bbox_4: 0.0696 (0.0901)  loss_giou_4: 0.4833 (0.5249)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.1698 (0.1693)  loss_bbox_interm: 0.0714 (0.0910)  loss_giou_interm: 0.4653 (0.5142)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.1484 (0.1741)  class_error_unscaled: 0.0000 (0.3953)  loss_bbox_unscaled: 0.0139 (0.0193)  loss_giou_unscaled: 0.2419 (0.2671)  loss_xy_unscaled: 0.0044 (0.0061)  loss_hw_unscaled: 0.0095 (0.0132)  cardinality_error_unscaled: 895.0000 (891.9091)  loss_ce_0_unscaled: 0.1492 (0.1588)  loss_bbox_0_unscaled: 0.0146 (0.0194)  loss_giou_0_unscaled: 0.2402 (0.2637)  loss_xy_0_unscaled: 0.0049 (0.0061)  loss_hw_0_unscaled: 0.0094 (0.0133)  cardinality_error_0_unscaled: 895.0000 (891.9091)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.1501 (0.1798)  loss_bbox_1_unscaled: 0.0145 (0.0177)  loss_giou_1_unscaled: 0.2363 (0.2586)  loss_xy_1_unscaled: 0.0045 (0.0056)  loss_hw_1_unscaled: 0.0093 (0.0121)  cardinality_error_1_unscaled: 895.0000 (891.9091)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.1492 (0.1705)  loss_bbox_2_unscaled: 0.0142 (0.0175)  loss_giou_2_unscaled: 0.2382 (0.2567)  loss_xy_2_unscaled: 0.0045 (0.0056)  loss_hw_2_unscaled: 0.0094 (0.0119)  cardinality_error_2_unscaled: 895.0000 (891.9091)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.1418 (0.1609)  loss_bbox_3_unscaled: 0.0140 (0.0190)  loss_giou_3_unscaled: 0.2419 (0.2650)  loss_xy_3_unscaled: 0.0044 (0.0059)  loss_hw_3_unscaled: 0.0095 (0.0131)  cardinality_error_3_unscaled: 895.0000 (891.9091)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.1642 (0.1780)  loss_bbox_4_unscaled: 0.0139 (0.0180)  loss_giou_4_unscaled: 0.2416 (0.2625)  loss_xy_4_unscaled: 0.0044 (0.0058)  loss_hw_4_unscaled: 0.0095 (0.0122)  cardinality_error_4_unscaled: 895.0000 (891.9091)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.1698 (0.1693)  loss_bbox_interm_unscaled: 0.0143 (0.0182)  loss_giou_interm_unscaled: 0.2327 (0.2571)  loss_xy_interm_unscaled: 0.0050 (0.0058)  loss_hw_interm_unscaled: 0.0089 (0.0124)  cardinality_error_interm_unscaled: 895.0000 (891.9091)  time: 0.3121  data: 0.0402  max mem: 649\nTest:  [20/40]  eta: 0:00:05  class_error: 0.00  loss: 5.4758 (5.7271)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.1718 (0.2236)  loss_bbox: 0.0766 (0.1121)  loss_giou: 0.4715 (0.5038)  loss_ce_0: 0.1592 (0.1864)  loss_bbox_0: 0.0945 (0.1137)  loss_giou_0: 0.4680 (0.4989)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.1853 (0.2130)  loss_bbox_1: 0.0953 (0.1090)  loss_giou_1: 0.4726 (0.4958)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.1730 (0.2099)  loss_bbox_2: 0.0981 (0.1095)  loss_giou_2: 0.4686 (0.4977)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.1612 (0.2101)  loss_bbox_3: 0.0980 (0.1133)  loss_giou_3: 0.4728 (0.5068)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.1787 (0.2224)  loss_bbox_4: 0.0765 (0.1087)  loss_giou_4: 0.4716 (0.4989)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.1736 (0.2000)  loss_bbox_interm: 0.0793 (0.1062)  loss_giou_interm: 0.4716 (0.4874)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.1718 (0.2236)  class_error_unscaled: 0.0000 (0.2070)  loss_bbox_unscaled: 0.0153 (0.0224)  loss_giou_unscaled: 0.2358 (0.2519)  loss_xy_unscaled: 0.0051 (0.0071)  loss_hw_unscaled: 0.0104 (0.0153)  cardinality_error_unscaled: 894.0000 (892.1429)  loss_ce_0_unscaled: 0.1592 (0.1864)  loss_bbox_0_unscaled: 0.0189 (0.0227)  loss_giou_0_unscaled: 0.2340 (0.2495)  loss_xy_0_unscaled: 0.0060 (0.0072)  loss_hw_0_unscaled: 0.0120 (0.0156)  cardinality_error_0_unscaled: 894.0000 (892.1429)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.1853 (0.2130)  loss_bbox_1_unscaled: 0.0191 (0.0218)  loss_giou_1_unscaled: 0.2363 (0.2479)  loss_xy_1_unscaled: 0.0057 (0.0069)  loss_hw_1_unscaled: 0.0123 (0.0149)  cardinality_error_1_unscaled: 894.0000 (892.1429)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.1730 (0.2099)  loss_bbox_2_unscaled: 0.0196 (0.0219)  loss_giou_2_unscaled: 0.2343 (0.2488)  loss_xy_2_unscaled: 0.0061 (0.0070)  loss_hw_2_unscaled: 0.0127 (0.0149)  cardinality_error_2_unscaled: 894.0000 (892.1429)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.1612 (0.2101)  loss_bbox_3_unscaled: 0.0196 (0.0227)  loss_giou_3_unscaled: 0.2364 (0.2534)  loss_xy_3_unscaled: 0.0059 (0.0072)  loss_hw_3_unscaled: 0.0128 (0.0155)  cardinality_error_3_unscaled: 894.0000 (892.1429)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.1787 (0.2224)  loss_bbox_4_unscaled: 0.0153 (0.0217)  loss_giou_4_unscaled: 0.2358 (0.2495)  loss_xy_4_unscaled: 0.0051 (0.0069)  loss_hw_4_unscaled: 0.0104 (0.0148)  cardinality_error_4_unscaled: 894.0000 (892.1429)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.1736 (0.2000)  loss_bbox_interm_unscaled: 0.0159 (0.0212)  loss_giou_interm_unscaled: 0.2358 (0.2437)  loss_xy_interm_unscaled: 0.0061 (0.0070)  loss_hw_interm_unscaled: 0.0103 (0.0143)  cardinality_error_interm_unscaled: 894.0000 (892.1429)  time: 0.1967  data: 0.0069  max mem: 649\nTest:  [30/40]  eta: 0:00:02  class_error: 0.00  loss: 6.2298 (6.3722)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.2845 (0.2844)  loss_bbox: 0.0961 (0.1207)  loss_giou: 0.5092 (0.5291)  loss_ce_0: 0.1924 (0.2462)  loss_bbox_0: 0.0945 (0.1216)  loss_giou_0: 0.5089 (0.5263)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.1993 (0.2738)  loss_bbox_1: 0.0953 (0.1181)  loss_giou_1: 0.5122 (0.5241)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.2059 (0.2673)  loss_bbox_2: 0.0981 (0.1184)  loss_giou_2: 0.5220 (0.5241)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.2311 (0.2635)  loss_bbox_3: 0.0980 (0.1211)  loss_giou_3: 0.5259 (0.5313)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.2520 (0.2820)  loss_bbox_4: 0.0958 (0.1179)  loss_giou_4: 0.5087 (0.5257)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.1894 (0.2424)  loss_bbox_interm: 0.0888 (0.1159)  loss_giou_interm: 0.4922 (0.5183)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.2845 (0.2844)  class_error_unscaled: 0.0000 (0.1403)  loss_bbox_unscaled: 0.0192 (0.0241)  loss_giou_unscaled: 0.2546 (0.2645)  loss_xy_unscaled: 0.0054 (0.0077)  loss_hw_unscaled: 0.0135 (0.0164)  cardinality_error_unscaled: 893.0000 (892.6452)  loss_ce_0_unscaled: 0.1924 (0.2462)  loss_bbox_0_unscaled: 0.0189 (0.0243)  loss_giou_0_unscaled: 0.2545 (0.2631)  loss_xy_0_unscaled: 0.0057 (0.0077)  loss_hw_0_unscaled: 0.0131 (0.0166)  cardinality_error_0_unscaled: 893.0000 (892.6452)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.1993 (0.2738)  loss_bbox_1_unscaled: 0.0191 (0.0236)  loss_giou_1_unscaled: 0.2561 (0.2620)  loss_xy_1_unscaled: 0.0059 (0.0077)  loss_hw_1_unscaled: 0.0133 (0.0159)  cardinality_error_1_unscaled: 893.0000 (892.6452)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.2059 (0.2673)  loss_bbox_2_unscaled: 0.0196 (0.0237)  loss_giou_2_unscaled: 0.2610 (0.2621)  loss_xy_2_unscaled: 0.0061 (0.0077)  loss_hw_2_unscaled: 0.0135 (0.0160)  cardinality_error_2_unscaled: 893.0000 (892.6452)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.2311 (0.2635)  loss_bbox_3_unscaled: 0.0196 (0.0242)  loss_giou_3_unscaled: 0.2630 (0.2657)  loss_xy_3_unscaled: 0.0059 (0.0078)  loss_hw_3_unscaled: 0.0136 (0.0164)  cardinality_error_3_unscaled: 893.0000 (892.6452)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.2520 (0.2820)  loss_bbox_4_unscaled: 0.0192 (0.0236)  loss_giou_4_unscaled: 0.2544 (0.2629)  loss_xy_4_unscaled: 0.0056 (0.0077)  loss_hw_4_unscaled: 0.0135 (0.0159)  cardinality_error_4_unscaled: 893.0000 (892.6452)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.1894 (0.2424)  loss_bbox_interm_unscaled: 0.0178 (0.0232)  loss_giou_interm_unscaled: 0.2461 (0.2591)  loss_xy_interm_unscaled: 0.0061 (0.0075)  loss_hw_interm_unscaled: 0.0124 (0.0156)  cardinality_error_interm_unscaled: 893.0000 (892.6452)  time: 0.1900  data: 0.0055  max mem: 649\nTest:  [39/40]  eta: 0:00:00  class_error: 0.00  loss: 6.7759 (6.6134)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.2876 (0.3399)  loss_bbox: 0.0840 (0.1194)  loss_giou: 0.5348 (0.5242)  loss_ce_0: 0.2038 (0.2800)  loss_bbox_0: 0.0838 (0.1203)  loss_giou_0: 0.5119 (0.5216)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.2239 (0.3132)  loss_bbox_1: 0.0834 (0.1168)  loss_giou_1: 0.5268 (0.5182)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.2097 (0.3122)  loss_bbox_2: 0.0847 (0.1180)  loss_giou_2: 0.5220 (0.5206)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.2540 (0.3096)  loss_bbox_3: 0.0843 (0.1203)  loss_giou_3: 0.5346 (0.5260)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.2314 (0.3370)  loss_bbox_4: 0.0835 (0.1142)  loss_giou_4: 0.4993 (0.5188)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.1894 (0.2527)  loss_bbox_interm: 0.0828 (0.1159)  loss_giou_interm: 0.5093 (0.5147)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.2876 (0.3399)  class_error_unscaled: 0.0000 (1.2992)  loss_bbox_unscaled: 0.0168 (0.0239)  loss_giou_unscaled: 0.2674 (0.2621)  loss_xy_unscaled: 0.0052 (0.0076)  loss_hw_unscaled: 0.0126 (0.0162)  cardinality_error_unscaled: 893.0000 (892.9250)  loss_ce_0_unscaled: 0.2038 (0.2800)  loss_bbox_0_unscaled: 0.0168 (0.0241)  loss_giou_0_unscaled: 0.2560 (0.2608)  loss_xy_0_unscaled: 0.0046 (0.0075)  loss_hw_0_unscaled: 0.0125 (0.0166)  cardinality_error_0_unscaled: 893.0000 (892.9250)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.2239 (0.3132)  loss_bbox_1_unscaled: 0.0167 (0.0234)  loss_giou_1_unscaled: 0.2634 (0.2591)  loss_xy_1_unscaled: 0.0052 (0.0075)  loss_hw_1_unscaled: 0.0125 (0.0159)  cardinality_error_1_unscaled: 893.0000 (892.9250)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.2097 (0.3122)  loss_bbox_2_unscaled: 0.0169 (0.0236)  loss_giou_2_unscaled: 0.2610 (0.2603)  loss_xy_2_unscaled: 0.0052 (0.0076)  loss_hw_2_unscaled: 0.0127 (0.0160)  cardinality_error_2_unscaled: 893.0000 (892.9250)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.2540 (0.3096)  loss_bbox_3_unscaled: 0.0169 (0.0241)  loss_giou_3_unscaled: 0.2673 (0.2630)  loss_xy_3_unscaled: 0.0051 (0.0077)  loss_hw_3_unscaled: 0.0126 (0.0164)  cardinality_error_3_unscaled: 893.0000 (892.9250)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.2314 (0.3370)  loss_bbox_4_unscaled: 0.0167 (0.0228)  loss_giou_4_unscaled: 0.2497 (0.2594)  loss_xy_4_unscaled: 0.0048 (0.0074)  loss_hw_4_unscaled: 0.0125 (0.0154)  cardinality_error_4_unscaled: 893.0000 (892.9250)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.1894 (0.2527)  loss_bbox_interm_unscaled: 0.0166 (0.0232)  loss_giou_interm_unscaled: 0.2546 (0.2573)  loss_xy_interm_unscaled: 0.0046 (0.0075)  loss_hw_interm_unscaled: 0.0122 (0.0157)  cardinality_error_interm_unscaled: 893.0000 (892.9250)  time: 0.1874  data: 0.0063  max mem: 649\nTest: Total time: 0:00:09 (0.2269 s / it)\nAveraged stats: class_error: 0.00  loss: 6.7759 (6.6134)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.2876 (0.3399)  loss_bbox: 0.0840 (0.1194)  loss_giou: 0.5348 (0.5242)  loss_ce_0: 0.2038 (0.2800)  loss_bbox_0: 0.0838 (0.1203)  loss_giou_0: 0.5119 (0.5216)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.2239 (0.3132)  loss_bbox_1: 0.0834 (0.1168)  loss_giou_1: 0.5268 (0.5182)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.2097 (0.3122)  loss_bbox_2: 0.0847 (0.1180)  loss_giou_2: 0.5220 (0.5206)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.2540 (0.3096)  loss_bbox_3: 0.0843 (0.1203)  loss_giou_3: 0.5346 (0.5260)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.2314 (0.3370)  loss_bbox_4: 0.0835 (0.1142)  loss_giou_4: 0.4993 (0.5188)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.1894 (0.2527)  loss_bbox_interm: 0.0828 (0.1159)  loss_giou_interm: 0.5093 (0.5147)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.2876 (0.3399)  class_error_unscaled: 0.0000 (1.2992)  loss_bbox_unscaled: 0.0168 (0.0239)  loss_giou_unscaled: 0.2674 (0.2621)  loss_xy_unscaled: 0.0052 (0.0076)  loss_hw_unscaled: 0.0126 (0.0162)  cardinality_error_unscaled: 893.0000 (892.9250)  loss_ce_0_unscaled: 0.2038 (0.2800)  loss_bbox_0_unscaled: 0.0168 (0.0241)  loss_giou_0_unscaled: 0.2560 (0.2608)  loss_xy_0_unscaled: 0.0046 (0.0075)  loss_hw_0_unscaled: 0.0125 (0.0166)  cardinality_error_0_unscaled: 893.0000 (892.9250)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.2239 (0.3132)  loss_bbox_1_unscaled: 0.0167 (0.0234)  loss_giou_1_unscaled: 0.2634 (0.2591)  loss_xy_1_unscaled: 0.0052 (0.0075)  loss_hw_1_unscaled: 0.0125 (0.0159)  cardinality_error_1_unscaled: 893.0000 (892.9250)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.2097 (0.3122)  loss_bbox_2_unscaled: 0.0169 (0.0236)  loss_giou_2_unscaled: 0.2610 (0.2603)  loss_xy_2_unscaled: 0.0052 (0.0076)  loss_hw_2_unscaled: 0.0127 (0.0160)  cardinality_error_2_unscaled: 893.0000 (892.9250)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.2540 (0.3096)  loss_bbox_3_unscaled: 0.0169 (0.0241)  loss_giou_3_unscaled: 0.2673 (0.2630)  loss_xy_3_unscaled: 0.0051 (0.0077)  loss_hw_3_unscaled: 0.0126 (0.0164)  cardinality_error_3_unscaled: 893.0000 (892.9250)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.2314 (0.3370)  loss_bbox_4_unscaled: 0.0167 (0.0228)  loss_giou_4_unscaled: 0.2497 (0.2594)  loss_xy_4_unscaled: 0.0048 (0.0074)  loss_hw_4_unscaled: 0.0125 (0.0154)  cardinality_error_4_unscaled: 893.0000 (892.9250)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.1894 (0.2527)  loss_bbox_interm_unscaled: 0.0166 (0.0232)  loss_giou_interm_unscaled: 0.2546 (0.2573)  loss_xy_interm_unscaled: 0.0046 (0.0075)  loss_hw_interm_unscaled: 0.0122 (0.0157)  cardinality_error_interm_unscaled: 893.0000 (892.9250)\nAccumulating evaluation results...\nTraceback (most recent call last):\n  File \"/kaggle/working/DINO/main.py\", line 388, in <module>\n    main(args)\n  File \"/kaggle/working/DINO/main.py\", line 255, in main\n    test_stats, coco_evaluator = evaluate(model, criterion, postprocessors,\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/kaggle/working/DINO/engine.py\", line 278, in evaluate\n    coco_evaluator.accumulate()\n  File \"/kaggle/working/DINO/datasets/coco_eval.py\", line 65, in accumulate\n    coco_eval.accumulate()\n  File \"/opt/conda/lib/python3.10/site-packages/pycocotools/cocoeval.py\", line 378, in accumulate\n    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n  File \"/opt/conda/lib/python3.10/site-packages/numpy/__init__.py\", line 324, in __getattr__\n    raise AttributeError(__former_attrs__[attr])\nAttributeError: module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'cfloat'?\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-09-24T18:15:10.300181Z","iopub.execute_input":"2024-09-24T18:15:10.300967Z","iopub.status.idle":"2024-09-24T18:15:10.305997Z","shell.execute_reply.started":"2024-09-24T18:15:10.300921Z","shell.execute_reply":"2024-09-24T18:15:10.304919Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-09-24T17:53:25.230694Z","iopub.execute_input":"2024-09-24T17:53:25.231716Z","iopub.status.idle":"2024-09-24T17:53:25.238427Z","shell.execute_reply.started":"2024-09-24T17:53:25.231657Z","shell.execute_reply":"2024-09-24T17:53:25.237473Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"/kaggle/working/DINO/models/dino\n","output_type":"stream"}]},{"cell_type":"code","source":"\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T18:07:24.114733Z","iopub.execute_input":"2024-09-24T18:07:24.115413Z","iopub.status.idle":"2024-09-24T18:07:25.103998Z","shell.execute_reply.started":"2024-09-24T18:07:24.115369Z","shell.execute_reply":"2024-09-24T18:07:25.102934Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"/bin/bash: line 1: nano: command not found\n","output_type":"stream"}]},{"cell_type":"code","source":"!bash scripts/DINO_train.sh /kaggle/input/dreamdelhi/COCODIR/COCODIR","metadata":{"execution":{"iopub.status.busy":"2024-09-24T18:44:48.191208Z","iopub.execute_input":"2024-09-24T18:44:48.191613Z","iopub.status.idle":"2024-09-24T18:45:57.940556Z","shell.execute_reply.started":"2024-09-24T18:44:48.191574Z","shell.execute_reply":"2024-09-24T18:45:57.939369Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Not using distributed mode\nLoading config file from config/DINO/DINO_4scale.py\n[09/24 18:44:52.352]: git:\n  sha: d84a491d41898b3befd8294d1cf2614661fc0953, status: clean, branch: main\n\n[09/24 18:44:52.352]: Command: main.py --output_dir logs/DINO/R50-MS4 -c config/DINO/DINO_4scale.py --coco_path /kaggle/input/dreamdelhi/COCODIR/COCODIR --options dn_scalar=100 embed_init_tgt=TRUE dn_label_coef=1.0 dn_bbox_coef=1.0 use_ema=False dn_box_noise_scale=1.0\n[09/24 18:44:52.353]: Full config saved to logs/DINO/R50-MS4/config_args_all.json\n[09/24 18:44:52.353]: world size: 1\n[09/24 18:44:52.353]: rank: 0\n[09/24 18:44:52.353]: local_rank: 0\n[09/24 18:44:52.353]: args: Namespace(config_file='config/DINO/DINO_4scale.py', options={'dn_scalar': 100, 'embed_init_tgt': True, 'dn_label_coef': 1.0, 'dn_bbox_coef': 1.0, 'use_ema': False, 'dn_box_noise_scale': 1.0}, dataset_file='coco', coco_path='/kaggle/input/dreamdelhi/COCODIR/COCODIR', coco_panoptic_path=None, remove_difficult=False, fix_size=False, output_dir='logs/DINO/R50-MS4', note='', device='cuda', seed=42, resume='', pretrain_model_path=None, finetune_ignore=None, start_epoch=0, eval=False, num_workers=10, test=False, debug=False, find_unused_params=False, save_results=False, save_log=False, world_size=1, dist_url='env://', rank=0, local_rank=0, amp=False, distributed=False, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_max_size=1333, data_aug_scales2_resize=[400, 500, 600], data_aug_scales2_crop=[384, 600], data_aug_scale_overlap=None, num_classes=91, lr=0.0001, param_dict_type='default', lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, ddetr_lr_param=False, batch_size=2, weight_decay=0.0001, epochs=12, lr_drop=11, save_checkpoint_interval=1, clip_max_norm=0.1, onecyclelr=False, multi_step_lr=False, lr_drop_list=[33, 45], modelname='dino', frozen_weights=None, backbone='resnet50', use_checkpoint=False, dilation=False, position_embedding='sine', pe_temperatureH=20, pe_temperatureW=20, return_interm_indices=[1, 2, 3], backbone_freeze_keywords=None, enc_layers=6, dec_layers=6, unic_layers=0, pre_norm=False, dim_feedforward=2048, hidden_dim=256, dropout=0.0, nheads=8, num_queries=900, query_dim=4, num_patterns=0, pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, random_refpoints_xy=False, fix_refpoints_hw=-1, dabdetr_yolo_like_anchor_update=False, dabdetr_deformable_encoder=False, dabdetr_deformable_decoder=False, use_deformable_box_attn=False, box_attn_type='roi_align', dec_layer_number=None, num_feature_levels=4, enc_n_points=4, dec_n_points=4, decoder_layer_noise=False, dln_xy_noise=0.2, dln_hw_noise=0.2, add_channel_attention=False, add_pos_value=False, two_stage_type='standard', two_stage_pat_embed=0, two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_learn_wh=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, num_select=300, transformer_activation='relu', batch_norm_type='FrozenBatchNorm2d', masks=False, aux_loss=True, set_cost_class=2.0, set_cost_bbox=5.0, set_cost_giou=2.0, cls_loss_coef=1.0, mask_loss_coef=1.0, dice_loss_coef=1.0, bbox_loss_coef=5.0, giou_loss_coef=2.0, enc_loss_coef=1.0, interm_loss_coef=1.0, no_interm_box_loss=False, focal_alpha=0.25, decoder_sa_type='sa', matcher_type='HungarianMatcher', decoder_module_seq=['sa', 'ca', 'ffn'], nms_iou_threshold=-1, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, use_dn=True, dn_number=100, dn_box_noise_scale=1.0, dn_label_noise_ratio=0.5, embed_init_tgt=True, dn_labelbook_size=91, match_unstable_error=True, use_ema=False, ema_decay=0.9997, ema_epoch=0, use_detached_boxes_dec_out=False, dn_scalar=100, dn_label_coef=1.0, dn_bbox_coef=1.0)\n\nNamespace(config_file='config/DINO/DINO_4scale.py', options={'dn_scalar': 100, 'embed_init_tgt': True, 'dn_label_coef': 1.0, 'dn_bbox_coef': 1.0, 'use_ema': False, 'dn_box_noise_scale': 1.0}, dataset_file='coco', coco_path='/kaggle/input/dreamdelhi/COCODIR/COCODIR', coco_panoptic_path=None, remove_difficult=False, fix_size=False, output_dir='logs/DINO/R50-MS4', note='', device='cuda', seed=42, resume='', pretrain_model_path=None, finetune_ignore=None, start_epoch=0, eval=False, num_workers=10, test=False, debug=False, find_unused_params=False, save_results=False, save_log=False, world_size=1, dist_url='env://', rank=0, local_rank=0, amp=False, distributed=False, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_max_size=1333, data_aug_scales2_resize=[400, 500, 600], data_aug_scales2_crop=[384, 600], data_aug_scale_overlap=None, num_classes=91, lr=0.0001, param_dict_type='default', lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, ddetr_lr_param=False, batch_size=2, weight_decay=0.0001, epochs=12, lr_drop=11, save_checkpoint_interval=1, clip_max_norm=0.1, onecyclelr=False, multi_step_lr=False, lr_drop_list=[33, 45], modelname='dino', frozen_weights=None, backbone='resnet50', use_checkpoint=False, dilation=False, position_embedding='sine', pe_temperatureH=20, pe_temperatureW=20, return_interm_indices=[1, 2, 3], backbone_freeze_keywords=None, enc_layers=6, dec_layers=6, unic_layers=0, pre_norm=False, dim_feedforward=2048, hidden_dim=256, dropout=0.0, nheads=8, num_queries=900, query_dim=4, num_patterns=0, pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, random_refpoints_xy=False, fix_refpoints_hw=-1, dabdetr_yolo_like_anchor_update=False, dabdetr_deformable_encoder=False, dabdetr_deformable_decoder=False, use_deformable_box_attn=False, box_attn_type='roi_align', dec_layer_number=None, num_feature_levels=4, enc_n_points=4, dec_n_points=4, decoder_layer_noise=False, dln_xy_noise=0.2, dln_hw_noise=0.2, add_channel_attention=False, add_pos_value=False, two_stage_type='standard', two_stage_pat_embed=0, two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_learn_wh=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, num_select=300, transformer_activation='relu', batch_norm_type='FrozenBatchNorm2d', masks=False, aux_loss=True, set_cost_class=2.0, set_cost_bbox=5.0, set_cost_giou=2.0, cls_loss_coef=1.0, mask_loss_coef=1.0, dice_loss_coef=1.0, bbox_loss_coef=5.0, giou_loss_coef=2.0, enc_loss_coef=1.0, interm_loss_coef=1.0, no_interm_box_loss=False, focal_alpha=0.25, decoder_sa_type='sa', matcher_type='HungarianMatcher', decoder_module_seq=['sa', 'ca', 'ffn'], nms_iou_threshold=-1, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, use_dn=True, dn_number=100, dn_box_noise_scale=1.0, dn_label_noise_ratio=0.5, embed_init_tgt=True, dn_labelbook_size=91, match_unstable_error=True, use_ema=False, ema_decay=0.9997, ema_epoch=0, use_detached_boxes_dec_out=False, dn_scalar=100, dn_label_coef=1.0, dn_bbox_coef=1.0)\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n[09/24 18:44:54.559]: number of params:46670782\n[09/24 18:44:54.561]: params:\n{\n  \"transformer.level_embed\": 1024,\n  \"transformer.encoder.layers.0.self_attn.sampling_offsets.weight\": 65536,\n  \"transformer.encoder.layers.0.self_attn.sampling_offsets.bias\": 256,\n  \"transformer.encoder.layers.0.self_attn.attention_weights.weight\": 32768,\n  \"transformer.encoder.layers.0.self_attn.attention_weights.bias\": 128,\n  \"transformer.encoder.layers.0.self_attn.value_proj.weight\": 65536,\n  \"transformer.encoder.layers.0.self_attn.value_proj.bias\": 256,\n  \"transformer.encoder.layers.0.self_attn.output_proj.weight\": 65536,\n  \"transformer.encoder.layers.0.self_attn.output_proj.bias\": 256,\n  \"transformer.encoder.layers.0.norm1.weight\": 256,\n  \"transformer.encoder.layers.0.norm1.bias\": 256,\n  \"transformer.encoder.layers.0.linear1.weight\": 524288,\n  \"transformer.encoder.layers.0.linear1.bias\": 2048,\n  \"transformer.encoder.layers.0.linear2.weight\": 524288,\n  \"transformer.encoder.layers.0.linear2.bias\": 256,\n  \"transformer.encoder.layers.0.norm2.weight\": 256,\n  \"transformer.encoder.layers.0.norm2.bias\": 256,\n  \"transformer.encoder.layers.1.self_attn.sampling_offsets.weight\": 65536,\n  \"transformer.encoder.layers.1.self_attn.sampling_offsets.bias\": 256,\n  \"transformer.encoder.layers.1.self_attn.attention_weights.weight\": 32768,\n  \"transformer.encoder.layers.1.self_attn.attention_weights.bias\": 128,\n  \"transformer.encoder.layers.1.self_attn.value_proj.weight\": 65536,\n  \"transformer.encoder.layers.1.self_attn.value_proj.bias\": 256,\n  \"transformer.encoder.layers.1.self_attn.output_proj.weight\": 65536,\n  \"transformer.encoder.layers.1.self_attn.output_proj.bias\": 256,\n  \"transformer.encoder.layers.1.norm1.weight\": 256,\n  \"transformer.encoder.layers.1.norm1.bias\": 256,\n  \"transformer.encoder.layers.1.linear1.weight\": 524288,\n  \"transformer.encoder.layers.1.linear1.bias\": 2048,\n  \"transformer.encoder.layers.1.linear2.weight\": 524288,\n  \"transformer.encoder.layers.1.linear2.bias\": 256,\n  \"transformer.encoder.layers.1.norm2.weight\": 256,\n  \"transformer.encoder.layers.1.norm2.bias\": 256,\n  \"transformer.encoder.layers.2.self_attn.sampling_offsets.weight\": 65536,\n  \"transformer.encoder.layers.2.self_attn.sampling_offsets.bias\": 256,\n  \"transformer.encoder.layers.2.self_attn.attention_weights.weight\": 32768,\n  \"transformer.encoder.layers.2.self_attn.attention_weights.bias\": 128,\n  \"transformer.encoder.layers.2.self_attn.value_proj.weight\": 65536,\n  \"transformer.encoder.layers.2.self_attn.value_proj.bias\": 256,\n  \"transformer.encoder.layers.2.self_attn.output_proj.weight\": 65536,\n  \"transformer.encoder.layers.2.self_attn.output_proj.bias\": 256,\n  \"transformer.encoder.layers.2.norm1.weight\": 256,\n  \"transformer.encoder.layers.2.norm1.bias\": 256,\n  \"transformer.encoder.layers.2.linear1.weight\": 524288,\n  \"transformer.encoder.layers.2.linear1.bias\": 2048,\n  \"transformer.encoder.layers.2.linear2.weight\": 524288,\n  \"transformer.encoder.layers.2.linear2.bias\": 256,\n  \"transformer.encoder.layers.2.norm2.weight\": 256,\n  \"transformer.encoder.layers.2.norm2.bias\": 256,\n  \"transformer.encoder.layers.3.self_attn.sampling_offsets.weight\": 65536,\n  \"transformer.encoder.layers.3.self_attn.sampling_offsets.bias\": 256,\n  \"transformer.encoder.layers.3.self_attn.attention_weights.weight\": 32768,\n  \"transformer.encoder.layers.3.self_attn.attention_weights.bias\": 128,\n  \"transformer.encoder.layers.3.self_attn.value_proj.weight\": 65536,\n  \"transformer.encoder.layers.3.self_attn.value_proj.bias\": 256,\n  \"transformer.encoder.layers.3.self_attn.output_proj.weight\": 65536,\n  \"transformer.encoder.layers.3.self_attn.output_proj.bias\": 256,\n  \"transformer.encoder.layers.3.norm1.weight\": 256,\n  \"transformer.encoder.layers.3.norm1.bias\": 256,\n  \"transformer.encoder.layers.3.linear1.weight\": 524288,\n  \"transformer.encoder.layers.3.linear1.bias\": 2048,\n  \"transformer.encoder.layers.3.linear2.weight\": 524288,\n  \"transformer.encoder.layers.3.linear2.bias\": 256,\n  \"transformer.encoder.layers.3.norm2.weight\": 256,\n  \"transformer.encoder.layers.3.norm2.bias\": 256,\n  \"transformer.encoder.layers.4.self_attn.sampling_offsets.weight\": 65536,\n  \"transformer.encoder.layers.4.self_attn.sampling_offsets.bias\": 256,\n  \"transformer.encoder.layers.4.self_attn.attention_weights.weight\": 32768,\n  \"transformer.encoder.layers.4.self_attn.attention_weights.bias\": 128,\n  \"transformer.encoder.layers.4.self_attn.value_proj.weight\": 65536,\n  \"transformer.encoder.layers.4.self_attn.value_proj.bias\": 256,\n  \"transformer.encoder.layers.4.self_attn.output_proj.weight\": 65536,\n  \"transformer.encoder.layers.4.self_attn.output_proj.bias\": 256,\n  \"transformer.encoder.layers.4.norm1.weight\": 256,\n  \"transformer.encoder.layers.4.norm1.bias\": 256,\n  \"transformer.encoder.layers.4.linear1.weight\": 524288,\n  \"transformer.encoder.layers.4.linear1.bias\": 2048,\n  \"transformer.encoder.layers.4.linear2.weight\": 524288,\n  \"transformer.encoder.layers.4.linear2.bias\": 256,\n  \"transformer.encoder.layers.4.norm2.weight\": 256,\n  \"transformer.encoder.layers.4.norm2.bias\": 256,\n  \"transformer.encoder.layers.5.self_attn.sampling_offsets.weight\": 65536,\n  \"transformer.encoder.layers.5.self_attn.sampling_offsets.bias\": 256,\n  \"transformer.encoder.layers.5.self_attn.attention_weights.weight\": 32768,\n  \"transformer.encoder.layers.5.self_attn.attention_weights.bias\": 128,\n  \"transformer.encoder.layers.5.self_attn.value_proj.weight\": 65536,\n  \"transformer.encoder.layers.5.self_attn.value_proj.bias\": 256,\n  \"transformer.encoder.layers.5.self_attn.output_proj.weight\": 65536,\n  \"transformer.encoder.layers.5.self_attn.output_proj.bias\": 256,\n  \"transformer.encoder.layers.5.norm1.weight\": 256,\n  \"transformer.encoder.layers.5.norm1.bias\": 256,\n  \"transformer.encoder.layers.5.linear1.weight\": 524288,\n  \"transformer.encoder.layers.5.linear1.bias\": 2048,\n  \"transformer.encoder.layers.5.linear2.weight\": 524288,\n  \"transformer.encoder.layers.5.linear2.bias\": 256,\n  \"transformer.encoder.layers.5.norm2.weight\": 256,\n  \"transformer.encoder.layers.5.norm2.bias\": 256,\n  \"transformer.decoder.layers.0.cross_attn.sampling_offsets.weight\": 65536,\n  \"transformer.decoder.layers.0.cross_attn.sampling_offsets.bias\": 256,\n  \"transformer.decoder.layers.0.cross_attn.attention_weights.weight\": 32768,\n  \"transformer.decoder.layers.0.cross_attn.attention_weights.bias\": 128,\n  \"transformer.decoder.layers.0.cross_attn.value_proj.weight\": 65536,\n  \"transformer.decoder.layers.0.cross_attn.value_proj.bias\": 256,\n  \"transformer.decoder.layers.0.cross_attn.output_proj.weight\": 65536,\n  \"transformer.decoder.layers.0.cross_attn.output_proj.bias\": 256,\n  \"transformer.decoder.layers.0.norm1.weight\": 256,\n  \"transformer.decoder.layers.0.norm1.bias\": 256,\n  \"transformer.decoder.layers.0.self_attn.in_proj_weight\": 196608,\n  \"transformer.decoder.layers.0.self_attn.in_proj_bias\": 768,\n  \"transformer.decoder.layers.0.self_attn.out_proj.weight\": 65536,\n  \"transformer.decoder.layers.0.self_attn.out_proj.bias\": 256,\n  \"transformer.decoder.layers.0.norm2.weight\": 256,\n  \"transformer.decoder.layers.0.norm2.bias\": 256,\n  \"transformer.decoder.layers.0.linear1.weight\": 524288,\n  \"transformer.decoder.layers.0.linear1.bias\": 2048,\n  \"transformer.decoder.layers.0.linear2.weight\": 524288,\n  \"transformer.decoder.layers.0.linear2.bias\": 256,\n  \"transformer.decoder.layers.0.norm3.weight\": 256,\n  \"transformer.decoder.layers.0.norm3.bias\": 256,\n  \"transformer.decoder.layers.1.cross_attn.sampling_offsets.weight\": 65536,\n  \"transformer.decoder.layers.1.cross_attn.sampling_offsets.bias\": 256,\n  \"transformer.decoder.layers.1.cross_attn.attention_weights.weight\": 32768,\n  \"transformer.decoder.layers.1.cross_attn.attention_weights.bias\": 128,\n  \"transformer.decoder.layers.1.cross_attn.value_proj.weight\": 65536,\n  \"transformer.decoder.layers.1.cross_attn.value_proj.bias\": 256,\n  \"transformer.decoder.layers.1.cross_attn.output_proj.weight\": 65536,\n  \"transformer.decoder.layers.1.cross_attn.output_proj.bias\": 256,\n  \"transformer.decoder.layers.1.norm1.weight\": 256,\n  \"transformer.decoder.layers.1.norm1.bias\": 256,\n  \"transformer.decoder.layers.1.self_attn.in_proj_weight\": 196608,\n  \"transformer.decoder.layers.1.self_attn.in_proj_bias\": 768,\n  \"transformer.decoder.layers.1.self_attn.out_proj.weight\": 65536,\n  \"transformer.decoder.layers.1.self_attn.out_proj.bias\": 256,\n  \"transformer.decoder.layers.1.norm2.weight\": 256,\n  \"transformer.decoder.layers.1.norm2.bias\": 256,\n  \"transformer.decoder.layers.1.linear1.weight\": 524288,\n  \"transformer.decoder.layers.1.linear1.bias\": 2048,\n  \"transformer.decoder.layers.1.linear2.weight\": 524288,\n  \"transformer.decoder.layers.1.linear2.bias\": 256,\n  \"transformer.decoder.layers.1.norm3.weight\": 256,\n  \"transformer.decoder.layers.1.norm3.bias\": 256,\n  \"transformer.decoder.layers.2.cross_attn.sampling_offsets.weight\": 65536,\n  \"transformer.decoder.layers.2.cross_attn.sampling_offsets.bias\": 256,\n  \"transformer.decoder.layers.2.cross_attn.attention_weights.weight\": 32768,\n  \"transformer.decoder.layers.2.cross_attn.attention_weights.bias\": 128,\n  \"transformer.decoder.layers.2.cross_attn.value_proj.weight\": 65536,\n  \"transformer.decoder.layers.2.cross_attn.value_proj.bias\": 256,\n  \"transformer.decoder.layers.2.cross_attn.output_proj.weight\": 65536,\n  \"transformer.decoder.layers.2.cross_attn.output_proj.bias\": 256,\n  \"transformer.decoder.layers.2.norm1.weight\": 256,\n  \"transformer.decoder.layers.2.norm1.bias\": 256,\n  \"transformer.decoder.layers.2.self_attn.in_proj_weight\": 196608,\n  \"transformer.decoder.layers.2.self_attn.in_proj_bias\": 768,\n  \"transformer.decoder.layers.2.self_attn.out_proj.weight\": 65536,\n  \"transformer.decoder.layers.2.self_attn.out_proj.bias\": 256,\n  \"transformer.decoder.layers.2.norm2.weight\": 256,\n  \"transformer.decoder.layers.2.norm2.bias\": 256,\n  \"transformer.decoder.layers.2.linear1.weight\": 524288,\n  \"transformer.decoder.layers.2.linear1.bias\": 2048,\n  \"transformer.decoder.layers.2.linear2.weight\": 524288,\n  \"transformer.decoder.layers.2.linear2.bias\": 256,\n  \"transformer.decoder.layers.2.norm3.weight\": 256,\n  \"transformer.decoder.layers.2.norm3.bias\": 256,\n  \"transformer.decoder.layers.3.cross_attn.sampling_offsets.weight\": 65536,\n  \"transformer.decoder.layers.3.cross_attn.sampling_offsets.bias\": 256,\n  \"transformer.decoder.layers.3.cross_attn.attention_weights.weight\": 32768,\n  \"transformer.decoder.layers.3.cross_attn.attention_weights.bias\": 128,\n  \"transformer.decoder.layers.3.cross_attn.value_proj.weight\": 65536,\n  \"transformer.decoder.layers.3.cross_attn.value_proj.bias\": 256,\n  \"transformer.decoder.layers.3.cross_attn.output_proj.weight\": 65536,\n  \"transformer.decoder.layers.3.cross_attn.output_proj.bias\": 256,\n  \"transformer.decoder.layers.3.norm1.weight\": 256,\n  \"transformer.decoder.layers.3.norm1.bias\": 256,\n  \"transformer.decoder.layers.3.self_attn.in_proj_weight\": 196608,\n  \"transformer.decoder.layers.3.self_attn.in_proj_bias\": 768,\n  \"transformer.decoder.layers.3.self_attn.out_proj.weight\": 65536,\n  \"transformer.decoder.layers.3.self_attn.out_proj.bias\": 256,\n  \"transformer.decoder.layers.3.norm2.weight\": 256,\n  \"transformer.decoder.layers.3.norm2.bias\": 256,\n  \"transformer.decoder.layers.3.linear1.weight\": 524288,\n  \"transformer.decoder.layers.3.linear1.bias\": 2048,\n  \"transformer.decoder.layers.3.linear2.weight\": 524288,\n  \"transformer.decoder.layers.3.linear2.bias\": 256,\n  \"transformer.decoder.layers.3.norm3.weight\": 256,\n  \"transformer.decoder.layers.3.norm3.bias\": 256,\n  \"transformer.decoder.layers.4.cross_attn.sampling_offsets.weight\": 65536,\n  \"transformer.decoder.layers.4.cross_attn.sampling_offsets.bias\": 256,\n  \"transformer.decoder.layers.4.cross_attn.attention_weights.weight\": 32768,\n  \"transformer.decoder.layers.4.cross_attn.attention_weights.bias\": 128,\n  \"transformer.decoder.layers.4.cross_attn.value_proj.weight\": 65536,\n  \"transformer.decoder.layers.4.cross_attn.value_proj.bias\": 256,\n  \"transformer.decoder.layers.4.cross_attn.output_proj.weight\": 65536,\n  \"transformer.decoder.layers.4.cross_attn.output_proj.bias\": 256,\n  \"transformer.decoder.layers.4.norm1.weight\": 256,\n  \"transformer.decoder.layers.4.norm1.bias\": 256,\n  \"transformer.decoder.layers.4.self_attn.in_proj_weight\": 196608,\n  \"transformer.decoder.layers.4.self_attn.in_proj_bias\": 768,\n  \"transformer.decoder.layers.4.self_attn.out_proj.weight\": 65536,\n  \"transformer.decoder.layers.4.self_attn.out_proj.bias\": 256,\n  \"transformer.decoder.layers.4.norm2.weight\": 256,\n  \"transformer.decoder.layers.4.norm2.bias\": 256,\n  \"transformer.decoder.layers.4.linear1.weight\": 524288,\n  \"transformer.decoder.layers.4.linear1.bias\": 2048,\n  \"transformer.decoder.layers.4.linear2.weight\": 524288,\n  \"transformer.decoder.layers.4.linear2.bias\": 256,\n  \"transformer.decoder.layers.4.norm3.weight\": 256,\n  \"transformer.decoder.layers.4.norm3.bias\": 256,\n  \"transformer.decoder.layers.5.cross_attn.sampling_offsets.weight\": 65536,\n  \"transformer.decoder.layers.5.cross_attn.sampling_offsets.bias\": 256,\n  \"transformer.decoder.layers.5.cross_attn.attention_weights.weight\": 32768,\n  \"transformer.decoder.layers.5.cross_attn.attention_weights.bias\": 128,\n  \"transformer.decoder.layers.5.cross_attn.value_proj.weight\": 65536,\n  \"transformer.decoder.layers.5.cross_attn.value_proj.bias\": 256,\n  \"transformer.decoder.layers.5.cross_attn.output_proj.weight\": 65536,\n  \"transformer.decoder.layers.5.cross_attn.output_proj.bias\": 256,\n  \"transformer.decoder.layers.5.norm1.weight\": 256,\n  \"transformer.decoder.layers.5.norm1.bias\": 256,\n  \"transformer.decoder.layers.5.self_attn.in_proj_weight\": 196608,\n  \"transformer.decoder.layers.5.self_attn.in_proj_bias\": 768,\n  \"transformer.decoder.layers.5.self_attn.out_proj.weight\": 65536,\n  \"transformer.decoder.layers.5.self_attn.out_proj.bias\": 256,\n  \"transformer.decoder.layers.5.norm2.weight\": 256,\n  \"transformer.decoder.layers.5.norm2.bias\": 256,\n  \"transformer.decoder.layers.5.linear1.weight\": 524288,\n  \"transformer.decoder.layers.5.linear1.bias\": 2048,\n  \"transformer.decoder.layers.5.linear2.weight\": 524288,\n  \"transformer.decoder.layers.5.linear2.bias\": 256,\n  \"transformer.decoder.layers.5.norm3.weight\": 256,\n  \"transformer.decoder.layers.5.norm3.bias\": 256,\n  \"transformer.decoder.norm.weight\": 256,\n  \"transformer.decoder.norm.bias\": 256,\n  \"transformer.decoder.ref_point_head.layers.0.weight\": 131072,\n  \"transformer.decoder.ref_point_head.layers.0.bias\": 256,\n  \"transformer.decoder.ref_point_head.layers.1.weight\": 65536,\n  \"transformer.decoder.ref_point_head.layers.1.bias\": 256,\n  \"transformer.decoder.bbox_embed.0.layers.0.weight\": 65536,\n  \"transformer.decoder.bbox_embed.0.layers.0.bias\": 256,\n  \"transformer.decoder.bbox_embed.0.layers.1.weight\": 65536,\n  \"transformer.decoder.bbox_embed.0.layers.1.bias\": 256,\n  \"transformer.decoder.bbox_embed.0.layers.2.weight\": 1024,\n  \"transformer.decoder.bbox_embed.0.layers.2.bias\": 4,\n  \"transformer.decoder.class_embed.0.weight\": 23296,\n  \"transformer.decoder.class_embed.0.bias\": 91,\n  \"transformer.tgt_embed.weight\": 230400,\n  \"transformer.enc_output.weight\": 65536,\n  \"transformer.enc_output.bias\": 256,\n  \"transformer.enc_output_norm.weight\": 256,\n  \"transformer.enc_output_norm.bias\": 256,\n  \"transformer.enc_out_bbox_embed.layers.0.weight\": 65536,\n  \"transformer.enc_out_bbox_embed.layers.0.bias\": 256,\n  \"transformer.enc_out_bbox_embed.layers.1.weight\": 65536,\n  \"transformer.enc_out_bbox_embed.layers.1.bias\": 256,\n  \"transformer.enc_out_bbox_embed.layers.2.weight\": 1024,\n  \"transformer.enc_out_bbox_embed.layers.2.bias\": 4,\n  \"transformer.enc_out_class_embed.weight\": 23296,\n  \"transformer.enc_out_class_embed.bias\": 91,\n  \"label_enc.weight\": 23552,\n  \"input_proj.0.0.weight\": 131072,\n  \"input_proj.0.0.bias\": 256,\n  \"input_proj.0.1.weight\": 256,\n  \"input_proj.0.1.bias\": 256,\n  \"input_proj.1.0.weight\": 262144,\n  \"input_proj.1.0.bias\": 256,\n  \"input_proj.1.1.weight\": 256,\n  \"input_proj.1.1.bias\": 256,\n  \"input_proj.2.0.weight\": 524288,\n  \"input_proj.2.0.bias\": 256,\n  \"input_proj.2.1.weight\": 256,\n  \"input_proj.2.1.bias\": 256,\n  \"input_proj.3.0.weight\": 4718592,\n  \"input_proj.3.0.bias\": 256,\n  \"input_proj.3.1.weight\": 256,\n  \"input_proj.3.1.bias\": 256,\n  \"backbone.0.body.layer2.0.conv1.weight\": 32768,\n  \"backbone.0.body.layer2.0.conv2.weight\": 147456,\n  \"backbone.0.body.layer2.0.conv3.weight\": 65536,\n  \"backbone.0.body.layer2.0.downsample.0.weight\": 131072,\n  \"backbone.0.body.layer2.1.conv1.weight\": 65536,\n  \"backbone.0.body.layer2.1.conv2.weight\": 147456,\n  \"backbone.0.body.layer2.1.conv3.weight\": 65536,\n  \"backbone.0.body.layer2.2.conv1.weight\": 65536,\n  \"backbone.0.body.layer2.2.conv2.weight\": 147456,\n  \"backbone.0.body.layer2.2.conv3.weight\": 65536,\n  \"backbone.0.body.layer2.3.conv1.weight\": 65536,\n  \"backbone.0.body.layer2.3.conv2.weight\": 147456,\n  \"backbone.0.body.layer2.3.conv3.weight\": 65536,\n  \"backbone.0.body.layer3.0.conv1.weight\": 131072,\n  \"backbone.0.body.layer3.0.conv2.weight\": 589824,\n  \"backbone.0.body.layer3.0.conv3.weight\": 262144,\n  \"backbone.0.body.layer3.0.downsample.0.weight\": 524288,\n  \"backbone.0.body.layer3.1.conv1.weight\": 262144,\n  \"backbone.0.body.layer3.1.conv2.weight\": 589824,\n  \"backbone.0.body.layer3.1.conv3.weight\": 262144,\n  \"backbone.0.body.layer3.2.conv1.weight\": 262144,\n  \"backbone.0.body.layer3.2.conv2.weight\": 589824,\n  \"backbone.0.body.layer3.2.conv3.weight\": 262144,\n  \"backbone.0.body.layer3.3.conv1.weight\": 262144,\n  \"backbone.0.body.layer3.3.conv2.weight\": 589824,\n  \"backbone.0.body.layer3.3.conv3.weight\": 262144,\n  \"backbone.0.body.layer3.4.conv1.weight\": 262144,\n  \"backbone.0.body.layer3.4.conv2.weight\": 589824,\n  \"backbone.0.body.layer3.4.conv3.weight\": 262144,\n  \"backbone.0.body.layer3.5.conv1.weight\": 262144,\n  \"backbone.0.body.layer3.5.conv2.weight\": 589824,\n  \"backbone.0.body.layer3.5.conv3.weight\": 262144,\n  \"backbone.0.body.layer4.0.conv1.weight\": 524288,\n  \"backbone.0.body.layer4.0.conv2.weight\": 2359296,\n  \"backbone.0.body.layer4.0.conv3.weight\": 1048576,\n  \"backbone.0.body.layer4.0.downsample.0.weight\": 2097152,\n  \"backbone.0.body.layer4.1.conv1.weight\": 1048576,\n  \"backbone.0.body.layer4.1.conv2.weight\": 2359296,\n  \"backbone.0.body.layer4.1.conv3.weight\": 1048576,\n  \"backbone.0.body.layer4.2.conv1.weight\": 1048576,\n  \"backbone.0.body.layer4.2.conv2.weight\": 2359296,\n  \"backbone.0.body.layer4.2.conv3.weight\": 1048576\n}\ndata_aug_params: {\n  \"scales\": [\n    480,\n    512,\n    544,\n    576,\n    608,\n    640,\n    672,\n    704,\n    736,\n    768,\n    800\n  ],\n  \"max_size\": 1333,\n  \"scales2_resize\": [\n    400,\n    500,\n    600\n  ],\n  \"scales2_crop\": [\n    384,\n    600\n  ]\n}\nloading annotations into memory...\nDone (t=0.02s)\ncreating index...\nindex created!\ndata_aug_params: {\n  \"scales\": [\n    480,\n    512,\n    544,\n    576,\n    608,\n    640,\n    672,\n    704,\n    736,\n    768,\n    800\n  ],\n  \"max_size\": 1333,\n  \"scales2_resize\": [\n    400,\n    500,\n    600\n  ],\n  \"scales2_crop\": [\n    384,\n    600\n  ]\n}\nloading annotations into memory...\nDone (t=0.00s)\ncreating index...\nindex created!\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\nStart training\n/kaggle/working/DINO/engine.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)\n/kaggle/working/DINO/engine.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=args.amp):\n/opt/conda/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3609.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n/kaggle/working/DINO/models/dino/dino.py:512: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n  t = torch.range(0, len(targets[i]['labels']) - 1).long().cuda()\nEpoch: [0]  [ 0/79]  eta: 0:03:06  lr: 0.000100  class_error: 100.00  loss: 43.8160 (43.8160)  loss_ce_dn: 1.1881 (1.1881)  loss_bbox_dn: 0.4516 (0.4516)  loss_giou_dn: 1.3225 (1.3225)  loss_ce: 1.2288 (1.2288)  loss_bbox: 0.8110 (0.8110)  loss_giou: 1.7389 (1.7389)  loss_ce_0: 1.1893 (1.1893)  loss_bbox_0: 0.8178 (0.8178)  loss_giou_0: 1.7389 (1.7389)  loss_ce_dn_0: 1.2139 (1.2139)  loss_bbox_dn_0: 0.4516 (0.4516)  loss_giou_dn_0: 1.3225 (1.3225)  loss_ce_1: 1.2623 (1.2623)  loss_bbox_1: 0.8178 (0.8178)  loss_giou_1: 1.7389 (1.7389)  loss_ce_dn_1: 1.2601 (1.2601)  loss_bbox_dn_1: 0.4516 (0.4516)  loss_giou_dn_1: 1.3225 (1.3225)  loss_ce_2: 1.2916 (1.2916)  loss_bbox_2: 0.8178 (0.8178)  loss_giou_2: 1.7389 (1.7389)  loss_ce_dn_2: 1.2679 (1.2679)  loss_bbox_dn_2: 0.4516 (0.4516)  loss_giou_dn_2: 1.3225 (1.3225)  loss_ce_3: 1.1108 (1.1108)  loss_bbox_3: 0.8178 (0.8178)  loss_giou_3: 1.7389 (1.7389)  loss_ce_dn_3: 1.0920 (1.0920)  loss_bbox_dn_3: 0.4516 (0.4516)  loss_giou_dn_3: 1.3225 (1.3225)  loss_ce_4: 1.1719 (1.1719)  loss_bbox_4: 0.8178 (0.8178)  loss_giou_4: 1.7389 (1.7389)  loss_ce_dn_4: 1.1207 (1.1207)  loss_bbox_dn_4: 0.4516 (0.4516)  loss_giou_dn_4: 1.3225 (1.3225)  loss_ce_interm: 0.8806 (0.8806)  loss_bbox_interm: 0.8212 (0.8212)  loss_giou_interm: 1.7389 (1.7389)  loss_ce_dn_unscaled: 1.1881 (1.1881)  loss_bbox_dn_unscaled: 0.0903 (0.0903)  loss_giou_dn_unscaled: 0.6613 (0.6613)  loss_xy_dn_unscaled: 0.0273 (0.0273)  loss_hw_dn_unscaled: 0.0631 (0.0631)  cardinality_error_dn_unscaled: 186.0000 (186.0000)  loss_ce_unscaled: 1.2288 (1.2288)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.1622 (0.1622)  loss_giou_unscaled: 0.8695 (0.8695)  loss_xy_unscaled: 0.0293 (0.0293)  loss_hw_unscaled: 0.1329 (0.1329)  cardinality_error_unscaled: 894.0000 (894.0000)  loss_ce_0_unscaled: 1.1893 (1.1893)  loss_bbox_0_unscaled: 0.1636 (0.1636)  loss_giou_0_unscaled: 0.8695 (0.8695)  loss_xy_0_unscaled: 0.0306 (0.0306)  loss_hw_0_unscaled: 0.1329 (0.1329)  cardinality_error_0_unscaled: 893.0000 (893.0000)  loss_ce_dn_0_unscaled: 1.2139 (1.2139)  loss_bbox_dn_0_unscaled: 0.0903 (0.0903)  loss_giou_dn_0_unscaled: 0.6613 (0.6613)  loss_xy_dn_0_unscaled: 0.0273 (0.0273)  loss_hw_dn_0_unscaled: 0.0631 (0.0631)  cardinality_error_dn_0_unscaled: 186.0000 (186.0000)  loss_ce_1_unscaled: 1.2623 (1.2623)  loss_bbox_1_unscaled: 0.1636 (0.1636)  loss_giou_1_unscaled: 0.8695 (0.8695)  loss_xy_1_unscaled: 0.0306 (0.0306)  loss_hw_1_unscaled: 0.1329 (0.1329)  cardinality_error_1_unscaled: 894.0000 (894.0000)  loss_ce_dn_1_unscaled: 1.2601 (1.2601)  loss_bbox_dn_1_unscaled: 0.0903 (0.0903)  loss_giou_dn_1_unscaled: 0.6613 (0.6613)  loss_xy_dn_1_unscaled: 0.0273 (0.0273)  loss_hw_dn_1_unscaled: 0.0631 (0.0631)  cardinality_error_dn_1_unscaled: 186.0000 (186.0000)  loss_ce_2_unscaled: 1.2916 (1.2916)  loss_bbox_2_unscaled: 0.1636 (0.1636)  loss_giou_2_unscaled: 0.8695 (0.8695)  loss_xy_2_unscaled: 0.0306 (0.0306)  loss_hw_2_unscaled: 0.1329 (0.1329)  cardinality_error_2_unscaled: 894.0000 (894.0000)  loss_ce_dn_2_unscaled: 1.2679 (1.2679)  loss_bbox_dn_2_unscaled: 0.0903 (0.0903)  loss_giou_dn_2_unscaled: 0.6613 (0.6613)  loss_xy_dn_2_unscaled: 0.0273 (0.0273)  loss_hw_dn_2_unscaled: 0.0631 (0.0631)  cardinality_error_dn_2_unscaled: 186.0000 (186.0000)  loss_ce_3_unscaled: 1.1108 (1.1108)  loss_bbox_3_unscaled: 0.1636 (0.1636)  loss_giou_3_unscaled: 0.8695 (0.8695)  loss_xy_3_unscaled: 0.0306 (0.0306)  loss_hw_3_unscaled: 0.1329 (0.1329)  cardinality_error_3_unscaled: 894.0000 (894.0000)  loss_ce_dn_3_unscaled: 1.0920 (1.0920)  loss_bbox_dn_3_unscaled: 0.0903 (0.0903)  loss_giou_dn_3_unscaled: 0.6613 (0.6613)  loss_xy_dn_3_unscaled: 0.0273 (0.0273)  loss_hw_dn_3_unscaled: 0.0631 (0.0631)  cardinality_error_dn_3_unscaled: 186.0000 (186.0000)  loss_ce_4_unscaled: 1.1719 (1.1719)  loss_bbox_4_unscaled: 0.1636 (0.1636)  loss_giou_4_unscaled: 0.8695 (0.8695)  loss_xy_4_unscaled: 0.0306 (0.0306)  loss_hw_4_unscaled: 0.1329 (0.1329)  cardinality_error_4_unscaled: 894.0000 (894.0000)  loss_ce_dn_4_unscaled: 1.1207 (1.1207)  loss_bbox_dn_4_unscaled: 0.0903 (0.0903)  loss_giou_dn_4_unscaled: 0.6613 (0.6613)  loss_xy_dn_4_unscaled: 0.0273 (0.0273)  loss_hw_dn_4_unscaled: 0.0631 (0.0631)  cardinality_error_dn_4_unscaled: 186.0000 (186.0000)  loss_ce_interm_unscaled: 0.8806 (0.8806)  loss_bbox_interm_unscaled: 0.1642 (0.1642)  loss_giou_interm_unscaled: 0.8695 (0.8695)  loss_xy_interm_unscaled: 0.0313 (0.0313)  loss_hw_interm_unscaled: 0.1329 (0.1329)  cardinality_error_interm_unscaled: 894.0000 (894.0000)  time: 2.3617  data: 0.5770  max mem: 3682\nEpoch: [0]  [10/79]  eta: 0:00:51  lr: 0.000100  class_error: 0.00  loss: 36.3750 (36.5899)  loss_ce_dn: 0.5159 (0.6535)  loss_bbox_dn: 0.5201 (0.5360)  loss_giou_dn: 1.3968 (1.3878)  loss_ce: 0.8504 (1.0236)  loss_bbox: 0.7329 (0.7363)  loss_giou: 1.5776 (1.5666)  loss_ce_0: 0.6219 (0.6613)  loss_bbox_0: 0.8178 (0.7926)  loss_giou_0: 1.7122 (1.6756)  loss_ce_dn_0: 0.3142 (0.4059)  loss_bbox_dn_0: 0.5046 (0.5251)  loss_giou_dn_0: 1.3781 (1.3665)  loss_ce_1: 0.6333 (0.6982)  loss_bbox_1: 0.7891 (0.7762)  loss_giou_1: 1.6603 (1.6242)  loss_ce_dn_1: 0.3100 (0.3980)  loss_bbox_dn_1: 0.5081 (0.5266)  loss_giou_dn_1: 1.3825 (1.3697)  loss_ce_2: 0.6797 (0.7279)  loss_bbox_2: 0.7749 (0.7622)  loss_giou_2: 1.6340 (1.6030)  loss_ce_dn_2: 0.3813 (0.4383)  loss_bbox_dn_2: 0.5115 (0.5284)  loss_giou_dn_2: 1.3840 (1.3730)  loss_ce_3: 0.6966 (0.8233)  loss_bbox_3: 0.7591 (0.7538)  loss_giou_3: 1.6106 (1.5850)  loss_ce_dn_3: 0.5149 (0.5026)  loss_bbox_dn_3: 0.5163 (0.5308)  loss_giou_dn_3: 1.3877 (1.3775)  loss_ce_4: 0.8021 (0.8794)  loss_bbox_4: 0.7476 (0.7453)  loss_giou_4: 1.5945 (1.5774)  loss_ce_dn_4: 0.5072 (0.5603)  loss_bbox_dn_4: 0.5190 (0.5329)  loss_giou_dn_4: 1.3909 (1.3816)  loss_ce_interm: 0.7590 (0.7663)  loss_bbox_interm: 0.8212 (0.7802)  loss_giou_interm: 1.7017 (1.6369)  loss_ce_dn_unscaled: 0.5159 (0.6535)  loss_bbox_dn_unscaled: 0.1040 (0.1072)  loss_giou_dn_unscaled: 0.6984 (0.6939)  loss_xy_dn_unscaled: 0.0335 (0.0351)  loss_hw_dn_unscaled: 0.0695 (0.0721)  cardinality_error_dn_unscaled: 187.5000 (189.1364)  loss_ce_unscaled: 0.8504 (1.0236)  class_error_unscaled: 0.0000 (26.5605)  loss_bbox_unscaled: 0.1466 (0.1473)  loss_giou_unscaled: 0.7888 (0.7833)  loss_xy_unscaled: 0.0283 (0.0305)  loss_hw_unscaled: 0.1290 (0.1168)  cardinality_error_unscaled: 895.5000 (893.8636)  loss_ce_0_unscaled: 0.6219 (0.6613)  loss_bbox_0_unscaled: 0.1636 (0.1585)  loss_giou_0_unscaled: 0.8561 (0.8378)  loss_xy_0_unscaled: 0.0342 (0.0343)  loss_hw_0_unscaled: 0.1329 (0.1242)  cardinality_error_0_unscaled: 895.5000 (893.6818)  loss_ce_dn_0_unscaled: 0.3142 (0.4059)  loss_bbox_dn_0_unscaled: 0.1009 (0.1050)  loss_giou_dn_0_unscaled: 0.6891 (0.6833)  loss_xy_dn_0_unscaled: 0.0334 (0.0347)  loss_hw_dn_0_unscaled: 0.0652 (0.0703)  cardinality_error_dn_0_unscaled: 187.5000 (189.1364)  loss_ce_1_unscaled: 0.6333 (0.6982)  loss_bbox_1_unscaled: 0.1578 (0.1552)  loss_giou_1_unscaled: 0.8301 (0.8121)  loss_xy_1_unscaled: 0.0306 (0.0323)  loss_hw_1_unscaled: 0.1329 (0.1229)  cardinality_error_1_unscaled: 895.5000 (893.8636)  loss_ce_dn_1_unscaled: 0.3100 (0.3980)  loss_bbox_dn_1_unscaled: 0.1016 (0.1053)  loss_giou_dn_1_unscaled: 0.6913 (0.6849)  loss_xy_dn_1_unscaled: 0.0334 (0.0348)  loss_hw_dn_1_unscaled: 0.0659 (0.0705)  cardinality_error_dn_1_unscaled: 187.5000 (189.1364)  loss_ce_2_unscaled: 0.6797 (0.7279)  loss_bbox_2_unscaled: 0.1550 (0.1524)  loss_giou_2_unscaled: 0.8170 (0.8015)  loss_xy_2_unscaled: 0.0302 (0.0304)  loss_hw_2_unscaled: 0.1329 (0.1220)  cardinality_error_2_unscaled: 895.5000 (893.8636)  loss_ce_dn_2_unscaled: 0.3813 (0.4383)  loss_bbox_dn_2_unscaled: 0.1023 (0.1057)  loss_giou_dn_2_unscaled: 0.6920 (0.6865)  loss_xy_dn_2_unscaled: 0.0334 (0.0348)  loss_hw_dn_2_unscaled: 0.0666 (0.0708)  cardinality_error_dn_2_unscaled: 187.5000 (189.1364)  loss_ce_3_unscaled: 0.6966 (0.8233)  loss_bbox_3_unscaled: 0.1518 (0.1508)  loss_giou_3_unscaled: 0.8053 (0.7925)  loss_xy_3_unscaled: 0.0284 (0.0310)  loss_hw_3_unscaled: 0.1329 (0.1198)  cardinality_error_3_unscaled: 895.5000 (893.8636)  loss_ce_dn_3_unscaled: 0.5149 (0.5026)  loss_bbox_dn_3_unscaled: 0.1033 (0.1062)  loss_giou_dn_3_unscaled: 0.6938 (0.6887)  loss_xy_dn_3_unscaled: 0.0334 (0.0349)  loss_hw_dn_3_unscaled: 0.0676 (0.0712)  cardinality_error_dn_3_unscaled: 187.5000 (189.1364)  loss_ce_4_unscaled: 0.8021 (0.8794)  loss_bbox_4_unscaled: 0.1495 (0.1491)  loss_giou_4_unscaled: 0.7973 (0.7887)  loss_xy_4_unscaled: 0.0284 (0.0306)  loss_hw_4_unscaled: 0.1321 (0.1184)  cardinality_error_4_unscaled: 895.5000 (893.8636)  loss_ce_dn_4_unscaled: 0.5072 (0.5603)  loss_bbox_dn_4_unscaled: 0.1038 (0.1066)  loss_giou_dn_4_unscaled: 0.6954 (0.6908)  loss_xy_dn_4_unscaled: 0.0335 (0.0350)  loss_hw_dn_4_unscaled: 0.0684 (0.0716)  cardinality_error_dn_4_unscaled: 187.5000 (189.1364)  loss_ce_interm_unscaled: 0.7590 (0.7663)  loss_bbox_interm_unscaled: 0.1642 (0.1560)  loss_giou_interm_unscaled: 0.8509 (0.8185)  loss_xy_interm_unscaled: 0.0293 (0.0292)  loss_hw_interm_unscaled: 0.1329 (0.1268)  cardinality_error_interm_unscaled: 895.5000 (893.8636)  time: 0.7534  data: 0.0620  max mem: 5583\nEpoch: [0]  [20/79]  eta: 0:00:40  lr: 0.000100  class_error: 0.00  loss: 31.9141 (34.1507)  loss_ce_dn: 0.4563 (0.5607)  loss_bbox_dn: 0.5201 (0.5492)  loss_giou_dn: 1.4177 (1.4493)  loss_ce: 0.7203 (0.8568)  loss_bbox: 0.4833 (0.5946)  loss_giou: 1.2996 (1.3921)  loss_ce_0: 0.5895 (0.6274)  loss_bbox_0: 0.5775 (0.7028)  loss_giou_0: 1.5328 (1.5824)  loss_ce_dn_0: 0.2525 (0.3397)  loss_bbox_dn_0: 0.4855 (0.5046)  loss_giou_dn_0: 1.3793 (1.3745)  loss_ce_1: 0.6333 (0.6796)  loss_bbox_1: 0.5594 (0.6687)  loss_giou_1: 1.4299 (1.5052)  loss_ce_dn_1: 0.2784 (0.3375)  loss_bbox_dn_1: 0.4893 (0.5095)  loss_giou_dn_1: 1.3831 (1.3811)  loss_ce_2: 0.6837 (0.7121)  loss_bbox_2: 0.5156 (0.6406)  loss_giou_2: 1.3886 (1.4584)  loss_ce_dn_2: 0.3096 (0.3605)  loss_bbox_dn_2: 0.5071 (0.5164)  loss_giou_dn_2: 1.3890 (1.3918)  loss_ce_3: 0.6721 (0.7497)  loss_bbox_3: 0.5018 (0.6206)  loss_giou_3: 1.3828 (1.4252)  loss_ce_dn_3: 0.3687 (0.4412)  loss_bbox_dn_3: 0.5163 (0.5268)  loss_giou_dn_3: 1.3962 (1.4088)  loss_ce_4: 0.7067 (0.7884)  loss_bbox_4: 0.5015 (0.6067)  loss_giou_4: 1.3466 (1.4093)  loss_ce_dn_4: 0.4934 (0.5319)  loss_bbox_dn_4: 0.5190 (0.5362)  loss_giou_dn_4: 1.4028 (1.4254)  loss_ce_interm: 0.7423 (0.7325)  loss_bbox_interm: 0.5809 (0.6919)  loss_giou_interm: 1.4954 (1.5605)  loss_ce_dn_unscaled: 0.4563 (0.5607)  loss_bbox_dn_unscaled: 0.1040 (0.1098)  loss_giou_dn_unscaled: 0.7089 (0.7246)  loss_xy_dn_unscaled: 0.0330 (0.0349)  loss_hw_dn_unscaled: 0.0695 (0.0750)  cardinality_error_dn_unscaled: 188.0000 (189.2619)  loss_ce_unscaled: 0.7203 (0.8568)  class_error_unscaled: 0.0000 (13.9127)  loss_bbox_unscaled: 0.0967 (0.1189)  loss_giou_unscaled: 0.6498 (0.6960)  loss_xy_unscaled: 0.0239 (0.0286)  loss_hw_unscaled: 0.0728 (0.0903)  cardinality_error_unscaled: 895.0000 (894.4048)  loss_ce_0_unscaled: 0.5895 (0.6274)  loss_bbox_0_unscaled: 0.1155 (0.1406)  loss_giou_0_unscaled: 0.7664 (0.7912)  loss_xy_0_unscaled: 0.0268 (0.0312)  loss_hw_0_unscaled: 0.0944 (0.1094)  cardinality_error_0_unscaled: 895.0000 (894.1905)  loss_ce_dn_0_unscaled: 0.2525 (0.3397)  loss_bbox_dn_0_unscaled: 0.0971 (0.1009)  loss_giou_dn_0_unscaled: 0.6896 (0.6873)  loss_xy_dn_0_unscaled: 0.0325 (0.0336)  loss_hw_dn_0_unscaled: 0.0638 (0.0673)  cardinality_error_dn_0_unscaled: 188.0000 (189.2619)  loss_ce_1_unscaled: 0.6333 (0.6796)  loss_bbox_1_unscaled: 0.1119 (0.1337)  loss_giou_1_unscaled: 0.7149 (0.7526)  loss_xy_1_unscaled: 0.0224 (0.0301)  loss_hw_1_unscaled: 0.0921 (0.1036)  cardinality_error_1_unscaled: 895.0000 (894.4048)  loss_ce_dn_1_unscaled: 0.2784 (0.3375)  loss_bbox_dn_1_unscaled: 0.0979 (0.1019)  loss_giou_dn_1_unscaled: 0.6915 (0.6905)  loss_xy_dn_1_unscaled: 0.0326 (0.0337)  loss_hw_dn_1_unscaled: 0.0656 (0.0682)  cardinality_error_dn_1_unscaled: 188.0000 (189.2619)  loss_ce_2_unscaled: 0.6837 (0.7121)  loss_bbox_2_unscaled: 0.1031 (0.1281)  loss_giou_2_unscaled: 0.6943 (0.7292)  loss_xy_2_unscaled: 0.0208 (0.0282)  loss_hw_2_unscaled: 0.0797 (0.0999)  cardinality_error_2_unscaled: 895.0000 (894.4048)  loss_ce_dn_2_unscaled: 0.3096 (0.3605)  loss_bbox_dn_2_unscaled: 0.1014 (0.1033)  loss_giou_dn_2_unscaled: 0.6945 (0.6959)  loss_xy_dn_2_unscaled: 0.0326 (0.0339)  loss_hw_dn_2_unscaled: 0.0666 (0.0694)  cardinality_error_dn_2_unscaled: 188.0000 (189.2619)  loss_ce_3_unscaled: 0.6721 (0.7497)  loss_bbox_3_unscaled: 0.1004 (0.1241)  loss_giou_3_unscaled: 0.6914 (0.7126)  loss_xy_3_unscaled: 0.0238 (0.0283)  loss_hw_3_unscaled: 0.0745 (0.0958)  cardinality_error_3_unscaled: 895.0000 (894.4048)  loss_ce_dn_3_unscaled: 0.3687 (0.4412)  loss_bbox_dn_3_unscaled: 0.1033 (0.1054)  loss_giou_dn_3_unscaled: 0.6981 (0.7044)  loss_xy_dn_3_unscaled: 0.0327 (0.0342)  loss_hw_dn_3_unscaled: 0.0676 (0.0712)  cardinality_error_dn_3_unscaled: 188.0000 (189.2619)  loss_ce_4_unscaled: 0.7067 (0.7884)  loss_bbox_4_unscaled: 0.1003 (0.1213)  loss_giou_4_unscaled: 0.6733 (0.7046)  loss_xy_4_unscaled: 0.0241 (0.0284)  loss_hw_4_unscaled: 0.0731 (0.0929)  cardinality_error_4_unscaled: 895.0000 (894.4048)  loss_ce_dn_4_unscaled: 0.4934 (0.5319)  loss_bbox_dn_4_unscaled: 0.1038 (0.1072)  loss_giou_dn_4_unscaled: 0.7014 (0.7127)  loss_xy_dn_4_unscaled: 0.0328 (0.0345)  loss_hw_dn_4_unscaled: 0.0684 (0.0728)  cardinality_error_dn_4_unscaled: 188.0000 (189.2619)  loss_ce_interm_unscaled: 0.7423 (0.7325)  loss_bbox_interm_unscaled: 0.1162 (0.1384)  loss_giou_interm_unscaled: 0.7477 (0.7803)  loss_xy_interm_unscaled: 0.0221 (0.0255)  loss_hw_interm_unscaled: 0.0972 (0.1129)  cardinality_error_interm_unscaled: 895.0000 (894.4048)  time: 0.6073  data: 0.0085  max mem: 6023\nEpoch: [0]  [30/79]  eta: 0:00:33  lr: 0.000100  class_error: 0.00  loss: 30.0250 (33.1427)  loss_ce_dn: 0.4291 (0.5155)  loss_bbox_dn: 0.5535 (0.5744)  loss_giou_dn: 1.6240 (1.5267)  loss_ce: 0.6176 (0.7801)  loss_bbox: 0.4133 (0.5474)  loss_giou: 1.1884 (1.3276)  loss_ce_0: 0.5842 (0.6094)  loss_bbox_0: 0.4791 (0.6548)  loss_giou_0: 1.4297 (1.5348)  loss_ce_dn_0: 0.2521 (0.3129)  loss_bbox_dn_0: 0.4170 (0.4920)  loss_giou_dn_0: 1.3924 (1.3827)  loss_ce_1: 0.6168 (0.6495)  loss_bbox_1: 0.4378 (0.6123)  loss_giou_1: 1.3057 (1.4319)  loss_ce_dn_1: 0.2641 (0.3090)  loss_bbox_dn_1: 0.4455 (0.5029)  loss_giou_dn_1: 1.4212 (1.3989)  loss_ce_2: 0.6160 (0.6752)  loss_bbox_2: 0.4140 (0.5807)  loss_giou_2: 1.2324 (1.3777)  loss_ce_dn_2: 0.2376 (0.3168)  loss_bbox_dn_2: 0.4692 (0.5172)  loss_giou_dn_2: 1.4464 (1.4234)  loss_ce_3: 0.6188 (0.7067)  loss_bbox_3: 0.4002 (0.5613)  loss_giou_3: 1.1894 (1.3454)  loss_ce_dn_3: 0.3410 (0.4038)  loss_bbox_dn_3: 0.4973 (0.5368)  loss_giou_dn_3: 1.4916 (1.4574)  loss_ce_4: 0.6739 (0.7594)  loss_bbox_4: 0.4085 (0.5529)  loss_giou_4: 1.1774 (1.3328)  loss_ce_dn_4: 0.4419 (0.4911)  loss_bbox_dn_4: 0.5233 (0.5531)  loss_giou_dn_4: 1.5503 (1.4870)  loss_ce_interm: 0.6467 (0.7017)  loss_bbox_interm: 0.4690 (0.6523)  loss_giou_interm: 1.4587 (1.5468)  loss_ce_dn_unscaled: 0.4291 (0.5155)  loss_bbox_dn_unscaled: 0.1107 (0.1149)  loss_giou_dn_unscaled: 0.8120 (0.7634)  loss_xy_dn_unscaled: 0.0310 (0.0346)  loss_hw_dn_unscaled: 0.0758 (0.0803)  cardinality_error_dn_unscaled: 189.0000 (189.4516)  loss_ce_unscaled: 0.6176 (0.7801)  class_error_unscaled: 0.0000 (9.4247)  loss_bbox_unscaled: 0.0827 (0.1095)  loss_giou_unscaled: 0.5942 (0.6638)  loss_xy_unscaled: 0.0192 (0.0276)  loss_hw_unscaled: 0.0559 (0.0819)  cardinality_error_unscaled: 895.0000 (894.0323)  loss_ce_0_unscaled: 0.5842 (0.6094)  loss_bbox_0_unscaled: 0.0958 (0.1310)  loss_giou_0_unscaled: 0.7148 (0.7674)  loss_xy_0_unscaled: 0.0246 (0.0309)  loss_hw_0_unscaled: 0.0707 (0.1001)  cardinality_error_0_unscaled: 895.0000 (893.8871)  loss_ce_dn_0_unscaled: 0.2521 (0.3129)  loss_bbox_dn_0_unscaled: 0.0834 (0.0984)  loss_giou_dn_0_unscaled: 0.6962 (0.6914)  loss_xy_dn_0_unscaled: 0.0282 (0.0328)  loss_hw_dn_0_unscaled: 0.0582 (0.0656)  cardinality_error_dn_0_unscaled: 189.0000 (189.4516)  loss_ce_1_unscaled: 0.6168 (0.6495)  loss_bbox_1_unscaled: 0.0876 (0.1225)  loss_giou_1_unscaled: 0.6528 (0.7159)  loss_xy_1_unscaled: 0.0212 (0.0293)  loss_hw_1_unscaled: 0.0636 (0.0931)  cardinality_error_1_unscaled: 895.0000 (894.0323)  loss_ce_dn_1_unscaled: 0.2641 (0.3090)  loss_bbox_dn_1_unscaled: 0.0891 (0.1006)  loss_giou_dn_1_unscaled: 0.7106 (0.6995)  loss_xy_dn_1_unscaled: 0.0295 (0.0331)  loss_hw_dn_1_unscaled: 0.0619 (0.0675)  cardinality_error_dn_1_unscaled: 189.0000 (189.4516)  loss_ce_2_unscaled: 0.6160 (0.6752)  loss_bbox_2_unscaled: 0.0828 (0.1161)  loss_giou_2_unscaled: 0.6162 (0.6888)  loss_xy_2_unscaled: 0.0208 (0.0274)  loss_hw_2_unscaled: 0.0577 (0.0888)  cardinality_error_2_unscaled: 895.0000 (894.0323)  loss_ce_dn_2_unscaled: 0.2376 (0.3168)  loss_bbox_dn_2_unscaled: 0.0938 (0.1034)  loss_giou_dn_2_unscaled: 0.7232 (0.7117)  loss_xy_dn_2_unscaled: 0.0298 (0.0334)  loss_hw_dn_2_unscaled: 0.0633 (0.0701)  cardinality_error_dn_2_unscaled: 189.0000 (189.4516)  loss_ce_3_unscaled: 0.6188 (0.7067)  loss_bbox_3_unscaled: 0.0800 (0.1123)  loss_giou_3_unscaled: 0.5947 (0.6727)  loss_xy_3_unscaled: 0.0209 (0.0275)  loss_hw_3_unscaled: 0.0528 (0.0848)  cardinality_error_3_unscaled: 895.0000 (894.0323)  loss_ce_dn_3_unscaled: 0.3410 (0.4038)  loss_bbox_dn_3_unscaled: 0.0995 (0.1074)  loss_giou_dn_3_unscaled: 0.7458 (0.7287)  loss_xy_dn_3_unscaled: 0.0305 (0.0338)  loss_hw_dn_3_unscaled: 0.0689 (0.0736)  cardinality_error_dn_3_unscaled: 189.0000 (189.4516)  loss_ce_4_unscaled: 0.6739 (0.7594)  loss_bbox_4_unscaled: 0.0817 (0.1106)  loss_giou_4_unscaled: 0.5887 (0.6664)  loss_xy_4_unscaled: 0.0203 (0.0276)  loss_hw_4_unscaled: 0.0548 (0.0830)  cardinality_error_4_unscaled: 895.0000 (894.0323)  loss_ce_dn_4_unscaled: 0.4419 (0.4911)  loss_bbox_dn_4_unscaled: 0.1047 (0.1106)  loss_giou_dn_4_unscaled: 0.7751 (0.7435)  loss_xy_dn_4_unscaled: 0.0308 (0.0341)  loss_hw_dn_4_unscaled: 0.0708 (0.0765)  cardinality_error_dn_4_unscaled: 189.0000 (189.4516)  loss_ce_interm_unscaled: 0.6467 (0.7017)  loss_bbox_interm_unscaled: 0.0938 (0.1305)  loss_giou_interm_unscaled: 0.7293 (0.7734)  loss_xy_interm_unscaled: 0.0184 (0.0247)  loss_hw_interm_unscaled: 0.0772 (0.1058)  cardinality_error_interm_unscaled: 895.0000 (894.0323)  time: 0.6437  data: 0.0083  max mem: 6729\nEpoch: [0]  [40/79]  eta: 0:00:26  lr: 0.000100  class_error: 0.00  loss: 28.8470 (32.1249)  loss_ce_dn: 0.4004 (0.4852)  loss_bbox_dn: 0.5564 (0.5680)  loss_giou_dn: 1.6322 (1.5371)  loss_ce: 0.6093 (0.7471)  loss_bbox: 0.3993 (0.5150)  loss_giou: 1.1338 (1.2892)  loss_ce_0: 0.5512 (0.5980)  loss_bbox_0: 0.4060 (0.6013)  loss_giou_0: 1.2421 (1.4512)  loss_ce_dn_0: 0.2259 (0.2879)  loss_bbox_dn_0: 0.4082 (0.4802)  loss_giou_dn_0: 1.3923 (1.3825)  loss_ce_1: 0.5565 (0.6328)  loss_bbox_1: 0.3950 (0.5632)  loss_giou_1: 1.1029 (1.3569)  loss_ce_dn_1: 0.2175 (0.2837)  loss_bbox_dn_1: 0.4335 (0.4940)  loss_giou_dn_1: 1.4249 (1.4036)  loss_ce_2: 0.5959 (0.6656)  loss_bbox_2: 0.3910 (0.5375)  loss_giou_2: 1.0752 (1.3138)  loss_ce_dn_2: 0.2237 (0.2958)  loss_bbox_dn_2: 0.4629 (0.5103)  loss_giou_dn_2: 1.4818 (1.4316)  loss_ce_3: 0.6002 (0.6881)  loss_bbox_3: 0.3800 (0.5224)  loss_giou_3: 1.0711 (1.2935)  loss_ce_dn_3: 0.2851 (0.3715)  loss_bbox_dn_3: 0.4973 (0.5308)  loss_giou_dn_3: 1.5374 (1.4677)  loss_ce_4: 0.6172 (0.7238)  loss_bbox_4: 0.3813 (0.5179)  loss_giou_4: 1.0892 (1.2857)  loss_ce_dn_4: 0.4395 (0.4831)  loss_bbox_dn_4: 0.5233 (0.5470)  loss_giou_dn_4: 1.5712 (1.4975)  loss_ce_interm: 0.6404 (0.6897)  loss_bbox_interm: 0.4176 (0.6002)  loss_giou_interm: 1.3297 (1.4746)  loss_ce_dn_unscaled: 0.4004 (0.4852)  loss_bbox_dn_unscaled: 0.1113 (0.1136)  loss_giou_dn_unscaled: 0.8161 (0.7686)  loss_xy_dn_unscaled: 0.0310 (0.0339)  loss_hw_dn_unscaled: 0.0791 (0.0797)  cardinality_error_dn_unscaled: 189.0000 (189.2439)  loss_ce_unscaled: 0.6093 (0.7471)  class_error_unscaled: 0.0000 (7.1260)  loss_bbox_unscaled: 0.0799 (0.1030)  loss_giou_unscaled: 0.5669 (0.6446)  loss_xy_unscaled: 0.0209 (0.0268)  loss_hw_unscaled: 0.0577 (0.0762)  cardinality_error_unscaled: 894.0000 (893.8293)  loss_ce_0_unscaled: 0.5512 (0.5980)  loss_bbox_0_unscaled: 0.0812 (0.1203)  loss_giou_0_unscaled: 0.6210 (0.7256)  loss_xy_0_unscaled: 0.0246 (0.0302)  loss_hw_0_unscaled: 0.0593 (0.0901)  cardinality_error_0_unscaled: 894.0000 (893.7195)  loss_ce_dn_0_unscaled: 0.2259 (0.2879)  loss_bbox_dn_0_unscaled: 0.0816 (0.0960)  loss_giou_dn_0_unscaled: 0.6962 (0.6913)  loss_xy_dn_0_unscaled: 0.0282 (0.0322)  loss_hw_dn_0_unscaled: 0.0549 (0.0638)  cardinality_error_dn_0_unscaled: 189.0000 (189.2439)  loss_ce_1_unscaled: 0.5565 (0.6328)  loss_bbox_1_unscaled: 0.0790 (0.1126)  loss_giou_1_unscaled: 0.5514 (0.6784)  loss_xy_1_unscaled: 0.0230 (0.0284)  loss_hw_1_unscaled: 0.0576 (0.0842)  cardinality_error_1_unscaled: 894.0000 (893.8293)  loss_ce_dn_1_unscaled: 0.2175 (0.2837)  loss_bbox_dn_1_unscaled: 0.0867 (0.0988)  loss_giou_dn_1_unscaled: 0.7125 (0.7018)  loss_xy_dn_1_unscaled: 0.0292 (0.0325)  loss_hw_dn_1_unscaled: 0.0575 (0.0663)  cardinality_error_dn_1_unscaled: 189.0000 (189.2439)  loss_ce_2_unscaled: 0.5959 (0.6656)  loss_bbox_2_unscaled: 0.0782 (0.1075)  loss_giou_2_unscaled: 0.5376 (0.6569)  loss_xy_2_unscaled: 0.0222 (0.0270)  loss_hw_2_unscaled: 0.0558 (0.0805)  cardinality_error_2_unscaled: 894.0000 (893.8293)  loss_ce_dn_2_unscaled: 0.2237 (0.2958)  loss_bbox_dn_2_unscaled: 0.0926 (0.1021)  loss_giou_dn_2_unscaled: 0.7409 (0.7158)  loss_xy_dn_2_unscaled: 0.0299 (0.0328)  loss_hw_dn_2_unscaled: 0.0627 (0.0693)  cardinality_error_dn_2_unscaled: 189.0000 (189.2439)  loss_ce_3_unscaled: 0.6002 (0.6881)  loss_bbox_3_unscaled: 0.0760 (0.1045)  loss_giou_3_unscaled: 0.5356 (0.6467)  loss_xy_3_unscaled: 0.0216 (0.0268)  loss_hw_3_unscaled: 0.0536 (0.0777)  cardinality_error_3_unscaled: 894.0000 (893.8293)  loss_ce_dn_3_unscaled: 0.2851 (0.3715)  loss_bbox_dn_3_unscaled: 0.0995 (0.1062)  loss_giou_dn_3_unscaled: 0.7687 (0.7338)  loss_xy_dn_3_unscaled: 0.0304 (0.0331)  loss_hw_dn_3_unscaled: 0.0698 (0.0730)  cardinality_error_dn_3_unscaled: 189.0000 (189.2439)  loss_ce_4_unscaled: 0.6172 (0.7238)  loss_bbox_4_unscaled: 0.0763 (0.1036)  loss_giou_4_unscaled: 0.5446 (0.6428)  loss_xy_4_unscaled: 0.0212 (0.0265)  loss_hw_4_unscaled: 0.0548 (0.0771)  cardinality_error_4_unscaled: 894.0000 (893.8293)  loss_ce_dn_4_unscaled: 0.4395 (0.4831)  loss_bbox_dn_4_unscaled: 0.1047 (0.1094)  loss_giou_dn_4_unscaled: 0.7856 (0.7487)  loss_xy_dn_4_unscaled: 0.0308 (0.0335)  loss_hw_dn_4_unscaled: 0.0747 (0.0759)  cardinality_error_dn_4_unscaled: 189.0000 (189.2439)  loss_ce_interm_unscaled: 0.6404 (0.6897)  loss_bbox_interm_unscaled: 0.0835 (0.1200)  loss_giou_interm_unscaled: 0.6648 (0.7373)  loss_xy_interm_unscaled: 0.0177 (0.0235)  loss_hw_interm_unscaled: 0.0691 (0.0965)  cardinality_error_interm_unscaled: 894.0000 (893.8293)  time: 0.6658  data: 0.0103  max mem: 6729\nEpoch: [0]  [50/79]  eta: 0:00:19  lr: 0.000100  class_error: 0.00  loss: 28.2663 (31.4299)  loss_ce_dn: 0.3964 (0.4738)  loss_bbox_dn: 0.4920 (0.5486)  loss_giou_dn: 1.4600 (1.5153)  loss_ce: 0.5942 (0.7231)  loss_bbox: 0.3993 (0.5034)  loss_giou: 1.2012 (1.2808)  loss_ce_0: 0.5491 (0.5922)  loss_bbox_0: 0.4060 (0.5728)  loss_giou_0: 1.2144 (1.3944)  loss_ce_dn_0: 0.2022 (0.2713)  loss_bbox_dn_0: 0.4352 (0.4751)  loss_giou_dn_0: 1.3822 (1.3816)  loss_ce_1: 0.5618 (0.6224)  loss_bbox_1: 0.3969 (0.5423)  loss_giou_1: 1.1117 (1.3233)  loss_ce_dn_1: 0.2028 (0.2691)  loss_bbox_dn_1: 0.4352 (0.4870)  loss_giou_dn_1: 1.4072 (1.4008)  loss_ce_2: 0.5930 (0.6471)  loss_bbox_2: 0.4043 (0.5223)  loss_giou_2: 1.1032 (1.2931)  loss_ce_dn_2: 0.2218 (0.2805)  loss_bbox_dn_2: 0.4580 (0.5007)  loss_giou_dn_2: 1.4209 (1.4251)  loss_ce_3: 0.5805 (0.6757)  loss_bbox_3: 0.3880 (0.5089)  loss_giou_3: 1.1574 (1.2784)  loss_ce_dn_3: 0.2644 (0.3500)  loss_bbox_dn_3: 0.4636 (0.5178)  loss_giou_dn_3: 1.4353 (1.4563)  loss_ce_4: 0.6024 (0.7070)  loss_bbox_4: 0.3934 (0.5048)  loss_giou_4: 1.1491 (1.2746)  loss_ce_dn_4: 0.3776 (0.4568)  loss_bbox_dn_4: 0.4801 (0.5312)  loss_giou_dn_4: 1.4400 (1.4816)  loss_ce_interm: 0.6353 (0.6713)  loss_bbox_interm: 0.3877 (0.5652)  loss_giou_interm: 1.2016 (1.4041)  loss_ce_dn_unscaled: 0.3964 (0.4738)  loss_bbox_dn_unscaled: 0.0984 (0.1097)  loss_giou_dn_unscaled: 0.7300 (0.7577)  loss_xy_dn_unscaled: 0.0305 (0.0334)  loss_hw_dn_unscaled: 0.0679 (0.0763)  cardinality_error_dn_unscaled: 189.0000 (188.6569)  loss_ce_unscaled: 0.5942 (0.7231)  class_error_unscaled: 0.0000 (5.7287)  loss_bbox_unscaled: 0.0799 (0.1007)  loss_giou_unscaled: 0.6006 (0.6404)  loss_xy_unscaled: 0.0228 (0.0268)  loss_hw_unscaled: 0.0580 (0.0739)  cardinality_error_unscaled: 893.5000 (893.5588)  loss_ce_0_unscaled: 0.5491 (0.5922)  loss_bbox_0_unscaled: 0.0812 (0.1146)  loss_giou_0_unscaled: 0.6072 (0.6972)  loss_xy_0_unscaled: 0.0277 (0.0308)  loss_hw_0_unscaled: 0.0560 (0.0838)  cardinality_error_0_unscaled: 893.5000 (893.4706)  loss_ce_dn_0_unscaled: 0.2022 (0.2713)  loss_bbox_dn_0_unscaled: 0.0870 (0.0950)  loss_giou_dn_0_unscaled: 0.6911 (0.6908)  loss_xy_dn_0_unscaled: 0.0295 (0.0319)  loss_hw_dn_0_unscaled: 0.0563 (0.0631)  cardinality_error_dn_0_unscaled: 189.0000 (188.6569)  loss_ce_1_unscaled: 0.5618 (0.6224)  loss_bbox_1_unscaled: 0.0794 (0.1085)  loss_giou_1_unscaled: 0.5558 (0.6617)  loss_xy_1_unscaled: 0.0257 (0.0290)  loss_hw_1_unscaled: 0.0559 (0.0794)  cardinality_error_1_unscaled: 893.5000 (893.5588)  loss_ce_dn_1_unscaled: 0.2028 (0.2691)  loss_bbox_dn_1_unscaled: 0.0870 (0.0974)  loss_giou_dn_1_unscaled: 0.7036 (0.7004)  loss_xy_dn_1_unscaled: 0.0295 (0.0321)  loss_hw_dn_1_unscaled: 0.0577 (0.0653)  cardinality_error_dn_1_unscaled: 189.0000 (188.6569)  loss_ce_2_unscaled: 0.5930 (0.6471)  loss_bbox_2_unscaled: 0.0809 (0.1045)  loss_giou_2_unscaled: 0.5516 (0.6465)  loss_xy_2_unscaled: 0.0253 (0.0278)  loss_hw_2_unscaled: 0.0551 (0.0767)  cardinality_error_2_unscaled: 893.5000 (893.5588)  loss_ce_dn_2_unscaled: 0.2218 (0.2805)  loss_bbox_dn_2_unscaled: 0.0916 (0.1001)  loss_giou_dn_2_unscaled: 0.7105 (0.7126)  loss_xy_dn_2_unscaled: 0.0299 (0.0324)  loss_hw_dn_2_unscaled: 0.0599 (0.0677)  cardinality_error_dn_2_unscaled: 189.0000 (188.6569)  loss_ce_3_unscaled: 0.5805 (0.6757)  loss_bbox_3_unscaled: 0.0776 (0.1018)  loss_giou_3_unscaled: 0.5787 (0.6392)  loss_xy_3_unscaled: 0.0229 (0.0271)  loss_hw_3_unscaled: 0.0563 (0.0747)  cardinality_error_3_unscaled: 893.5000 (893.5588)  loss_ce_dn_3_unscaled: 0.2644 (0.3500)  loss_bbox_dn_3_unscaled: 0.0927 (0.1036)  loss_giou_dn_3_unscaled: 0.7176 (0.7281)  loss_xy_dn_3_unscaled: 0.0304 (0.0327)  loss_hw_dn_3_unscaled: 0.0653 (0.0708)  cardinality_error_dn_3_unscaled: 189.0000 (188.6569)  loss_ce_4_unscaled: 0.6024 (0.7070)  loss_bbox_4_unscaled: 0.0787 (0.1010)  loss_giou_4_unscaled: 0.5746 (0.6373)  loss_xy_4_unscaled: 0.0227 (0.0266)  loss_hw_4_unscaled: 0.0575 (0.0744)  cardinality_error_4_unscaled: 893.5000 (893.5588)  loss_ce_dn_4_unscaled: 0.3776 (0.4568)  loss_bbox_dn_4_unscaled: 0.0960 (0.1062)  loss_giou_dn_4_unscaled: 0.7200 (0.7408)  loss_xy_dn_4_unscaled: 0.0305 (0.0330)  loss_hw_dn_4_unscaled: 0.0666 (0.0732)  cardinality_error_dn_4_unscaled: 189.0000 (188.6569)  loss_ce_interm_unscaled: 0.6353 (0.6713)  loss_bbox_interm_unscaled: 0.0775 (0.1130)  loss_giou_interm_unscaled: 0.6008 (0.7020)  loss_xy_interm_unscaled: 0.0211 (0.0242)  loss_hw_interm_unscaled: 0.0572 (0.0888)  cardinality_error_interm_unscaled: 893.5000 (893.5588)  time: 0.6367  data: 0.0103  max mem: 6729\nEpoch: [0]  [60/79]  eta: 0:00:12  lr: 0.000100  class_error: 0.00  loss: 27.2601 (30.7823)  loss_ce_dn: 0.4433 (0.4665)  loss_bbox_dn: 0.4370 (0.5325)  loss_giou_dn: 1.4061 (1.4952)  loss_ce: 0.6096 (0.7123)  loss_bbox: 0.3978 (0.4860)  loss_giou: 1.1355 (1.2521)  loss_ce_0: 0.5435 (0.5871)  loss_bbox_0: 0.4180 (0.5468)  loss_giou_0: 1.0750 (1.3541)  loss_ce_dn_0: 0.1947 (0.2583)  loss_bbox_dn_0: 0.4350 (0.4712)  loss_giou_dn_0: 1.3774 (1.3819)  loss_ce_1: 0.5704 (0.6120)  loss_bbox_1: 0.4041 (0.5221)  loss_giou_1: 1.1117 (1.2916)  loss_ce_dn_1: 0.1926 (0.2567)  loss_bbox_dn_1: 0.4347 (0.4811)  loss_giou_dn_1: 1.3834 (1.3983)  loss_ce_2: 0.5746 (0.6429)  loss_bbox_2: 0.4044 (0.5036)  loss_giou_2: 1.1352 (1.2641)  loss_ce_dn_2: 0.2214 (0.2727)  loss_bbox_dn_2: 0.4352 (0.4925)  loss_giou_dn_2: 1.3914 (1.4189)  loss_ce_3: 0.5800 (0.6632)  loss_bbox_3: 0.3948 (0.4919)  loss_giou_3: 1.1346 (1.2510)  loss_ce_dn_3: 0.2237 (0.3280)  loss_bbox_dn_3: 0.4361 (0.5068)  loss_giou_dn_3: 1.4011 (1.4453)  loss_ce_4: 0.5938 (0.6927)  loss_bbox_4: 0.3940 (0.4877)  loss_giou_4: 1.1313 (1.2467)  loss_ce_dn_4: 0.3437 (0.4384)  loss_bbox_dn_4: 0.4365 (0.5179)  loss_giou_dn_4: 1.4031 (1.4667)  loss_ce_interm: 0.5849 (0.6594)  loss_bbox_interm: 0.3713 (0.5357)  loss_giou_interm: 1.0208 (1.3502)  loss_ce_dn_unscaled: 0.4433 (0.4665)  loss_bbox_dn_unscaled: 0.0874 (0.1065)  loss_giou_dn_unscaled: 0.7031 (0.7476)  loss_xy_dn_unscaled: 0.0289 (0.0328)  loss_hw_dn_unscaled: 0.0596 (0.0737)  cardinality_error_dn_unscaled: 190.5000 (189.0164)  loss_ce_unscaled: 0.6096 (0.7123)  class_error_unscaled: 0.0000 (4.7896)  loss_bbox_unscaled: 0.0796 (0.0972)  loss_giou_unscaled: 0.5678 (0.6260)  loss_xy_unscaled: 0.0226 (0.0264)  loss_hw_unscaled: 0.0568 (0.0708)  cardinality_error_unscaled: 894.0000 (893.6066)  loss_ce_0_unscaled: 0.5435 (0.5871)  loss_bbox_0_unscaled: 0.0836 (0.1094)  loss_giou_0_unscaled: 0.5375 (0.6771)  loss_xy_0_unscaled: 0.0273 (0.0307)  loss_hw_0_unscaled: 0.0514 (0.0787)  cardinality_error_0_unscaled: 894.0000 (893.5328)  loss_ce_dn_0_unscaled: 0.1947 (0.2583)  loss_bbox_dn_0_unscaled: 0.0870 (0.0942)  loss_giou_dn_0_unscaled: 0.6887 (0.6910)  loss_xy_dn_0_unscaled: 0.0284 (0.0316)  loss_hw_dn_0_unscaled: 0.0563 (0.0627)  cardinality_error_dn_0_unscaled: 190.5000 (189.0164)  loss_ce_1_unscaled: 0.5704 (0.6120)  loss_bbox_1_unscaled: 0.0808 (0.1044)  loss_giou_1_unscaled: 0.5558 (0.6458)  loss_xy_1_unscaled: 0.0255 (0.0291)  loss_hw_1_unscaled: 0.0534 (0.0753)  cardinality_error_1_unscaled: 894.0000 (893.6066)  loss_ce_dn_1_unscaled: 0.1926 (0.2567)  loss_bbox_dn_1_unscaled: 0.0869 (0.0962)  loss_giou_dn_1_unscaled: 0.6917 (0.6992)  loss_xy_dn_1_unscaled: 0.0285 (0.0317)  loss_hw_dn_1_unscaled: 0.0577 (0.0645)  cardinality_error_dn_1_unscaled: 190.5000 (189.0164)  loss_ce_2_unscaled: 0.5746 (0.6429)  loss_bbox_2_unscaled: 0.0809 (0.1007)  loss_giou_2_unscaled: 0.5676 (0.6321)  loss_xy_2_unscaled: 0.0248 (0.0277)  loss_hw_2_unscaled: 0.0542 (0.0730)  cardinality_error_2_unscaled: 894.0000 (893.6066)  loss_ce_dn_2_unscaled: 0.2214 (0.2727)  loss_bbox_dn_2_unscaled: 0.0870 (0.0985)  loss_giou_dn_2_unscaled: 0.6957 (0.7095)  loss_xy_dn_2_unscaled: 0.0286 (0.0320)  loss_hw_dn_2_unscaled: 0.0584 (0.0665)  cardinality_error_dn_2_unscaled: 190.5000 (189.0164)  loss_ce_3_unscaled: 0.5800 (0.6632)  loss_bbox_3_unscaled: 0.0790 (0.0984)  loss_giou_3_unscaled: 0.5673 (0.6255)  loss_xy_3_unscaled: 0.0227 (0.0270)  loss_hw_3_unscaled: 0.0553 (0.0714)  cardinality_error_3_unscaled: 894.0000 (893.6066)  loss_ce_dn_3_unscaled: 0.2237 (0.3280)  loss_bbox_dn_3_unscaled: 0.0872 (0.1014)  loss_giou_dn_3_unscaled: 0.7005 (0.7227)  loss_xy_dn_3_unscaled: 0.0288 (0.0323)  loss_hw_dn_3_unscaled: 0.0592 (0.0691)  cardinality_error_dn_3_unscaled: 190.5000 (189.0164)  loss_ce_4_unscaled: 0.5938 (0.6927)  loss_bbox_4_unscaled: 0.0788 (0.0975)  loss_giou_4_unscaled: 0.5657 (0.6234)  loss_xy_4_unscaled: 0.0226 (0.0263)  loss_hw_4_unscaled: 0.0559 (0.0712)  cardinality_error_4_unscaled: 894.0000 (893.6066)  loss_ce_dn_4_unscaled: 0.3437 (0.4384)  loss_bbox_dn_4_unscaled: 0.0873 (0.1036)  loss_giou_dn_4_unscaled: 0.7016 (0.7333)  loss_xy_dn_4_unscaled: 0.0288 (0.0325)  loss_hw_dn_4_unscaled: 0.0597 (0.0711)  cardinality_error_dn_4_unscaled: 190.5000 (189.0164)  loss_ce_interm_unscaled: 0.5849 (0.6594)  loss_bbox_interm_unscaled: 0.0743 (0.1071)  loss_giou_interm_unscaled: 0.5104 (0.6751)  loss_xy_interm_unscaled: 0.0216 (0.0242)  loss_hw_interm_unscaled: 0.0484 (0.0830)  cardinality_error_interm_unscaled: 894.0000 (893.6066)  time: 0.6375  data: 0.0104  max mem: 6729\nEpoch: [0]  [70/79]  eta: 0:00:05  lr: 0.000100  class_error: 0.00  loss: 25.6859 (29.9909)  loss_ce_dn: 0.3957 (0.4512)  loss_bbox_dn: 0.4003 (0.5124)  loss_giou_dn: 1.3880 (1.4807)  loss_ce: 0.6096 (0.7060)  loss_bbox: 0.2748 (0.4569)  loss_giou: 0.9532 (1.2107)  loss_ce_0: 0.5396 (0.5816)  loss_bbox_0: 0.3002 (0.5130)  loss_giou_0: 1.0317 (1.3092)  loss_ce_dn_0: 0.1823 (0.2468)  loss_bbox_dn_0: 0.4055 (0.4603)  loss_giou_dn_0: 1.3812 (1.3824)  loss_ce_1: 0.5547 (0.6064)  loss_bbox_1: 0.2993 (0.4909)  loss_giou_1: 1.0210 (1.2513)  loss_ce_dn_1: 0.1916 (0.2468)  loss_bbox_dn_1: 0.4042 (0.4686)  loss_giou_dn_1: 1.3834 (1.3967)  loss_ce_2: 0.5764 (0.6323)  loss_bbox_2: 0.2930 (0.4740)  loss_giou_2: 0.9915 (1.2254)  loss_ce_dn_2: 0.2074 (0.2621)  loss_bbox_dn_2: 0.4030 (0.4783)  loss_giou_dn_2: 1.3847 (1.4145)  loss_ce_3: 0.5800 (0.6546)  loss_bbox_3: 0.2854 (0.4633)  loss_giou_3: 0.9812 (1.2118)  loss_ce_dn_3: 0.2119 (0.3116)  loss_bbox_dn_3: 0.4019 (0.4904)  loss_giou_dn_3: 1.3863 (1.4373)  loss_ce_4: 0.5938 (0.6844)  loss_bbox_4: 0.2784 (0.4584)  loss_giou_4: 0.9587 (1.2063)  loss_ce_dn_4: 0.3124 (0.4144)  loss_bbox_dn_4: 0.4012 (0.5000)  loss_giou_dn_4: 1.3873 (1.4559)  loss_ce_interm: 0.5908 (0.6473)  loss_bbox_interm: 0.2768 (0.4997)  loss_giou_interm: 0.9697 (1.2971)  loss_ce_dn_unscaled: 0.3957 (0.4512)  loss_bbox_dn_unscaled: 0.0801 (0.1025)  loss_giou_dn_unscaled: 0.6940 (0.7403)  loss_xy_dn_unscaled: 0.0266 (0.0318)  loss_hw_dn_unscaled: 0.0531 (0.0706)  cardinality_error_dn_unscaled: 190.5000 (188.3380)  loss_ce_unscaled: 0.6096 (0.7060)  class_error_unscaled: 0.0000 (4.1150)  loss_bbox_unscaled: 0.0550 (0.0914)  loss_giou_unscaled: 0.4766 (0.6053)  loss_xy_unscaled: 0.0127 (0.0247)  loss_hw_unscaled: 0.0439 (0.0667)  cardinality_error_unscaled: 894.5000 (893.4648)  loss_ce_0_unscaled: 0.5396 (0.5816)  loss_bbox_0_unscaled: 0.0600 (0.1026)  loss_giou_0_unscaled: 0.5159 (0.6546)  loss_xy_0_unscaled: 0.0191 (0.0291)  loss_hw_0_unscaled: 0.0428 (0.0735)  cardinality_error_0_unscaled: 894.5000 (893.4014)  loss_ce_dn_0_unscaled: 0.1823 (0.2468)  loss_bbox_dn_0_unscaled: 0.0811 (0.0921)  loss_giou_dn_0_unscaled: 0.6906 (0.6912)  loss_xy_dn_0_unscaled: 0.0265 (0.0307)  loss_hw_dn_0_unscaled: 0.0542 (0.0613)  cardinality_error_dn_0_unscaled: 190.5000 (188.3380)  loss_ce_1_unscaled: 0.5547 (0.6064)  loss_bbox_1_unscaled: 0.0599 (0.0982)  loss_giou_1_unscaled: 0.5105 (0.6257)  loss_xy_1_unscaled: 0.0167 (0.0276)  loss_hw_1_unscaled: 0.0462 (0.0706)  cardinality_error_1_unscaled: 894.5000 (893.4648)  loss_ce_dn_1_unscaled: 0.1916 (0.2468)  loss_bbox_dn_1_unscaled: 0.0808 (0.0937)  loss_giou_dn_1_unscaled: 0.6917 (0.6983)  loss_xy_dn_1_unscaled: 0.0265 (0.0309)  loss_hw_dn_1_unscaled: 0.0539 (0.0628)  cardinality_error_dn_1_unscaled: 190.5000 (188.3380)  loss_ce_2_unscaled: 0.5764 (0.6323)  loss_bbox_2_unscaled: 0.0586 (0.0948)  loss_giou_2_unscaled: 0.4958 (0.6127)  loss_xy_2_unscaled: 0.0152 (0.0262)  loss_hw_2_unscaled: 0.0464 (0.0686)  cardinality_error_2_unscaled: 894.5000 (893.4648)  loss_ce_dn_2_unscaled: 0.2074 (0.2621)  loss_bbox_dn_2_unscaled: 0.0806 (0.0957)  loss_giou_dn_2_unscaled: 0.6924 (0.7072)  loss_xy_dn_2_unscaled: 0.0266 (0.0311)  loss_hw_dn_2_unscaled: 0.0537 (0.0646)  cardinality_error_dn_2_unscaled: 190.5000 (188.3380)  loss_ce_3_unscaled: 0.5800 (0.6546)  loss_bbox_3_unscaled: 0.0571 (0.0927)  loss_giou_3_unscaled: 0.4906 (0.6059)  loss_xy_3_unscaled: 0.0144 (0.0254)  loss_hw_3_unscaled: 0.0466 (0.0672)  cardinality_error_3_unscaled: 894.5000 (893.4648)  loss_ce_dn_3_unscaled: 0.2119 (0.3116)  loss_bbox_dn_3_unscaled: 0.0804 (0.0981)  loss_giou_dn_3_unscaled: 0.6932 (0.7187)  loss_xy_dn_3_unscaled: 0.0266 (0.0313)  loss_hw_dn_3_unscaled: 0.0534 (0.0668)  cardinality_error_dn_3_unscaled: 190.5000 (188.3380)  loss_ce_4_unscaled: 0.5938 (0.6844)  loss_bbox_4_unscaled: 0.0557 (0.0917)  loss_giou_4_unscaled: 0.4794 (0.6032)  loss_xy_4_unscaled: 0.0140 (0.0247)  loss_hw_4_unscaled: 0.0438 (0.0670)  cardinality_error_4_unscaled: 894.5000 (893.4648)  loss_ce_dn_4_unscaled: 0.3124 (0.4144)  loss_bbox_dn_4_unscaled: 0.0802 (0.1000)  loss_giou_dn_4_unscaled: 0.6936 (0.7280)  loss_xy_dn_4_unscaled: 0.0266 (0.0315)  loss_hw_dn_4_unscaled: 0.0533 (0.0684)  cardinality_error_dn_4_unscaled: 190.5000 (188.3380)  loss_ce_interm_unscaled: 0.5908 (0.6473)  loss_bbox_interm_unscaled: 0.0554 (0.0999)  loss_giou_interm_unscaled: 0.4848 (0.6486)  loss_xy_interm_unscaled: 0.0120 (0.0227)  loss_hw_interm_unscaled: 0.0438 (0.0772)  cardinality_error_interm_unscaled: 894.5000 (893.4648)  time: 0.6505  data: 0.0101  max mem: 6729\nEpoch: [0]  [78/79]  eta: 0:00:00  lr: 0.000100  class_error: 0.00  loss: 24.7447 (29.4171)  loss_ce_dn: 0.3490 (0.4389)  loss_bbox_dn: 0.3381 (0.5005)  loss_giou_dn: 1.3802 (1.4708)  loss_ce: 0.5974 (0.6983)  loss_bbox: 0.2533 (0.4375)  loss_giou: 0.9392 (1.1792)  loss_ce_0: 0.5202 (0.5745)  loss_bbox_0: 0.2818 (0.4912)  loss_giou_0: 1.0010 (1.2727)  loss_ce_dn_0: 0.1714 (0.2381)  loss_bbox_dn_0: 0.3412 (0.4540)  loss_giou_dn_0: 1.3791 (1.3820)  loss_ce_1: 0.5316 (0.5973)  loss_bbox_1: 0.2807 (0.4707)  loss_giou_1: 0.9757 (1.2200)  loss_ce_dn_1: 0.1708 (0.2383)  loss_bbox_dn_1: 0.3405 (0.4614)  loss_giou_dn_1: 1.3801 (1.3949)  loss_ce_2: 0.5671 (0.6304)  loss_bbox_2: 0.2702 (0.4547)  loss_giou_2: 0.9666 (1.1952)  loss_ce_dn_2: 0.1942 (0.2548)  loss_bbox_dn_2: 0.3397 (0.4700)  loss_giou_dn_2: 1.3805 (1.4111)  loss_ce_3: 0.5484 (0.6461)  loss_bbox_3: 0.2612 (0.4446)  loss_giou_3: 0.9498 (1.1821)  loss_ce_dn_3: 0.2061 (0.3003)  loss_bbox_dn_3: 0.3391 (0.4809)  loss_giou_dn_3: 1.3807 (1.4317)  loss_ce_4: 0.5930 (0.6752)  loss_bbox_4: 0.2556 (0.4394)  loss_giou_4: 0.9380 (1.1758)  loss_ce_dn_4: 0.2182 (0.3937)  loss_bbox_dn_4: 0.3387 (0.4894)  loss_giou_dn_4: 1.3804 (1.4485)  loss_ce_interm: 0.5788 (0.6404)  loss_bbox_interm: 0.2623 (0.4761)  loss_giou_interm: 0.9504 (1.2564)  loss_ce_dn_unscaled: 0.3490 (0.4389)  loss_bbox_dn_unscaled: 0.0676 (0.1001)  loss_giou_dn_unscaled: 0.6901 (0.7354)  loss_xy_dn_unscaled: 0.0239 (0.0312)  loss_hw_dn_unscaled: 0.0456 (0.0689)  cardinality_error_dn_unscaled: 189.5000 (188.4684)  loss_ce_unscaled: 0.5974 (0.6983)  class_error_unscaled: 0.0000 (3.6983)  loss_bbox_unscaled: 0.0507 (0.0875)  loss_giou_unscaled: 0.4696 (0.5896)  loss_xy_unscaled: 0.0100 (0.0235)  loss_hw_unscaled: 0.0404 (0.0640)  cardinality_error_unscaled: 894.5000 (893.5570)  loss_ce_0_unscaled: 0.5202 (0.5745)  loss_bbox_0_unscaled: 0.0564 (0.0982)  loss_giou_0_unscaled: 0.5005 (0.6363)  loss_xy_0_unscaled: 0.0150 (0.0278)  loss_hw_0_unscaled: 0.0418 (0.0704)  cardinality_error_0_unscaled: 894.5000 (893.5000)  loss_ce_dn_0_unscaled: 0.1714 (0.2381)  loss_bbox_dn_0_unscaled: 0.0682 (0.0908)  loss_giou_dn_0_unscaled: 0.6896 (0.6910)  loss_xy_dn_0_unscaled: 0.0235 (0.0302)  loss_hw_dn_0_unscaled: 0.0461 (0.0606)  cardinality_error_dn_0_unscaled: 189.5000 (188.4684)  loss_ce_1_unscaled: 0.5316 (0.5973)  loss_bbox_1_unscaled: 0.0561 (0.0941)  loss_giou_1_unscaled: 0.4878 (0.6100)  loss_xy_1_unscaled: 0.0147 (0.0263)  loss_hw_1_unscaled: 0.0417 (0.0678)  cardinality_error_1_unscaled: 894.5000 (893.5570)  loss_ce_dn_1_unscaled: 0.1708 (0.2383)  loss_bbox_dn_1_unscaled: 0.0681 (0.0923)  loss_giou_dn_1_unscaled: 0.6901 (0.6975)  loss_xy_dn_1_unscaled: 0.0235 (0.0304)  loss_hw_dn_1_unscaled: 0.0460 (0.0619)  cardinality_error_dn_1_unscaled: 189.5000 (188.4684)  loss_ce_2_unscaled: 0.5671 (0.6304)  loss_bbox_2_unscaled: 0.0540 (0.0909)  loss_giou_2_unscaled: 0.4833 (0.5976)  loss_xy_2_unscaled: 0.0137 (0.0250)  loss_hw_2_unscaled: 0.0413 (0.0659)  cardinality_error_2_unscaled: 894.5000 (893.5570)  loss_ce_dn_2_unscaled: 0.1942 (0.2548)  loss_bbox_dn_2_unscaled: 0.0679 (0.0940)  loss_giou_dn_2_unscaled: 0.6903 (0.7055)  loss_xy_dn_2_unscaled: 0.0236 (0.0305)  loss_hw_dn_2_unscaled: 0.0458 (0.0635)  cardinality_error_dn_2_unscaled: 189.5000 (188.4684)  loss_ce_3_unscaled: 0.5484 (0.6461)  loss_bbox_3_unscaled: 0.0522 (0.0889)  loss_giou_3_unscaled: 0.4749 (0.5910)  loss_xy_3_unscaled: 0.0129 (0.0242)  loss_hw_3_unscaled: 0.0409 (0.0647)  cardinality_error_3_unscaled: 894.5000 (893.5570)  loss_ce_dn_3_unscaled: 0.2061 (0.3003)  loss_bbox_dn_3_unscaled: 0.0678 (0.0962)  loss_giou_dn_3_unscaled: 0.6903 (0.7159)  loss_xy_dn_3_unscaled: 0.0237 (0.0308)  loss_hw_dn_3_unscaled: 0.0457 (0.0654)  cardinality_error_dn_3_unscaled: 189.5000 (188.4684)  loss_ce_4_unscaled: 0.5930 (0.6752)  loss_bbox_4_unscaled: 0.0511 (0.0879)  loss_giou_4_unscaled: 0.4690 (0.5879)  loss_xy_4_unscaled: 0.0108 (0.0234)  loss_hw_4_unscaled: 0.0407 (0.0644)  cardinality_error_4_unscaled: 894.5000 (893.5570)  loss_ce_dn_4_unscaled: 0.2182 (0.3937)  loss_bbox_dn_4_unscaled: 0.0677 (0.0979)  loss_giou_dn_4_unscaled: 0.6902 (0.7242)  loss_xy_dn_4_unscaled: 0.0238 (0.0310)  loss_hw_dn_4_unscaled: 0.0457 (0.0669)  cardinality_error_dn_4_unscaled: 189.5000 (188.4684)  loss_ce_interm_unscaled: 0.5788 (0.6404)  loss_bbox_interm_unscaled: 0.0525 (0.0952)  loss_giou_interm_unscaled: 0.4752 (0.6282)  loss_xy_interm_unscaled: 0.0099 (0.0216)  loss_hw_interm_unscaled: 0.0428 (0.0736)  cardinality_error_interm_unscaled: 894.5000 (893.5570)  time: 0.6162  data: 0.0098  max mem: 6729\nEpoch: [0] Total time: 0:00:51 (0.6550 s / it)\nAveraged stats: lr: 0.000100  class_error: 0.00  loss: 24.7447 (29.4171)  loss_ce_dn: 0.3490 (0.4389)  loss_bbox_dn: 0.3381 (0.5005)  loss_giou_dn: 1.3802 (1.4708)  loss_ce: 0.5974 (0.6983)  loss_bbox: 0.2533 (0.4375)  loss_giou: 0.9392 (1.1792)  loss_ce_0: 0.5202 (0.5745)  loss_bbox_0: 0.2818 (0.4912)  loss_giou_0: 1.0010 (1.2727)  loss_ce_dn_0: 0.1714 (0.2381)  loss_bbox_dn_0: 0.3412 (0.4540)  loss_giou_dn_0: 1.3791 (1.3820)  loss_ce_1: 0.5316 (0.5973)  loss_bbox_1: 0.2807 (0.4707)  loss_giou_1: 0.9757 (1.2200)  loss_ce_dn_1: 0.1708 (0.2383)  loss_bbox_dn_1: 0.3405 (0.4614)  loss_giou_dn_1: 1.3801 (1.3949)  loss_ce_2: 0.5671 (0.6304)  loss_bbox_2: 0.2702 (0.4547)  loss_giou_2: 0.9666 (1.1952)  loss_ce_dn_2: 0.1942 (0.2548)  loss_bbox_dn_2: 0.3397 (0.4700)  loss_giou_dn_2: 1.3805 (1.4111)  loss_ce_3: 0.5484 (0.6461)  loss_bbox_3: 0.2612 (0.4446)  loss_giou_3: 0.9498 (1.1821)  loss_ce_dn_3: 0.2061 (0.3003)  loss_bbox_dn_3: 0.3391 (0.4809)  loss_giou_dn_3: 1.3807 (1.4317)  loss_ce_4: 0.5930 (0.6752)  loss_bbox_4: 0.2556 (0.4394)  loss_giou_4: 0.9380 (1.1758)  loss_ce_dn_4: 0.2182 (0.3937)  loss_bbox_dn_4: 0.3387 (0.4894)  loss_giou_dn_4: 1.3804 (1.4485)  loss_ce_interm: 0.5788 (0.6404)  loss_bbox_interm: 0.2623 (0.4761)  loss_giou_interm: 0.9504 (1.2564)  loss_ce_dn_unscaled: 0.3490 (0.4389)  loss_bbox_dn_unscaled: 0.0676 (0.1001)  loss_giou_dn_unscaled: 0.6901 (0.7354)  loss_xy_dn_unscaled: 0.0239 (0.0312)  loss_hw_dn_unscaled: 0.0456 (0.0689)  cardinality_error_dn_unscaled: 189.5000 (188.4684)  loss_ce_unscaled: 0.5974 (0.6983)  class_error_unscaled: 0.0000 (3.6983)  loss_bbox_unscaled: 0.0507 (0.0875)  loss_giou_unscaled: 0.4696 (0.5896)  loss_xy_unscaled: 0.0100 (0.0235)  loss_hw_unscaled: 0.0404 (0.0640)  cardinality_error_unscaled: 894.5000 (893.5570)  loss_ce_0_unscaled: 0.5202 (0.5745)  loss_bbox_0_unscaled: 0.0564 (0.0982)  loss_giou_0_unscaled: 0.5005 (0.6363)  loss_xy_0_unscaled: 0.0150 (0.0278)  loss_hw_0_unscaled: 0.0418 (0.0704)  cardinality_error_0_unscaled: 894.5000 (893.5000)  loss_ce_dn_0_unscaled: 0.1714 (0.2381)  loss_bbox_dn_0_unscaled: 0.0682 (0.0908)  loss_giou_dn_0_unscaled: 0.6896 (0.6910)  loss_xy_dn_0_unscaled: 0.0235 (0.0302)  loss_hw_dn_0_unscaled: 0.0461 (0.0606)  cardinality_error_dn_0_unscaled: 189.5000 (188.4684)  loss_ce_1_unscaled: 0.5316 (0.5973)  loss_bbox_1_unscaled: 0.0561 (0.0941)  loss_giou_1_unscaled: 0.4878 (0.6100)  loss_xy_1_unscaled: 0.0147 (0.0263)  loss_hw_1_unscaled: 0.0417 (0.0678)  cardinality_error_1_unscaled: 894.5000 (893.5570)  loss_ce_dn_1_unscaled: 0.1708 (0.2383)  loss_bbox_dn_1_unscaled: 0.0681 (0.0923)  loss_giou_dn_1_unscaled: 0.6901 (0.6975)  loss_xy_dn_1_unscaled: 0.0235 (0.0304)  loss_hw_dn_1_unscaled: 0.0460 (0.0619)  cardinality_error_dn_1_unscaled: 189.5000 (188.4684)  loss_ce_2_unscaled: 0.5671 (0.6304)  loss_bbox_2_unscaled: 0.0540 (0.0909)  loss_giou_2_unscaled: 0.4833 (0.5976)  loss_xy_2_unscaled: 0.0137 (0.0250)  loss_hw_2_unscaled: 0.0413 (0.0659)  cardinality_error_2_unscaled: 894.5000 (893.5570)  loss_ce_dn_2_unscaled: 0.1942 (0.2548)  loss_bbox_dn_2_unscaled: 0.0679 (0.0940)  loss_giou_dn_2_unscaled: 0.6903 (0.7055)  loss_xy_dn_2_unscaled: 0.0236 (0.0305)  loss_hw_dn_2_unscaled: 0.0458 (0.0635)  cardinality_error_dn_2_unscaled: 189.5000 (188.4684)  loss_ce_3_unscaled: 0.5484 (0.6461)  loss_bbox_3_unscaled: 0.0522 (0.0889)  loss_giou_3_unscaled: 0.4749 (0.5910)  loss_xy_3_unscaled: 0.0129 (0.0242)  loss_hw_3_unscaled: 0.0409 (0.0647)  cardinality_error_3_unscaled: 894.5000 (893.5570)  loss_ce_dn_3_unscaled: 0.2061 (0.3003)  loss_bbox_dn_3_unscaled: 0.0678 (0.0962)  loss_giou_dn_3_unscaled: 0.6903 (0.7159)  loss_xy_dn_3_unscaled: 0.0237 (0.0308)  loss_hw_dn_3_unscaled: 0.0457 (0.0654)  cardinality_error_dn_3_unscaled: 189.5000 (188.4684)  loss_ce_4_unscaled: 0.5930 (0.6752)  loss_bbox_4_unscaled: 0.0511 (0.0879)  loss_giou_4_unscaled: 0.4690 (0.5879)  loss_xy_4_unscaled: 0.0108 (0.0234)  loss_hw_4_unscaled: 0.0407 (0.0644)  cardinality_error_4_unscaled: 894.5000 (893.5570)  loss_ce_dn_4_unscaled: 0.2182 (0.3937)  loss_bbox_dn_4_unscaled: 0.0677 (0.0979)  loss_giou_dn_4_unscaled: 0.6902 (0.7242)  loss_xy_dn_4_unscaled: 0.0238 (0.0310)  loss_hw_dn_4_unscaled: 0.0457 (0.0669)  cardinality_error_dn_4_unscaled: 189.5000 (188.4684)  loss_ce_interm_unscaled: 0.5788 (0.6404)  loss_bbox_interm_unscaled: 0.0525 (0.0952)  loss_giou_interm_unscaled: 0.4752 (0.6282)  loss_xy_interm_unscaled: 0.0099 (0.0216)  loss_hw_interm_unscaled: 0.0428 (0.0736)  cardinality_error_interm_unscaled: 894.5000 (893.5570)\n/kaggle/working/DINO/engine.py:164: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=args.amp):\nTest:  [ 0/40]  eta: 0:00:40  class_error: 0.00  loss: 10.3141 (10.3141)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.9070 (0.9070)  loss_bbox: 0.0535 (0.0535)  loss_giou: 0.4784 (0.4784)  loss_ce_0: 0.8344 (0.8344)  loss_bbox_0: 0.0765 (0.0765)  loss_giou_0: 0.6219 (0.6219)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.6553 (0.6553)  loss_bbox_1: 0.0986 (0.0986)  loss_giou_1: 1.0683 (1.0683)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.6723 (0.6723)  loss_bbox_2: 0.0826 (0.0826)  loss_giou_2: 0.6822 (0.6822)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.7722 (0.7722)  loss_bbox_3: 0.0575 (0.0575)  loss_giou_3: 0.4772 (0.4772)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.8660 (0.8660)  loss_bbox_4: 0.0559 (0.0559)  loss_giou_4: 0.4790 (0.4790)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.7136 (0.7136)  loss_bbox_interm: 0.0702 (0.0702)  loss_giou_interm: 0.5914 (0.5914)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.9070 (0.9070)  class_error_unscaled: 0.0000 (0.0000)  loss_bbox_unscaled: 0.0107 (0.0107)  loss_giou_unscaled: 0.2392 (0.2392)  loss_xy_unscaled: 0.0068 (0.0068)  loss_hw_unscaled: 0.0039 (0.0039)  cardinality_error_unscaled: 898.0000 (898.0000)  loss_ce_0_unscaled: 0.8344 (0.8344)  loss_bbox_0_unscaled: 0.0153 (0.0153)  loss_giou_0_unscaled: 0.3110 (0.3110)  loss_xy_0_unscaled: 0.0090 (0.0090)  loss_hw_0_unscaled: 0.0063 (0.0063)  cardinality_error_0_unscaled: 898.0000 (898.0000)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.6553 (0.6553)  loss_bbox_1_unscaled: 0.0197 (0.0197)  loss_giou_1_unscaled: 0.5341 (0.5341)  loss_xy_1_unscaled: 0.0139 (0.0139)  loss_hw_1_unscaled: 0.0058 (0.0058)  cardinality_error_1_unscaled: 898.0000 (898.0000)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.6723 (0.6723)  loss_bbox_2_unscaled: 0.0165 (0.0165)  loss_giou_2_unscaled: 0.3411 (0.3411)  loss_xy_2_unscaled: 0.0112 (0.0112)  loss_hw_2_unscaled: 0.0053 (0.0053)  cardinality_error_2_unscaled: 898.0000 (898.0000)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.7722 (0.7722)  loss_bbox_3_unscaled: 0.0115 (0.0115)  loss_giou_3_unscaled: 0.2386 (0.2386)  loss_xy_3_unscaled: 0.0068 (0.0068)  loss_hw_3_unscaled: 0.0048 (0.0048)  cardinality_error_3_unscaled: 898.0000 (898.0000)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.8660 (0.8660)  loss_bbox_4_unscaled: 0.0112 (0.0112)  loss_giou_4_unscaled: 0.2395 (0.2395)  loss_xy_4_unscaled: 0.0068 (0.0068)  loss_hw_4_unscaled: 0.0044 (0.0044)  cardinality_error_4_unscaled: 898.0000 (898.0000)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.7136 (0.7136)  loss_bbox_interm_unscaled: 0.0140 (0.0140)  loss_giou_interm_unscaled: 0.2957 (0.2957)  loss_xy_interm_unscaled: 0.0070 (0.0070)  loss_hw_interm_unscaled: 0.0071 (0.0071)  cardinality_error_interm_unscaled: 898.0000 (898.0000)  time: 1.0242  data: 0.7791  max mem: 6729\nTest:  [10/40]  eta: 0:00:08  class_error: 0.00  loss: 10.9840 (11.6649)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.6018 (0.6228)  loss_bbox: 0.1914 (0.2081)  loss_giou: 0.8300 (0.8388)  loss_ce_0: 0.4883 (0.5319)  loss_bbox_0: 0.1933 (0.2353)  loss_giou_0: 0.9365 (0.9462)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.4958 (0.5241)  loss_bbox_1: 0.2015 (0.2369)  loss_giou_1: 0.9902 (0.9555)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.5164 (0.5111)  loss_bbox_2: 0.1891 (0.2283)  loss_giou_2: 0.8603 (0.9113)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.4888 (0.5250)  loss_bbox_3: 0.1972 (0.2213)  loss_giou_3: 0.8576 (0.8696)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.5359 (0.5760)  loss_bbox_4: 0.1957 (0.2132)  loss_giou_4: 0.8553 (0.8458)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.5678 (0.5707)  loss_bbox_interm: 0.1883 (0.2174)  loss_giou_interm: 0.8751 (0.8758)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.6018 (0.6228)  class_error_unscaled: 0.0000 (0.0000)  loss_bbox_unscaled: 0.0383 (0.0416)  loss_giou_unscaled: 0.4150 (0.4194)  loss_xy_unscaled: 0.0083 (0.0119)  loss_hw_unscaled: 0.0272 (0.0297)  cardinality_error_unscaled: 895.0000 (891.9091)  loss_ce_0_unscaled: 0.4883 (0.5319)  loss_bbox_0_unscaled: 0.0387 (0.0471)  loss_giou_0_unscaled: 0.4683 (0.4731)  loss_xy_0_unscaled: 0.0108 (0.0159)  loss_hw_0_unscaled: 0.0277 (0.0312)  cardinality_error_0_unscaled: 895.0000 (891.9091)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.4958 (0.5241)  loss_bbox_1_unscaled: 0.0403 (0.0474)  loss_giou_1_unscaled: 0.4951 (0.4777)  loss_xy_1_unscaled: 0.0124 (0.0161)  loss_hw_1_unscaled: 0.0303 (0.0312)  cardinality_error_1_unscaled: 895.0000 (891.9091)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.5164 (0.5111)  loss_bbox_2_unscaled: 0.0378 (0.0457)  loss_giou_2_unscaled: 0.4301 (0.4556)  loss_xy_2_unscaled: 0.0112 (0.0149)  loss_hw_2_unscaled: 0.0299 (0.0307)  cardinality_error_2_unscaled: 895.0000 (891.9091)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.4888 (0.5250)  loss_bbox_3_unscaled: 0.0394 (0.0443)  loss_giou_3_unscaled: 0.4288 (0.4348)  loss_xy_3_unscaled: 0.0101 (0.0138)  loss_hw_3_unscaled: 0.0290 (0.0305)  cardinality_error_3_unscaled: 895.0000 (891.9091)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.5359 (0.5760)  loss_bbox_4_unscaled: 0.0391 (0.0426)  loss_giou_4_unscaled: 0.4276 (0.4229)  loss_xy_4_unscaled: 0.0086 (0.0126)  loss_hw_4_unscaled: 0.0272 (0.0300)  cardinality_error_4_unscaled: 895.0000 (891.9091)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.5678 (0.5707)  loss_bbox_interm_unscaled: 0.0377 (0.0435)  loss_giou_interm_unscaled: 0.4376 (0.4379)  loss_xy_interm_unscaled: 0.0082 (0.0115)  loss_hw_interm_unscaled: 0.0283 (0.0320)  cardinality_error_interm_unscaled: 895.0000 (891.9091)  time: 0.2742  data: 0.0759  max mem: 6729\nTest:  [20/40]  eta: 0:00:04  class_error: 0.00  loss: 11.3693 (12.0628)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.5701 (0.6095)  loss_bbox: 0.1992 (0.2814)  loss_giou: 0.8410 (0.8460)  loss_ce_0: 0.4971 (0.5200)  loss_bbox_0: 0.2293 (0.3040)  loss_giou_0: 0.9350 (0.9305)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.4958 (0.5163)  loss_bbox_1: 0.2254 (0.3063)  loss_giou_1: 0.9145 (0.9352)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.5164 (0.5111)  loss_bbox_2: 0.2141 (0.2957)  loss_giou_2: 0.8917 (0.9025)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.4990 (0.5153)  loss_bbox_3: 0.2031 (0.2887)  loss_giou_3: 0.8576 (0.8808)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.5266 (0.5601)  loss_bbox_4: 0.2012 (0.2824)  loss_giou_4: 0.8553 (0.8621)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.5530 (0.5635)  loss_bbox_interm: 0.2070 (0.2841)  loss_giou_interm: 0.8606 (0.8673)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.5701 (0.6095)  class_error_unscaled: 0.0000 (0.0000)  loss_bbox_unscaled: 0.0398 (0.0563)  loss_giou_unscaled: 0.4205 (0.4230)  loss_xy_unscaled: 0.0095 (0.0156)  loss_hw_unscaled: 0.0310 (0.0407)  cardinality_error_unscaled: 894.0000 (892.1429)  loss_ce_0_unscaled: 0.4971 (0.5200)  loss_bbox_0_unscaled: 0.0459 (0.0608)  loss_giou_0_unscaled: 0.4675 (0.4652)  loss_xy_0_unscaled: 0.0134 (0.0193)  loss_hw_0_unscaled: 0.0323 (0.0415)  cardinality_error_0_unscaled: 894.0000 (892.1429)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.4958 (0.5163)  loss_bbox_1_unscaled: 0.0451 (0.0613)  loss_giou_1_unscaled: 0.4573 (0.4676)  loss_xy_1_unscaled: 0.0134 (0.0199)  loss_hw_1_unscaled: 0.0314 (0.0413)  cardinality_error_1_unscaled: 894.0000 (892.1429)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.5164 (0.5111)  loss_bbox_2_unscaled: 0.0428 (0.0591)  loss_giou_2_unscaled: 0.4459 (0.4513)  loss_xy_2_unscaled: 0.0123 (0.0183)  loss_hw_2_unscaled: 0.0310 (0.0409)  cardinality_error_2_unscaled: 894.0000 (892.1429)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.4990 (0.5153)  loss_bbox_3_unscaled: 0.0406 (0.0577)  loss_giou_3_unscaled: 0.4288 (0.4404)  loss_xy_3_unscaled: 0.0102 (0.0178)  loss_hw_3_unscaled: 0.0308 (0.0400)  cardinality_error_3_unscaled: 894.0000 (892.1429)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.5266 (0.5601)  loss_bbox_4_unscaled: 0.0402 (0.0565)  loss_giou_4_unscaled: 0.4276 (0.4310)  loss_xy_4_unscaled: 0.0101 (0.0167)  loss_hw_4_unscaled: 0.0309 (0.0398)  cardinality_error_4_unscaled: 894.0000 (892.1429)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.5530 (0.5635)  loss_bbox_interm_unscaled: 0.0414 (0.0568)  loss_giou_interm_unscaled: 0.4303 (0.4336)  loss_xy_interm_unscaled: 0.0086 (0.0150)  loss_hw_interm_unscaled: 0.0319 (0.0418)  cardinality_error_interm_unscaled: 894.0000 (892.1429)  time: 0.1967  data: 0.0046  max mem: 6729\nTest:  [30/40]  eta: 0:00:02  class_error: 0.00  loss: 11.0384 (12.0092)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.5538 (0.6128)  loss_bbox: 0.1869 (0.2700)  loss_giou: 0.8410 (0.8484)  loss_ce_0: 0.4971 (0.5242)  loss_bbox_0: 0.2293 (0.2936)  loss_giou_0: 0.9090 (0.9268)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.4746 (0.5213)  loss_bbox_1: 0.2254 (0.2956)  loss_giou_1: 0.9172 (0.9277)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.4987 (0.5151)  loss_bbox_2: 0.2141 (0.2869)  loss_giou_2: 0.9168 (0.9014)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.4990 (0.5194)  loss_bbox_3: 0.2006 (0.2795)  loss_giou_3: 0.9118 (0.8788)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.5210 (0.5628)  loss_bbox_4: 0.1981 (0.2730)  loss_giou_4: 0.8528 (0.8630)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.5530 (0.5674)  loss_bbox_interm: 0.2018 (0.2737)  loss_giou_interm: 0.8430 (0.8677)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.5538 (0.6128)  class_error_unscaled: 0.0000 (0.0000)  loss_bbox_unscaled: 0.0374 (0.0540)  loss_giou_unscaled: 0.4205 (0.4242)  loss_xy_unscaled: 0.0078 (0.0144)  loss_hw_unscaled: 0.0302 (0.0396)  cardinality_error_unscaled: 893.0000 (892.6452)  loss_ce_0_unscaled: 0.4971 (0.5242)  loss_bbox_0_unscaled: 0.0459 (0.0587)  loss_giou_0_unscaled: 0.4545 (0.4634)  loss_xy_0_unscaled: 0.0117 (0.0182)  loss_hw_0_unscaled: 0.0325 (0.0406)  cardinality_error_0_unscaled: 893.0000 (892.6452)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.4746 (0.5213)  loss_bbox_1_unscaled: 0.0451 (0.0591)  loss_giou_1_unscaled: 0.4586 (0.4638)  loss_xy_1_unscaled: 0.0112 (0.0185)  loss_hw_1_unscaled: 0.0307 (0.0406)  cardinality_error_1_unscaled: 893.0000 (892.6452)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.4987 (0.5151)  loss_bbox_2_unscaled: 0.0428 (0.0574)  loss_giou_2_unscaled: 0.4584 (0.4507)  loss_xy_2_unscaled: 0.0104 (0.0172)  loss_hw_2_unscaled: 0.0307 (0.0402)  cardinality_error_2_unscaled: 893.0000 (892.6452)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.4990 (0.5194)  loss_bbox_3_unscaled: 0.0401 (0.0559)  loss_giou_3_unscaled: 0.4559 (0.4394)  loss_xy_3_unscaled: 0.0101 (0.0167)  loss_hw_3_unscaled: 0.0308 (0.0392)  cardinality_error_3_unscaled: 893.0000 (892.6452)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.5210 (0.5628)  loss_bbox_4_unscaled: 0.0396 (0.0546)  loss_giou_4_unscaled: 0.4264 (0.4315)  loss_xy_4_unscaled: 0.0099 (0.0156)  loss_hw_4_unscaled: 0.0309 (0.0390)  cardinality_error_4_unscaled: 893.0000 (892.6452)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.5530 (0.5674)  loss_bbox_interm_unscaled: 0.0404 (0.0547)  loss_giou_interm_unscaled: 0.4215 (0.4338)  loss_xy_interm_unscaled: 0.0083 (0.0141)  loss_hw_interm_unscaled: 0.0313 (0.0406)  cardinality_error_interm_unscaled: 893.0000 (892.6452)  time: 0.1917  data: 0.0050  max mem: 6729\nTest:  [39/40]  eta: 0:00:00  class_error: 0.00  loss: 10.8575 (11.9965)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.5626 (0.6128)  loss_bbox: 0.1869 (0.2695)  loss_giou: 0.7953 (0.8501)  loss_ce_0: 0.4716 (0.5193)  loss_bbox_0: 0.2299 (0.2953)  loss_giou_0: 0.9034 (0.9304)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.4746 (0.5218)  loss_bbox_1: 0.2230 (0.2934)  loss_giou_1: 0.9006 (0.9251)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.4867 (0.5140)  loss_bbox_2: 0.2084 (0.2867)  loss_giou_2: 0.8231 (0.8963)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.4961 (0.5171)  loss_bbox_3: 0.2090 (0.2795)  loss_giou_3: 0.8092 (0.8784)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.5266 (0.5597)  loss_bbox_4: 0.2087 (0.2736)  loss_giou_4: 0.8109 (0.8648)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.5584 (0.5669)  loss_bbox_interm: 0.2000 (0.2729)  loss_giou_interm: 0.8189 (0.8691)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.5626 (0.6128)  class_error_unscaled: 0.0000 (0.0000)  loss_bbox_unscaled: 0.0374 (0.0539)  loss_giou_unscaled: 0.3976 (0.4250)  loss_xy_unscaled: 0.0071 (0.0151)  loss_hw_unscaled: 0.0302 (0.0388)  cardinality_error_unscaled: 893.0000 (892.9250)  loss_ce_0_unscaled: 0.4716 (0.5193)  loss_bbox_0_unscaled: 0.0460 (0.0591)  loss_giou_0_unscaled: 0.4517 (0.4652)  loss_xy_0_unscaled: 0.0117 (0.0195)  loss_hw_0_unscaled: 0.0330 (0.0396)  cardinality_error_0_unscaled: 893.0000 (892.9250)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.4746 (0.5218)  loss_bbox_1_unscaled: 0.0446 (0.0587)  loss_giou_1_unscaled: 0.4503 (0.4625)  loss_xy_1_unscaled: 0.0105 (0.0191)  loss_hw_1_unscaled: 0.0330 (0.0396)  cardinality_error_1_unscaled: 893.0000 (892.9250)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.4867 (0.5140)  loss_bbox_2_unscaled: 0.0417 (0.0573)  loss_giou_2_unscaled: 0.4115 (0.4482)  loss_xy_2_unscaled: 0.0096 (0.0180)  loss_hw_2_unscaled: 0.0331 (0.0393)  cardinality_error_2_unscaled: 893.0000 (892.9250)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.4961 (0.5171)  loss_bbox_3_unscaled: 0.0418 (0.0559)  loss_giou_3_unscaled: 0.4046 (0.4392)  loss_xy_3_unscaled: 0.0097 (0.0173)  loss_hw_3_unscaled: 0.0331 (0.0385)  cardinality_error_3_unscaled: 893.0000 (892.9250)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.5266 (0.5597)  loss_bbox_4_unscaled: 0.0417 (0.0547)  loss_giou_4_unscaled: 0.4054 (0.4324)  loss_xy_4_unscaled: 0.0085 (0.0163)  loss_hw_4_unscaled: 0.0330 (0.0384)  cardinality_error_4_unscaled: 893.0000 (892.9250)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.5584 (0.5669)  loss_bbox_interm_unscaled: 0.0400 (0.0546)  loss_giou_interm_unscaled: 0.4095 (0.4346)  loss_xy_interm_unscaled: 0.0078 (0.0148)  loss_hw_interm_unscaled: 0.0321 (0.0397)  cardinality_error_interm_unscaled: 893.0000 (892.9250)  time: 0.1889  data: 0.0060  max mem: 6729\nTest: Total time: 0:00:08 (0.2168 s / it)\nAveraged stats: class_error: 0.00  loss: 10.8575 (11.9965)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.5626 (0.6128)  loss_bbox: 0.1869 (0.2695)  loss_giou: 0.7953 (0.8501)  loss_ce_0: 0.4716 (0.5193)  loss_bbox_0: 0.2299 (0.2953)  loss_giou_0: 0.9034 (0.9304)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.4746 (0.5218)  loss_bbox_1: 0.2230 (0.2934)  loss_giou_1: 0.9006 (0.9251)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.4867 (0.5140)  loss_bbox_2: 0.2084 (0.2867)  loss_giou_2: 0.8231 (0.8963)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.4961 (0.5171)  loss_bbox_3: 0.2090 (0.2795)  loss_giou_3: 0.8092 (0.8784)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.5266 (0.5597)  loss_bbox_4: 0.2087 (0.2736)  loss_giou_4: 0.8109 (0.8648)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.5584 (0.5669)  loss_bbox_interm: 0.2000 (0.2729)  loss_giou_interm: 0.8189 (0.8691)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.5626 (0.6128)  class_error_unscaled: 0.0000 (0.0000)  loss_bbox_unscaled: 0.0374 (0.0539)  loss_giou_unscaled: 0.3976 (0.4250)  loss_xy_unscaled: 0.0071 (0.0151)  loss_hw_unscaled: 0.0302 (0.0388)  cardinality_error_unscaled: 893.0000 (892.9250)  loss_ce_0_unscaled: 0.4716 (0.5193)  loss_bbox_0_unscaled: 0.0460 (0.0591)  loss_giou_0_unscaled: 0.4517 (0.4652)  loss_xy_0_unscaled: 0.0117 (0.0195)  loss_hw_0_unscaled: 0.0330 (0.0396)  cardinality_error_0_unscaled: 893.0000 (892.9250)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.4746 (0.5218)  loss_bbox_1_unscaled: 0.0446 (0.0587)  loss_giou_1_unscaled: 0.4503 (0.4625)  loss_xy_1_unscaled: 0.0105 (0.0191)  loss_hw_1_unscaled: 0.0330 (0.0396)  cardinality_error_1_unscaled: 893.0000 (892.9250)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.4867 (0.5140)  loss_bbox_2_unscaled: 0.0417 (0.0573)  loss_giou_2_unscaled: 0.4115 (0.4482)  loss_xy_2_unscaled: 0.0096 (0.0180)  loss_hw_2_unscaled: 0.0331 (0.0393)  cardinality_error_2_unscaled: 893.0000 (892.9250)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.4961 (0.5171)  loss_bbox_3_unscaled: 0.0418 (0.0559)  loss_giou_3_unscaled: 0.4046 (0.4392)  loss_xy_3_unscaled: 0.0097 (0.0173)  loss_hw_3_unscaled: 0.0331 (0.0385)  cardinality_error_3_unscaled: 893.0000 (892.9250)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.5266 (0.5597)  loss_bbox_4_unscaled: 0.0417 (0.0547)  loss_giou_4_unscaled: 0.4054 (0.4324)  loss_xy_4_unscaled: 0.0085 (0.0163)  loss_hw_4_unscaled: 0.0330 (0.0384)  cardinality_error_4_unscaled: 893.0000 (892.9250)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.5584 (0.5669)  loss_bbox_interm_unscaled: 0.0400 (0.0546)  loss_giou_interm_unscaled: 0.4095 (0.4346)  loss_xy_interm_unscaled: 0.0078 (0.0148)  loss_hw_interm_unscaled: 0.0321 (0.0397)  cardinality_error_interm_unscaled: 893.0000 (892.9250)\nAccumulating evaluation results...\nTraceback (most recent call last):\n  File \"/kaggle/working/DINO/main.py\", line 388, in <module>\n    main(args)\n  File \"/kaggle/working/DINO/main.py\", line 302, in main\n    test_stats, coco_evaluator = evaluate(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/kaggle/working/DINO/engine.py\", line 278, in evaluate\n    coco_evaluator.accumulate()\n  File \"/kaggle/working/DINO/datasets/coco_eval.py\", line 65, in accumulate\n    coco_eval.accumulate()\n  File \"/opt/conda/lib/python3.10/site-packages/pycocotools/cocoeval.py\", line 378, in accumulate\n    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n  File \"/opt/conda/lib/python3.10/site-packages/numpy/__init__.py\", line 324, in __getattr__\n    raise AttributeError(__former_attrs__[attr])\nAttributeError: module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'cfloat'?\n","output_type":"stream"}]},{"cell_type":"code","source":"!--pretrain_model_path /kaggle/input/dreamdelhi/checkpoint/checkpoint/checkpoint0033_4scale.pth","metadata":{"execution":{"iopub.status.busy":"2024-09-24T18:56:01.838571Z","iopub.execute_input":"2024-09-24T18:56:01.839517Z","iopub.status.idle":"2024-09-24T18:56:02.845304Z","shell.execute_reply.started":"2024-09-24T18:56:01.839466Z","shell.execute_reply":"2024-09-24T18:56:02.844128Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"/bin/bash: --: invalid option\nUsage:\t/bin/bash [GNU long option] [option] ...\n\t/bin/bash [GNU long option] [option] script-file ...\nGNU long options:\n\t--debug\n\t--debugger\n\t--dump-po-strings\n\t--dump-strings\n\t--help\n\t--init-file\n\t--login\n\t--noediting\n\t--noprofile\n\t--norc\n\t--posix\n\t--pretty-print\n\t--rcfile\n\t--restricted\n\t--verbose\n\t--version\nShell options:\n\t-ilrsD or -c command or -O shopt_option\t\t(invocation only)\n\t-abefhkmnptuvxBCHP or -o option\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}